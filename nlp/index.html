<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Interactive Guide to Natural Language Processing</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" xintegrity="sha384-n8MVd4RsNIU0KOVEMVIhbCwHMAA5maf+CgnUEz1RRenpDxLesoZjCpTOeZWYORkZ" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" xintegrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmFAILkIcU" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" xintegrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
    <!-- 
        Application Structure Plan:
        This single-page application is designed based on the user-provided "DNN Canvas" theme. It features a persistent left-sidebar for easy navigation through NLP topics. The main content area transforms static content into an engaging learning experience with interactive modules. For instance, concepts like Word Embeddings are visualized with a dynamic Chart.js plot, and the RAG process is explained with a clickable, step-by-step diagram. This structure prioritizes interactive learning and conceptual clarity, making the complex topics of NLP more accessible and explorable than in a linear document.

        Visualization & Content Choices:
        - NLU/NLG: A CSS-based diagram to show the hierarchical levels of language analysis. Goal: Organize & Inform.
        - Vector Semantics: A dynamic Chart.js scatter plot to visualize word vector relationships (e.g., king - man + woman ‚âà queen). Interaction: Users can see the spatial relationship between concepts. Goal: Explain & Demonstrate.
        - N-gram Models: An interactive text input where users can type a word and see a probable next word based on a simple bigram model. Goal: Explain Process & Interact.
        - Prompt Engineering: An interactive card system where clicking a prompt type reveals an example. Goal: Compare & Inform.
        - POS Tagging: An interactive sentence where hovering over words reveals their part-of-speech tags. Goal: Interact & Explain.
        - Attention Mechanism: A CSS-based heatmap visualization. Interaction: Hovering a word shows its attention scores relative to other words in the sentence. Goal: Explain & Demonstrate a core concept visually.
        - Word Sense Disambiguation: A simple interactive quiz-like format for an ambiguous word. Goal: Interact & Explain.
        - RAG: A step-by-step clickable diagram. Goal: Explain Process.
        All visualizations are built with HTML/CSS or Canvas via Chart.js, with no external SVG or Mermaid JS dependencies.
    -->
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #FDFBF8;
            color: #4A4A4A;
        }
        .active-nav {
            background-color: #EAE3DA;
            color: #8A6D53;
            font-weight: 600;
        }
        .chart-container {
            position: relative;
            width: 100%;
            max-width: 600px;
            margin-left: auto;
            margin-right: auto;
            height: 400px;
            max-height: 50vh;
        }
        .katex-display { margin: 0.5em 0; font-size: 1.1em; }
        .pos-tag { position: relative; cursor: pointer; }
        .pos-tag .tag-tooltip {
            visibility: hidden; opacity: 0;
            position: absolute; bottom: 100%; left: 50%;
            transform: translateX(-50%);
            margin-bottom: 5px; padding: 4px 8px;
            background-color: #8A6D53; color: white;
            border-radius: 4px; font-size: 0.8em; white-space: nowrap;
            transition: opacity 0.2s;
        }
        .pos-tag:hover .tag-tooltip { visibility: visible; opacity: 1; }
        .attention-word { transition: background-color 0.2s; }
        .rag-step { transition: all 0.3s ease-in-out; }
    </style>
</head>
<body class="flex min-h-screen">
    <aside id="sidebar" class="w-72 bg-[#F5F1EC] p-4 fixed top-0 left-0 h-full transform -translate-x-full md:translate-x-0 transition-transform duration-300 z-30 overflow-y-auto">
        <h2 class="text-xl font-bold text-[#8A6D53] mb-2">NLP Guide</h2>
        <a href="https://jeringk.github.io/aiml" class="block text-sm text-[#8A6D53] hover:underline mb-4">‚Üê Back to AIML Home</a>
        <nav class="space-y-1 text-sm">
            <a href="#topic-1" class="block py-2 px-3 rounded-md hover:bg-[#EAE3DA] transition-colors nav-link">1. NLU & Generation</a>
            <a href="#topic-2" class="block py-2 px-3 rounded-md hover:bg-[#EAE3DA] transition-colors nav-link">2. Vector Semantics & Embedding</a>
            <a href="#topic-3" class="block py-2 px-3 rounded-md hover:bg-[#EAE3DA] transition-colors nav-link">3. N-gram Language Modelling</a>
            <a href="#topic-4" class="block py-2 px-3 rounded-md hover:bg-[#EAE3DA] transition-colors nav-link">4. Neural Language Modelling</a>
            <a href="#topic-5" class="block py-2 px-3 rounded-md hover:bg-[#EAE3DA] transition-colors nav-link">5. LLM & Prompt Engineering</a>
            <a href="#topic-6" class="block py-2 px-3 rounded-md hover:bg-[#EAE3DA] transition-colors nav-link">6. Part-of-Speech Tagging</a>
            <a href="#topic-7" class="block py-2 px-3 rounded-md hover:bg-[#EAE3DA] transition-colors nav-link">7. Models of POS Tagging</a>
            <a href="#topic-8" class="block py-2 px-3 rounded-md hover:bg-[#EAE3DA] transition-colors nav-link">8. Parsing</a>
            <a href="#topic-9" class="block py-2 px-3 rounded-md hover:bg-[#EAE3DA] transition-colors nav-link">9. Dependency Parsing</a>
            <a href="#topic-10" class="block py-2 px-3 rounded-md hover:bg-[#EAE3DA] transition-colors nav-link">10. Encoder-Decoder & Attention</a>
            <a href="#topic-11" class="block py-2 px-3 rounded-md hover:bg-[#EAE3DA] transition-colors nav-link">11. Word Sense Disambiguation</a>
            <a href="#topic-12" class="block py-2 px-3 rounded-md hover:bg-[#EAE3DA] transition-colors nav-link">12. Semantic Web & Knowledge Graph</a>
            <a href="#topic-13" class="block py-2 px-3 rounded-md hover:bg-[#EAE3DA] transition-colors nav-link">13. Retrieval Augmented Generation</a>
            <a href="#topic-14" class="block py-2 px-3 rounded-md hover:bg-[#EAE3DA] transition-colors nav-link">14. State of the Art Applications</a>
        </nav>
    </aside>

    <button id="menu-toggle" class="md:hidden fixed top-4 left-4 z-40 p-2 bg-white rounded-md shadow-md">
        <svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentColor">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16m-7 6h7" />
        </svg>
    </button>
    
    <div id="overlay" class="md:hidden fixed inset-0 bg-black opacity-0 z-20 transition-opacity duration-300 hidden"></div>

    <main class="flex-1 md:ml-72 p-6 md:p-10">
        <header class="mb-12">
            <h1 class="text-4xl font-bold text-[#8A6D53]">Interactive Guide to Natural Language Processing</h1>
            <p class="mt-2 text-lg text-gray-600">An explorable guide to core NLP concepts. Use the sidebar to navigate.</p>
        </header>
        
        <div id="dynamic-content" class="space-y-16">
            <!-- Content will be injected here by JavaScript -->
        </div>
    </main>

<script>
document.addEventListener('DOMContentLoaded', () => {

    const content = {
        'topic-1': {
            title: "1. Natural Language Understanding and Generation",
            description: "NLU and NLG are the two fundamental pillars of NLP. NLU focuses on a machine's ability to comprehend human language, while NLG focuses on its ability to produce it. This process involves multiple layers of analysis.",
            html: `
            <div class="bg-white p-6 rounded-lg shadow-md">
                <h3 class="text-2xl font-semibold text-gray-800 mb-4">The Different Levels of Language Analysis</h3>
                <p class="mb-6">To truly understand language, a system must analyze it at several hierarchical levels, from the basic sounds to the intended meaning in context.</p>
                <div class="space-y-4">
                    <div class="p-4 bg-blue-50 border-l-4 border-blue-400 rounded-r-lg"><strong>Pragmatics:</strong> Understanding the intended meaning and context beyond the literal words. (e.g., "Can you pass the salt?" is a request, not a question of ability).</div>
                    <div class="p-4 bg-green-50 border-l-4 border-green-400 rounded-r-lg"><strong>Semantics:</strong> Determining the literal meaning of words and phrases. (e.g., "The cat sat on the mat" describes an action and objects).</div>
                    <div class="p-4 bg-yellow-50 border-l-4 border-yellow-400 rounded-r-lg"><strong>Syntax:</strong> Analyzing the grammatical structure of a sentence. (e.g., Identifying the subject, verb, and object).</div>
                    <div class="p-4 bg-purple-50 border-l-4 border-purple-400 rounded-r-lg"><strong>Morphology:</strong> Breaking words down into their smallest meaningful units (morphemes). (e.g., "unhappiness" -> "un-", "happy", "-ness").</div>
                    <div class="p-4 bg-red-50 border-l-4 border-red-400 rounded-r-lg"><strong>Phonology:</strong> Understanding the sound system of a language. (e.g., How sounds combine to form words).</div>
                </div>
            </div>`
        },
        'topic-2': {
            title: "2. Vector Semantics and Embedding",
            description: "Vector semantics represents words as dense numerical vectors in a multi-dimensional space. The position of a word's vector is determined by its context, allowing models to capture semantic relationships like 'king' is to 'queen' as 'man' is to 'woman'.",
            html: `
            <div class="bg-white p-6 rounded-lg shadow-md">
                <h3 class="text-2xl font-semibold text-gray-800 mb-4">Visualizing Word Embeddings</h3>
                <p class="mb-4">This chart shows a 2D projection of word vectors. Notice how concepts with similar meanings are clustered together, and the vector relationships between pairs like (man, woman) and (king, queen) are similar. This geometric relationship allows for reasoning through vector arithmetic.</p>
                <div class="chart-container"><canvas id="wordEmbeddingChart"></canvas></div>
                <p class="text-center mt-4 font-mono text-gray-600">vec("king") - vec("man") + vec("woman") ‚âà vec("queen")</p>
            </div>`
        },
        'topic-3': {
            title: "3. N-gram Language Modelling",
            description: "N-gram models are a statistical approach to language modeling. They predict the next word in a sequence by looking at the previous 'n-1' words. A bigram model (n=2), for example, predicts the next word based only on the previous word.",
            html: `
            <div class="bg-white p-6 rounded-lg shadow-md">
                <h3 class="text-2xl font-semibold text-gray-800 mb-4">Interactive Bigram Model</h3>
                <p class="mb-4">Type a word from the list and press Enter to see what a simple bigram model might predict next based on its training probabilities. Our simple model knows about 'the student', 'the professor', 'student reads', and 'professor writes'.</p>
                <div class="flex items-center space-x-2">
                    <input type="text" id="bigram-input" class="w-full p-2 border border-gray-300 rounded-md" placeholder="Type 'the', 'student', or 'professor'...">
                    <button id="bigram-predict" class="bg-[#8A6D53] text-white font-bold py-2 px-4 rounded-lg hover:bg-[#a18163]">Predict</button>
                </div>
                <div id="bigram-output" class="mt-4 p-4 bg-gray-100 rounded-lg min-h-[50px]"></div>
            </div>`
        },
        'topic-4': {
            title: "4. Neural Network and Neural Language Modelling",
            description: "Neural Language Models (NLMs) overcome the limitations of N-gram models by using neural networks, typically RNNs or Transformers, to learn word representations (embeddings). This allows them to handle much longer contexts and understand semantic similarity between words, leading to more fluent and accurate predictions.",
            html: `
            <div class="bg-white p-6 rounded-lg shadow-md">
                <h3 class="text-2xl font-semibold text-gray-800 mb-4">From N-grams to Neural Nets</h3>
                <div class="grid md:grid-cols-2 gap-6">
                    <div class="p-4 border rounded-lg">
                        <h4 class="font-bold text-lg mb-2">N-gram Model Weakness</h4>
                        <p class="text-sm">Suffers from the 'curse of dimensionality' and data sparsity. It cannot generalize to n-grams it hasn't seen. For example, if it saw "student reads a book" but not "student reads a paper", it cannot infer the similarity.</p>
                    </div>
                    <div class="p-4 border rounded-lg">
                        <h4 class="font-bold text-lg mb-2">Neural Model Strength</h4>
                        <p class="text-sm">Learns distributed representations (embeddings) for words. If 'book' and 'paper' have similar embeddings because they appear in similar contexts, the model can generalize from one to the other, making it far more powerful.</p>
                    </div>
                </div>
            </div>`
        },
        'topic-5': {
            title: "5. Introduction to LLM and Prompt Engineering",
            description: "Large Language Models (LLMs) are massive neural networks trained on vast amounts of text. Prompt engineering is the art of designing effective inputs (prompts) to guide these models to produce the desired output.",
            html: `
            <div class="bg-white p-6 rounded-lg shadow-md">
                <h3 class="text-2xl font-semibold text-gray-800 mb-4">Common Prompting Techniques</h3>
                <p class="mb-4">The way you ask a question can dramatically change the quality of the answer. Click on a technique to see an example.</p>
                <div class="grid md:grid-cols-2 gap-4">
                    <div class="prompt-card cursor-pointer p-4 bg-gray-50 rounded-lg hover:bg-gray-100 border" data-prompt="zero-shot">
                        <h4 class="font-bold">Zero-shot Prompting</h4>
                        <p class="text-sm">Directly asking the model to perform a task without any examples.</p>
                    </div>
                    <div class="prompt-card cursor-pointer p-4 bg-gray-50 rounded-lg hover:bg-gray-100 border" data-prompt="few-shot">
                        <h4 class="font-bold">Few-shot Prompting</h4>
                        <p class="text-sm">Providing a few examples of the task within the prompt to guide the model.</p>
                    </div>
                    <div class="prompt-card cursor-pointer p-4 bg-gray-50 rounded-lg hover:bg-gray-100 border" data-prompt="cot">
                        <h4 class="font-bold">Chain-of-Thought (CoT)</h4>
                        <p class="text-sm">Encouraging the model to "think step by step" to solve complex reasoning problems.</p>
                    </div>
                     <div class="prompt-card cursor-pointer p-4 bg-gray-50 rounded-lg hover:bg-gray-100 border" data-prompt="knowledge">
                        <h4 class="font-bold">Generated Knowledge</h4>
                        <p class="text-sm">Asking the model to first generate relevant facts before answering the final question.</p>
                    </div>
                </div>
                <div id="prompt-display" class="mt-6 p-4 bg-yellow-50 rounded-lg border-l-4 border-yellow-400 min-h-[100px]">
                    <p class="text-gray-600">Click a technique above to see an example prompt.</p>
                </div>
            </div>`
        },
        'topic-6': {
            title: "6. Part-of-Speech Tagging",
            description: "Part-of-Speech (POS) tagging is the process of assigning a grammatical category (like noun, verb, adjective, etc.) to each word in a sentence. It's a fundamental step in many NLP pipelines.",
            html: `
            <div class="bg-white p-6 rounded-lg shadow-md text-center">
                <h3 class="text-2xl font-semibold text-gray-800 mb-4">Interactive POS Tagger</h3>
                <p class="mb-6">Hover over the words in the sentence below to see their predicted Part-of-Speech tag from the Penn Treebank Tagset.</p>
                <p id="pos-sentence" class="text-2xl p-4 bg-gray-50 rounded-lg">
                    <span class="pos-tag" data-tag="DET">The</span>
                    <span class="pos-tag" data-tag="ADJ">quick</span>
                    <span class="pos-tag" data-tag="ADJ">brown</span>
                    <span class="pos-tag" data-tag="NOUN">fox</span>
                    <span class="pos-tag" data-tag="VERB">jumps</span>
                    <span class="pos-tag" data-tag="PREP">over</span>
                    <span class="pos-tag" data-tag="DET">the</span>
                    <span class="pos-tag" data-tag="ADJ">lazy</span>
                    <span class="pos-tag" data-tag="NOUN">dog</span>.
                </p>
            </div>`
        },
        'topic-7': {
            title: "7. Statistical, ML and Neural models of POS tagging",
            description: "POS tagging has evolved from rule-based systems to statistical models like Hidden Markov Models (HMMs), which use probabilities to find the most likely sequence of tags. Modern approaches often use deep learning models like Bi-LSTMs for state-of-the-art performance.",
            html: `
             <div class="bg-white p-6 rounded-lg shadow-md">
                <h3 class="text-2xl font-semibold text-gray-800 mb-4">Evolution of POS Taggers</h3>
                <div class="space-y-4">
                    <div class="p-4 bg-gray-50 rounded-lg">
                        <h4 class="font-bold">Hidden Markov Model (HMM)</h4>
                        <p class="text-sm">A statistical model that calculates the probability of a sequence of tags given a sequence of words. It uses two main probabilities: emission probability P(word|tag) and transition probability P(tag|previous_tag). The Viterbi algorithm is used to find the most likely tag sequence.</p>
                    </div>
                     <div class="p-4 bg-gray-50 rounded-lg">
                        <h4 class="font-bold">Maximum Entropy Markov Model (MEMM)</h4>
                        <p class="text-sm">An improvement over HMMs that allows for the inclusion of more features about the words and context, providing a more discriminative model.</p>
                    </div>
                     <div class="p-4 bg-gray-50 rounded-lg">
                        <h4 class="font-bold">Neural Network Models (e.g., Bi-LSTM)</h4>
                        <p class="text-sm">The current state-of-the-art. These models learn rich, contextualized representations of words in a sentence. A bidirectional LSTM can consider both past and future context for each word, making it highly accurate for POS tagging.</p>
                    </div>
                </div>
            </div>`
        },
        'topic-8': {
            title: "8. Parsing",
            description: "Parsing, or syntactic analysis, is the process of analyzing a sentence to determine its grammatical structure. This often results in a parse tree, which shows how words combine to form phrases and clauses.",
            html: `
            <div class="bg-white p-6 rounded-lg shadow-md">
                <h3 class="text-2xl font-semibold text-gray-800 mb-4">Constituency Parse Tree</h3>
                <p class="mb-4">A constituency parse tree breaks a sentence down into its constituent parts, or phrases, according to a Probabilistic Context-Free Grammar (PCFG). Below is a simplified example for the sentence "The cat sat on the mat".</p>
                <div class="p-4 bg-gray-50 rounded-lg border-2 border-gray-200 font-mono text-sm overflow-x-auto">
<pre>
(S
  (NP (DET The) (NOUN cat))
  (VP
    (VERB sat)
    (PP (PREP on) (NP (DET the) (NOUN mat)))))
</pre>
                </div>
            </div>`
        },
        'topic-9': {
            title: "9. Dependency Parsing",
            description: "Dependency parsing focuses on the relationships between words. It represents the sentence structure as a directed graph where words are nodes and grammatical relationships (dependencies) are arcs, typically with a 'root' verb.",
            html: `
            <div class="bg-white p-6 rounded-lg shadow-md">
                <h3 class="text-2xl font-semibold text-gray-800 mb-4">Dependency Graph</h3>
                <p class="mb-4">This shows the dependencies for "The quick brown fox jumps." The arrows point from the head word to the dependent word, and the label describes the grammatical relationship.</p>
                <div class="p-4 bg-gray-50 rounded-lg text-center">
                    <p class="text-lg">jumps ‚Üí fox <span class="text-gray-500">(nsubj)</span></p>
                    <p class="text-lg">fox ‚Üí The <span class="text-gray-500">(det)</span></p>
                    <p class="text-lg">fox ‚Üí quick <span class="text-gray-500">(amod)</span></p>
                    <p class="text-lg">fox ‚Üí brown <span class="text-gray-500">(amod)</span></p>
                </div>
            </div>`
        },
        'topic-10': {
            title: "10. Encoder-Decoder Models, Attention and Contextual Embeddings",
            description: "Encoder-Decoder models, the foundation of Transformers, map an input sequence to an output sequence. The 'attention mechanism' is a key innovation that allows the decoder to selectively focus on different parts of the input sequence when generating each output word. This leads to contextual embeddings like BERT.",
            html: `
            <div class="bg-white p-6 rounded-lg shadow-md">
                <h3 class="text-2xl font-semibold text-gray-800 mb-4">Visualizing Self-Attention</h3>
                <p class="mb-4">In the sentence "The animal didn't cross the street because it was too tired", the attention mechanism helps the model understand what "it" refers to. Hover over "it" to see which word it pays the most attention to.</p>
                <div id="attention-viz" class="p-4 bg-gray-50 rounded-lg text-xl leading-loose">
                    <span class="attention-word" data-scores="[0.05, 0.6, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]">The</span>
                    <span class="attention-word" data-scores="[0.8, 0.03, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.03]">animal</span>
                    <span class="attention-word" data-scores="[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]">didn't</span>
                    <span class="attention-word" data-scores="[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]">cross</span>
                    <span class="attention-word" data-scores="[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]">the</span>
                    <span class="attention-word" data-scores="[0.05, 0.6, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]">street</span>
                    <span class="attention-word" data-scores="[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]">because</span>
                    <span id="attention-it" class="attention-word font-bold cursor-pointer underline decoration-dotted" data-scores="[0.05, 0.8, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02]">it</span>
                    <span class="attention-word" data-scores="[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]">was</span>
                    <span class="attention-word" data-scores="[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]">too</span>
                    <span class="attention-word" data-scores="[0.05, 0.7, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]">tired.</span>
                </div>
                 <div class="mt-4 p-4 bg-gray-50 rounded-lg">
                    <h4 class="font-bold text-lg mb-2">Scaled Dot-Product Attention</h4>
                    <p>The core formula used by Transformers to calculate attention scores.</p>
                     <div class="text-center my-2">\`Attention(Q, K, V) = softmax( (QK^T) / (sqrt(d_k)) )V\`</div>
                </div>
            </div>`
        },
        'topic-11': {
            title: "11. Word Sense Disambiguation",
            description: "Words can have multiple meanings. Word Sense Disambiguation (WSD) is the task of identifying which specific meaning (sense) of a word is used in a given sentence.",
            html: `
            <div class="bg-white p-6 rounded-lg shadow-md">
                <h3 class="text-2xl font-semibold text-gray-800 mb-4">What does 'bank' mean here?</h3>
                <p class="text-xl mb-4 p-4 bg-gray-100 rounded-lg">"After the flood, the boat was found on the river <strong class="text-blue-600">bank</strong>."</p>
                <div id="wsd-options" class="space-y-2">
                    <button class="wsd-btn block w-full text-left p-3 bg-gray-50 hover:bg-gray-200 rounded-md border" data-correct="false">A financial institution.</button>
                    <button class="wsd-btn block w-full text-left p-3 bg-gray-50 hover:bg-gray-200 rounded-md border" data-correct="true">The land alongside a river.</button>
                </div>
                <div id="wsd-feedback" class="mt-4 font-bold"></div>
            </div>`
        },
        'topic-12': {
            title: "12. Semantic Web, Ontology and Knowledge Graph",
            description: "The Semantic Web aims to make web data machine-readable. Ontologies define a formal representation of knowledge with concepts and relationships. A Knowledge Graph is a practical implementation, storing real-world entities and their connections in a graph structure.",
            html: `
            <div class="bg-white p-6 rounded-lg shadow-md">
                <h3 class="text-2xl font-semibold text-gray-800 mb-4">Simple Knowledge Graph Example</h3>
                <p class="mb-4">A knowledge graph connects entities (nodes) with relationships (edges). This allows for complex queries that are difficult with traditional databases.</p>
                <div class="p-4 bg-gray-50 rounded-lg text-center">
                    <p class="text-lg">[<span class="font-semibold text-blue-700">Marie Curie</span>] --(<span class="text-gray-500">discovered</span>)--> [<span class="font-semibold text-green-700">Radium</span>]</p>
                    <p class="text-lg">[<span class="font-semibold text-blue-700">Marie Curie</span>] --(<span class="text-gray-500">field of study</span>)--> [<span class="font-semibold text-purple-700">Physics</span>]</p>
                    <p class="text-lg">[<span class="font-semibold text-blue-700">Marie Curie</span>] --(<span class="text-gray-500">won</span>)--> [<span class="font-semibold text-yellow-700">Nobel Prize</span>]</p>
                </div>
            </div>`
        },
        'topic-13': {
            title: "13. Retrieval Augmented Generation (RAG)",
            description: "RAG enhances the capabilities of LLMs by grounding them in external knowledge. Instead of relying solely on its training data, the model first retrieves relevant information from a knowledge base (like a set of documents or a database) and uses that information to generate a more accurate and up-to-date response.",
            html: `
            <div class="bg-white p-6 rounded-lg shadow-md">
                <h3 class="text-2xl font-semibold text-gray-800 mb-4 text-center">The RAG Process</h3>
                <div class="flex flex-col md:flex-row items-center justify-around gap-4 text-center">
                    <div id="rag-step-1" class="rag-step p-3 rounded-lg border-2 border-transparent">
                        <div class="text-4xl mb-2">ü§î</div><h4 class="font-bold">1. User Query</h4>
                    </div>
                    <div class="text-2xl font-bold text-gray-400 self-center">‚Üí</div>
                    <div id="rag-step-2" class="rag-step p-3 rounded-lg border-2 border-transparent">
                        <div class="text-4xl mb-2">üìö</div><h4 class="font-bold">2. Retrieve Docs</h4>
                    </div>
                    <div class="text-2xl font-bold text-gray-400 self-center">‚Üí</div>
                    <div id="rag-step-3" class="rag-step p-3 rounded-lg border-2 border-transparent">
                        <div class="text-4xl mb-2">üìù</div><h4 class="font-bold">3. Augment Prompt</h4>
                    </div>
                    <div class="text-2xl font-bold text-gray-400 self-center">‚Üí</div>
                    <div id="rag-step-4" class="rag-step p-3 rounded-lg border-2 border-transparent">
                        <div class="text-4xl mb-2">ü§ñ</div><h4 class="font-bold">4. Generate</h4>
                    </div>
                </div>
                <div id="rag-description" class="mt-6 p-4 bg-gray-100 rounded-lg text-center">
                    <p>Click the button to start the RAG cycle.</p>
                </div>
                <div class="text-center mt-6">
                    <button id="rag-next-btn" class="bg-[#8A6D53] text-white font-bold py-2 px-6 rounded-lg hover:bg-[#a18163]">Start Cycle</button>
                </div>
            </div>`
        },
        'topic-14': {
            title: "14. State of the Art Applications",
            description: "Modern NLP powers a wide array of applications that are becoming integral to our daily lives, from simple text summaries to complex conversational agents.",
            html: `
            <div class="bg-white p-6 rounded-lg shadow-md">
                <h3 class="text-2xl font-semibold text-gray-800 mb-4">Key Applications</h3>
                <div class="grid md:grid-cols-2 lg:grid-cols-3 gap-4">
                    <div class="p-4 bg-gray-50 rounded-lg">
                        <h4 class="font-bold text-lg">Text Summarization</h4>
                        <p class="text-sm">Automatically creating a short, coherent summary of a longer document.</p>
                    </div>
                    <div class="p-4 bg-gray-50 rounded-lg">
                        <h4 class="font-bold text-lg">Machine Translation</h4>
                        <p class="text-sm">Translating text from one language to another (e.g., Google Translate).</p>
                    </div>
                    <div class="p-4 bg-gray-50 rounded-lg">
                        <h4 class="font-bold text-lg">Sentiment Analysis</h4>
                        <p class="text-sm">Determining the emotional tone behind a piece of text (positive, negative, neutral).</p>
                    </div>
                     <div class="p-4 bg-gray-50 rounded-lg">
                        <h4 class="font-bold text-lg">Information Extraction</h4>
                        <p class="text-sm">Identifying and extracting specific pieces of information (like names, dates, locations) from text.</p>
                    </div>
                     <div class="p-4 bg-gray-50 rounded-lg">
                        <h4 class="font-bold text-lg">Question Answering</h4>
                        <p class="text-sm">Providing direct answers to questions posed in natural language.</p>
                    </div>
                     <div class="p-4 bg-gray-50 rounded-lg">
                        <h4 class="font-bold text-lg">Chatbots & Virtual Assistants</h4>
                        <p class="text-sm">Simulating human conversation to assist users with tasks.</p>
                    </div>
                </div>
            </div>`
        }
    };

    const dynamicContentContainer = document.getElementById('dynamic-content');
    Object.keys(content).forEach(key => {
        const topic = content[key];
        const section = document.createElement('section');
        section.id = key;
        section.className = "scroll-mt-20";
        section.innerHTML = `
            <h2 class="text-3xl font-semibold mb-4 text-[#8A6D53]">${topic.title}</h2>
            <p class="text-gray-700 mb-6">${topic.description}</p>
            <div>${topic.html}</div>
        `;
        dynamicContentContainer.appendChild(section);
    });

    if (window.renderMathInElement) {
        renderMathInElement(document.body, {
            delimiters: [ { left: '`', right: '`', display: false } ]
        });
    }

    // Mobile menu toggle
    const menuToggle = document.getElementById('menu-toggle');
    const sidebar = document.getElementById('sidebar');
    const overlay = document.getElementById('overlay');
    
    menuToggle.addEventListener('click', () => {
        sidebar.classList.toggle('-translate-x-full');
        overlay.classList.toggle('hidden');
        overlay.classList.toggle('opacity-0');
    });

    overlay.addEventListener('click', () => {
        sidebar.classList.add('-translate-x-full');
        overlay.classList.add('hidden');
        overlay.classList.add('opacity-0');
    });

    // Active nav link highlighting
    const sections = document.querySelectorAll('section');
    const navLinks = document.querySelectorAll('.nav-link');
    const observer = new IntersectionObserver((entries) => {
        entries.forEach(entry => {
            if (entry.isIntersecting) {
                navLinks.forEach(link => {
                    link.classList.remove('active-nav');
                    if (link.getAttribute('href').substring(1) === entry.target.id) {
                        link.classList.add('active-nav');
                    }
                });
            }
        });
    }, { rootMargin: '-40% 0px -60% 0px', threshold: 0 });
    sections.forEach(section => observer.observe(section));

    navLinks.forEach(link => {
        link.addEventListener('click', () => {
             if (!sidebar.classList.contains('-translate-x-full') && window.innerWidth < 768) {
                sidebar.classList.add('-translate-x-full');
                overlay.classList.add('hidden');
                overlay.classList.add('opacity-0');
             }
        });
    });

    // --- Interactive Module Logic ---

    function setupWordEmbeddingChart() {
        const ctx = document.getElementById('wordEmbeddingChart');
        if (!ctx) return;
        new Chart(ctx, {
            type: 'scatter',
            data: {
                datasets: [{
                    label: 'Words',
                    data: [
                        {x: 2, y: 8, label: 'king'}, {x: 8, y: 8, label: 'queen'},
                        {x: 2, y: 2, label: 'man'}, {x: 8, y: 2, label: 'woman'},
                        {x: 7, y: 3, label: 'girl'}, {x: 3, y: 3, label: 'boy'},
                        {x: 9, y: 9, label: 'princess'}, {x: 3, y: 9, label: 'prince'},
                    ],
                    backgroundColor: 'rgba(138, 109, 83, 0.7)'
                }]
            },
            options: {
                responsive: true, maintainAspectRatio: false,
                scales: {
                    x: { min: 0, max: 10, title: {display: true, text: 'Gender Dimension'} },
                    y: { min: 0, max: 10, title: {display: true, text: 'Royalty Dimension'} }
                },
                plugins: {
                    tooltip: { callbacks: { label: (context) => context.raw.label } }
                }
            }
        });
    }
    
    function setupBigramModel() {
        const input = document.getElementById('bigram-input');
        const output = document.getElementById('bigram-output');
        const button = document.getElementById('bigram-predict');
        if (!input || !output || !button) return;

        const bigrams = {
            'the': ['student', 'professor'],
            'student': ['reads'],
            'professor': ['writes']
        };

        const predict = () => {
            const word = input.value.toLowerCase().trim();
            if (bigrams[word]) {
                const prediction = bigrams[word][Math.floor(Math.random() * bigrams[word].length)];
                output.innerHTML = `After "<strong>${word}</strong>", a likely next word is "<strong>${prediction}</strong>". <br><small class="text-gray-500">P(${prediction} | ${word}) is high in our model.</small>`;
            } else {
                output.innerHTML = `Sorry, the model doesn't have a prediction for "<strong>${word}</strong>".`;
            }
        };
        button.addEventListener('click', predict);
        input.addEventListener('keyup', (e) => {
            if (e.key === 'Enter') predict();
        });
    }

    function setupPromptEngineering() {
        const cards = document.querySelectorAll('.prompt-card');
        const display = document.getElementById('prompt-display');
        if (!cards.length || !display) return;
        const prompts = {
            'zero-shot': '<strong>Prompt:</strong><br>Classify the following text into "positive", "negative", or "neutral".<br><br>Text: "The movie was okay, but the acting could have been better."<br>Sentiment:',
            'few-shot': '<strong>Prompt:</strong><br>Text: "I loved the new menu!"<br>Sentiment: positive<br><br>Text: "The service was terrible."<br>Sentiment: negative<br><br>Text: "The movie was okay, but the acting could have been better."<br>Sentiment:',
            'cot': '<strong>Prompt:</strong><br>Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?<br><br>A: Let\'s think step by step. Roger started with 5 balls. He bought 2 cans, and each can has 3 balls, so that\'s 2 * 3 = 6 new balls. Therefore, 5 + 6 = 11. The answer is 11.',
            'knowledge': '<strong>Prompt:</strong><br>Question: "Could a 1969 Ford Mustang use unleaded gasoline?"<br><br>Generated Knowledge: "Unleaded gasoline was widely introduced in the early 1970s. Cars manufactured before this period were typically designed for leaded gasoline to protect valve seats. Using unleaded fuel in older cars without hardened valve seats could cause engine wear over time."<br><br>Answer based on knowledge:'
        };
        cards.forEach(card => {
            card.addEventListener('click', () => {
                const promptType = card.dataset.prompt;
                display.innerHTML = prompts[promptType];
                cards.forEach(c => c.classList.remove('bg-yellow-100', 'ring-2', 'ring-yellow-400'));
                card.classList.add('bg-yellow-100', 'ring-2', 'ring-yellow-400');
            });
        });
    }

    function setupPosTagger() {
        const sentence = document.getElementById('pos-sentence');
        if (!sentence) return;
        sentence.querySelectorAll('.pos-tag').forEach(word => {
            const tag = word.dataset.tag;
            const tooltip = document.createElement('span');
            tooltip.className = 'tag-tooltip';
            tooltip.textContent = tag;
            word.appendChild(tooltip);
        });
    }

    function setupAttentionViz() {
        const container = document.getElementById('attention-viz');
        if (!container) return;
        const trigger = document.getElementById('attention-it');
        const words = Array.from(container.querySelectorAll('.attention-word'));

        const resetStyles = () => words.forEach(w => w.style.backgroundColor = '');

        trigger.addEventListener('mouseenter', () => {
            const scores = JSON.parse(trigger.dataset.scores);
            words.forEach((word, index) => {
                const score = scores[index] || 0;
                word.style.backgroundColor = `rgba(234, 179, 8, ${score})`;
            });
        });
        trigger.addEventListener('mouseleave', resetStyles);
    }
    
    function setupWSD() {
        const options = document.querySelectorAll('.wsd-btn');
        const feedback = document.getElementById('wsd-feedback');
        if(!options.length || !feedback) return;
        
        options.forEach(button => {
            button.addEventListener('click', () => {
                const isCorrect = button.dataset.correct === 'true';
                options.forEach(btn => btn.disabled = true);
                if (isCorrect) {
                    button.classList.add('bg-green-200', 'ring-2', 'ring-green-500');
                    feedback.textContent = "Correct! The context of 'river' indicates the land alongside it.";
                    feedback.className = "mt-4 font-bold text-green-700";
                } else {
                    button.classList.add('bg-red-200', 'ring-2', 'ring-red-500');
                    feedback.textContent = "Not quite. A financial institution doesn't fit the context.";
                    feedback.className = "mt-4 font-bold text-red-700";
                }
            });
        });
    }

    function setupRAG() {
        const steps = document.querySelectorAll('.rag-step');
        const desc = document.getElementById('rag-description');
        const nextBtn = document.getElementById('rag-next-btn');
        if (!steps.length || !desc || !nextBtn) return;
        
        let currentStep = 0;
        const descriptions = [
            "A user asks a question, e.g., 'What were the key findings of BITS Pilani's latest NLP research paper?'",
            "The system searches a knowledge base (e.g., a vector database of internal research papers) for documents relevant to the query.",
            "The original query and the retrieved document chunks are combined into a new, more informative prompt for the LLM.",
            "The LLM uses the provided context to generate a factual, grounded answer, avoiding hallucination."
        ];
        
        const updateStep = () => {
            steps.forEach((step, index) => {
                if (index + 1 === currentStep) {
                    step.classList.add('border-[#8A6D53]', 'scale-110', 'bg-yellow-50');
                } else {
                    step.classList.remove('border-[#8A6D53]', 'scale-110', 'bg-yellow-50');
                }
            });
            if(desc && descriptions[currentStep - 1]){
                desc.innerHTML = `<p>${descriptions[currentStep - 1]}</p>`;
            }
            if (nextBtn) nextBtn.textContent = 'Next Step';
        };

        nextBtn.addEventListener('click', () => {
            currentStep = (currentStep % steps.length) + 1;
            updateStep();
            if (currentStep === 4) nextBtn.textContent = 'Restart Cycle';
        });
    }

    // Call all setup functions
    setupWordEmbeddingChart();
    setupBigramModel();
    setupPromptEngineering();
    setupPosTagger();
    setupAttentionViz();
    setupWSD();
    setupRAG();
});
</script>
</body>
</html>


<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Interactive Reinforcement Learning Canvas</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #FAF8F5; /* Warm off-white */
            color: #3D4451; /* Soft dark gray for text */
        }
        aside {
            background-color: #F5F2ED; /* Beige sidebar */
        }
        .sidebar-item {
            border-left: 4px solid transparent;
            transition: all 0.2s ease-in-out;
        }
        .sidebar-item.active {
            background-color: #E9E4DB; /* Darker beige */
            color: #000000;
            border-left-color: #4A90E2; /* Accent blue */
            font-weight: 600;
        }
        .sidebar-item:not(.active):hover {
            background-color: #F0ECE5; /* Lighter beige for hover */
        }
        .content-section {
            display: none;
        }
        .content-section.active {
            display: block;
        }
        .card {
            background-color: #ffffff;
            border: 1px solid #EAEAEA;
        }
        canvas {
            background-color: #fcfcfc;
            border-radius: 0.5rem;
            border: 1px solid #e5e7eb;
        }
        .formula {
            background-color: #F5F2ED;
            padding: 1rem;
            border-radius: 0.5rem;
            margin-top: 0.5rem;
            text-align: center;
            font-size: 1.1rem;
            overflow-x: auto;
        }
        .quiz-feedback {
            padding: 0.5rem 1rem;
            border-radius: 0.375rem;
            margin-top: 1rem;
            font-weight: 500;
        }
        .quiz-feedback.correct {
            background-color: #dcfce7; /* green-100 */
            color: #166534; /* green-800 */
        }
        .quiz-feedback.incorrect {
            background-color: #fee2e2; /* red-100 */
            color: #991b1b; /* red-800 */
        }
        .btn-primary {
             background-color: #4A90E2;
             color: white;
        }
        .btn-primary:hover {
            background-color: #357ABD;
        }
         .btn-secondary {
            background-color: #7DBA31;
             color: white;
        }
        .btn-secondary:hover {
            background-color: #689D2A;
        }
        .btn-danger {
            background-color: #D0021B;
            color: white;
        }
        .btn-danger:hover {
            background-color: #B00216;
        }
        .td-q-value {
            font-size: 10px;
            position: absolute;
            color: #555;
            font-family: monospace;
        }
        .solution-link {
            color: #4A90E2;
            text-decoration: underline;
            cursor: pointer;
        }
    </style>
</head>
<body class="flex min-h-screen">

    <!-- Sidebar Navigation -->
    <aside id="sidebar" class="w-64 flex-shrink-0 p-6 overflow-y-auto h-screen sticky top-0 border-r border-gray-200">
        <h2 class="text-xl font-bold text-[#8A6D53] mb-2 px-4">DRL Guide</h2>
        <a href="https://jeringk.github.io/aiml" class="block text-sm text-[#8A6D53] hover:underline mb-4 px-4">← Back to AIML Home</a>
        <nav id="sidebar-nav" class="space-y-1">
            <!-- Navigation items will be injected by JS -->
        </nav>
    </aside>

    <!-- Main Content -->
    <main id="main-content" class="flex-1 p-8 lg:p-12 overflow-y-auto">
        <!-- Content sections will be injected here -->
    </main>

    <script>
        document.addEventListener('DOMContentLoaded', () => {
            const topics = [
                { id: 'intro-rl', title: 'Introduction to RL', content: `
                    <h2 class="text-3xl font-bold text-gray-900 mb-4 pb-2 border-b border-gray-200">Introduction to Reinforcement Learning (RL)</h2>
                    <p class="text-lg text-gray-700 mb-6">Reinforcement Learning is an area of machine learning where an agent learns to make decisions by performing actions in an environment to maximize cumulative reward. Unlike supervised learning, the agent is not told which actions to take, but instead must discover which actions yield the most reward by trying them.</p>
                    <div class="card p-6 rounded-lg">
                        <h3 class="text-xl font-semibold text-gray-900 mb-3">The Agent-Environment Loop</h3>
                        <p class="text-gray-600 mb-4">The core of RL is the interaction loop between the agent and the environment. At each step, the agent observes a state, takes an action, receives a reward, and observes the next state. This process continues, and the agent's goal is to learn a strategy, or policy, that maximizes its total reward over time.</p>
                        <ul class="list-disc list-inside text-gray-600 space-y-2">
                            <li><strong>Agent:</strong> The learner or decision-maker.</li>
                            <li><strong>Environment:</strong> Everything the agent interacts with.</li>
                            <li><strong>Action (A<sub>t</sub>):</strong> A choice made by the agent.</li>
                            <li><strong>State (S<sub>t</sub>):</strong> The agent's current situation.</li>
                            <li><strong>Reward (R<sub>t</sub>):</strong> Immediate feedback from the environment.</li>
                        </ul>
                    </div>
                    <div class="mt-6 card p-6 rounded-lg quiz-container" data-quiz-id="intro-rl-quiz" data-correct-answer="b">
                        <h3 class="text-xl font-semibold text-gray-900 mb-3">Quick Quiz</h3>
                        <p class="font-medium text-gray-700 mb-2">What is the primary goal of a Reinforcement Learning agent?</p>
                        <div class="space-y-2">
                            <label class="flex items-center"><input type="radio" name="intro-rl-quiz" value="a" class="mr-2"> Minimize computation time.</label>
                            <label class="flex items-center"><input type="radio" name="intro-rl-quiz" value="b" class="mr-2"> Maximize cumulative reward.</label>
                            <label class="flex items-center"><input type="radio" name="intro-rl-quiz" value="c" class="mr-2"> Classify data accurately.</label>
                            <label class="flex items-center"><input type="radio" name="intro-rl-quiz" value="d" class="mr-2"> Follow a predefined path.</label>
                        </div>
                        <button class="quiz-submit-btn mt-4 btn-primary font-semibold py-2 px-4 rounded">Check Answer</button>
                        <div class="quiz-feedback hidden"></div>
                    </div>
                `},
                { id: 'elements-rl', title: 'Elements of RL', content: `
                    <h2 class="text-3xl font-bold text-gray-900 mb-4 pb-2 border-b border-gray-200">Elements of Reinforcement Learning</h2>
                    <div class="space-y-6">
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Policy (π)</h3>
                            <p class="text-gray-600 mb-4">The agent's strategy for selecting actions. It can be deterministic (a specific action for a state) or stochastic (a probability distribution over actions).</p>
                            <div class="formula" data-katex="a = \\pi(s) \\quad \\text{(deterministic)}"></div>
                            <div class="formula mt-2" data-katex="\\pi(a|s) = P[A_t=a | S_t=s] \\quad \\text{(stochastic)}"></div>
                        </div>
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Reward Signal (R)</h3>
                            <p class="text-gray-600">A scalar feedback from the environment indicating the immediate benefit of an action. The agent's sole objective is to maximize the total reward it receives.</p>
                        </div>
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Value Function (V, Q)</h3>
                            <p class="text-gray-600">The expected long-term return from a state or state-action pair, following a particular policy. It quantifies the "goodness" of a state. The return $G_t$ is the sum of discounted future rewards.</p>
                            <div class="formula" data-katex="\\begin{aligned}G_t &= R_{t+1} + \\gamma R_{t+2} + \\dots \\\\ &= \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\\end{aligned}"></div>
                        </div>
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Model of the Environment</h3>
                            <p class="text-gray-600">The agent’s representation of how the environment behaves. It predicts state transitions ($P(s'|s,a)$) and rewards ($R(s,a,s')$). RL methods can be model-based or model-free.</p>
                        </div>
                    </div>
                    <div class="mt-6 card p-6 rounded-lg quiz-container" data-quiz-id="elements-rl-quiz" data-correct-answer="c">
                        <h3 class="text-xl font-semibold text-gray-900 mb-3">Quick Quiz</h3>
                        <p class="font-medium text-gray-700 mb-2">Which element of RL defines the agent's strategy or "brain"?</p>
                        <div class="space-y-2">
                            <label class="flex items-center"><input type="radio" name="elements-rl-quiz" value="a" class="mr-2"> Reward Signal</label>
                            <label class="flex items-center"><input type="radio" name="elements-rl-quiz" value="b" class="mr-2"> Value Function</label>
                            <label class="flex items-center"><input type="radio" name="elements-rl-quiz" value="c" class="mr-2"> Policy</label>
                            <label class="flex items-center"><input type="radio" name="elements-rl-quiz" value="d" class="mr-2"> Model</label>
                        </div>
                        <button class="quiz-submit-btn mt-4 btn-primary font-semibold py-2 px-4 rounded">Check Answer</button>
                        <div class="quiz-feedback hidden"></div>
                    </div>
                `},
                { id: 'mab', title: 'Multi-Armed Bandit', content: `
                    <h2 class="text-3xl font-bold text-gray-900 mb-4 pb-2 border-b border-gray-200">Multi-Armed Bandit Problem</h2>
                    <p class="text-lg text-gray-700 mb-6">The MAB problem exemplifies the exploration-exploitation tradeoff. The agent must choose between multiple options (arms) to maximize total reward, without knowing which option is best. It must 'exploit' the best-known option but also 'explore' others to find potentially better ones.</p>
                    <div class="space-y-6">
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Action-Value Estimation & ε-Greedy</h3>
                            <p class="text-gray-600">We estimate the value of each action $Q(a)$ by averaging rewards. The ε-Greedy policy balances exploring random actions with exploiting the current best action.</p>
                            <div class="formula" data-katex="Q_{n+1} = Q_n + \\frac{1}{n}(R_n - Q_n)"></div>
                        </div>
                        <div class="card p-6 rounded-lg">
                             <h3 class="text-xl font-semibold text-gray-900 mb-2">Bandit Gradient Algorithm</h3>
                             <p class="text-gray-600">Instead of estimating action values, this method learns a preference $H_t(a)$ for each action. Preferences are updated via stochastic gradient ascent. Actions are then selected via a softmax distribution over preferences.</p>
                             <div class="formula" data-katex="H_{t+1}(A_t) = H_t(A_t) + \\alpha(R_t - \\bar{R}_t)(1 - \\pi_t(A_t))"></div>
                        </div>
                         <div class="card p-6 rounded-lg">
                             <h3 class="text-xl font-semibold text-gray-900 mb-2">Associative Search (Contextual Bandits)</h3>
                             <p class="text-gray-600">This is an extension where the agent is given a "context" or state before making a choice. The goal is to learn the best action for each context, effectively learning a policy $\\pi(a|s)$ instead of just finding the single best action overall.</p>
                        </div>
                    </div>
                    <div class="card p-6 rounded-lg mt-6">
                        <h3 class="text-xl font-semibold text-gray-900 mt-6 mb-4">Interactive Bandit Simulation (ε-Greedy)</h3>
                        <div class="flex items-center space-x-4 mb-4">
                            <label for="epsilon-slider" class="text-gray-600">Exploration Rate (ε):</label>
                            <input type="range" id="epsilon-slider" min="0" max="1" step="0.01" value="0.1" class="w-48">
                            <span id="epsilon-value" class="font-mono text-blue-600">0.10</span>
                            <button id="reset-bandit-btn" class="btn-danger font-semibold py-2 px-4 rounded">Reset</button>
                        </div>
                        <div id="bandit-arms-container" class="flex justify-around items-end h-64 p-4 bg-gray-50 rounded-lg"></div>
                        <div class="mt-4 grid grid-cols-2 gap-4 text-center">
                            <div><p class="text-gray-600">Total Reward</p><p id="bandit-total-reward" class="text-2xl font-bold text-gray-900">0</p></div>
                            <div><p class="text-gray-600">Total Pulls</p><p id="bandit-total-pulls" class="text-2xl font-bold text-gray-900">0</p></div>
                        </div>
                    </div>
                     <div class="mt-6 card p-6 rounded-lg quiz-container" data-quiz-id="mab-quiz" data-correct-answer="b">
                        <h3 class="text-xl font-semibold text-gray-900 mb-3">Quick Quiz</h3>
                        <p class="font-medium text-gray-700 mb-2">What fundamental tradeoff is the Multi-Armed Bandit problem designed to illustrate?</p>
                        <div class="space-y-2">
                            <label class="flex items-center"><input type="radio" name="mab-quiz" value="a" class="mr-2"> Bias vs. Variance</label>
                            <label class="flex items-center"><input type="radio" name="mab-quiz" value="b" class="mr-2"> Exploration vs. Exploitation</label>
                            <label class="flex items-center"><input type="radio" name="mab-quiz" value="c" class="mr-2"> Speed vs. Accuracy</label>
                            <label class="flex items-center"><input type="radio" name="mab-quiz" value="d" class="mr-2"> Model-based vs. Model-free</label>
                        </div>
                        <button class="quiz-submit-btn mt-4 btn-primary font-semibold py-2 px-4 rounded">Check Answer</button>
                        <div class="quiz-feedback hidden"></div>
                    </div>
                `},
                { id: 'mdp', title: 'Markov Decision Processes', content: `
                    <h2 class="text-3xl font-bold text-gray-900 mb-4 pb-2 border-b border-gray-200">(Finite) Markov Decision Processes (MDP)</h2>
                    <p class="text-lg text-gray-700 mb-6">An MDP is a formal framework for RL problems. A key assumption is the Markov Property: the future is independent of the past given the present. An MDP is defined by a tuple $(S, A, P, R, \\gamma)$.</p>
                    <div class="space-y-6">
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Value Functions & Bellman Equations</h3>
                            <p class="text-gray-600">The Bellman expectation equation expresses the value of a state in terms of the values of successor states, creating a recursive relationship:</p>
                            <div class="formula" data-katex="\\begin{aligned}v_\\pi(s) = \\sum_{a} \\pi(a|s) \\sum_{s', r} & p(s', r | s, a) \\\\ & \\cdot [r + \\gamma v_\\pi(s')].\\end{aligned}"></div>
                        </div>
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Optimal Value Functions & Policies</h3>
                            <p class="text-gray-600">An optimal policy $\\pi_*$ is one that maximizes the expected return from any state. There is always at least one such policy. Optimal policies share the same optimal value functions, $v_*(s)$ and $q_*(s,a)$. The Bellman optimality equations show how they relate:</p>
                            <div class="formula" data-katex="v_*(s) = \\max_a \\sum_{s', r} p(s', r | s, a) [r + \\gamma v_*(s')]"></div>
                        </div>
                    </div>
                    <div class="card p-6 rounded-lg mt-6">
                        <h3 class="text-xl font-semibold text-gray-900 mt-6 mb-4">Interactive Gridworld</h3>
                        <p class="text-gray-600 mb-4">This Gridworld is an MDP. The agent learns the value of each state by iteratively applying the Bellman equation in an algorithm called Value Iteration.</p>
                        <canvas id="gridworld-canvas" width="400" height="400"></canvas>
                        <div class="mt-4 flex space-x-4">
                            <button id="gridworld-vi-btn" class="btn-primary font-semibold py-2 px-4 rounded">Run Value Iteration</button>
                            <button id="gridworld-reset-btn" class="bg-gray-200 hover:bg-gray-300 text-gray-800 font-semibold py-2 px-4 rounded">Reset</button>
                        </div>
                         <div class="mt-4"><p id="gridworld-iterations" class="text-gray-600">Iterations: 0</p></div>
                    </div>
                     <div class="mt-6 card p-6 rounded-lg quiz-container" data-quiz-id="mdp-quiz" data-correct-answer="c">
                        <h3 class="text-xl font-semibold text-gray-900 mb-3">Quick Quiz</h3>
                        <p class="font-medium text-gray-700 mb-2">The Bellman equation provides a recursive relationship for which of the following?</p>
                        <div class="space-y-2">
                            <label class="flex items-center"><input type="radio" name="mdp-quiz" value="a" class="mr-2"> The policy</label>
                            <label class="flex items-center"><input type="radio" name="mdp-quiz" value="b" class="mr-2"> The reward signal</label>
                            <label class="flex items-center"><input type="radio" name="mdp-quiz" value="c" class="mr-2"> The value function</label>
                            <label class="flex items-center"><input type="radio" name="mdp-quiz" value="d" class="mr-2"> The state transitions</label>
                        </div>
                        <button class="quiz-submit-btn mt-4 btn-primary font-semibold py-2 px-4 rounded">Check Answer</button>
                        <div class="quiz-feedback hidden"></div>
                    </div>
                `},
                { id: 'dp', title: 'Dynamic Programming', content: `
                    <h2 class="text-3xl font-bold text-gray-900 mb-4 pb-2 border-b border-gray-200">Dynamic Programming in RL</h2>
                    <p class="text-lg text-gray-700 mb-6">Dynamic Programming (DP) refers to a collection of algorithms that can compute optimal policies given a perfect model of the environment as a Markov Decision Process (MDP). The key idea is to use value functions to organize and structure the search for good policies. DP methods are powerful but are limited by the need for a full model and their computational expense.</p>
                    <div class="space-y-6">
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Policy Iteration</h3>
                            <p class="text-gray-600">This method finds an optimal policy by alternating between two processes: policy evaluation and policy improvement.</p>
                            <ul class="list-decimal list-inside text-gray-600 space-y-2 mt-4">
                                <li><strong>Policy Evaluation:</strong> For the current policy $\\pi$, we compute the state-value function $v_\\pi$. This is done by iteratively applying the Bellman expectation equation until the values converge. For each state $s$:</li>
                                <div class="formula" data-katex="v_{k+1}(s) = \\sum_{a} \\pi(a|s) \\sum_{s', r} p(s', r | s, a) [r + \\gamma v_k(s')]"></div>
                                <li><strong>Policy Improvement:</strong> Once we have $v_\\pi$, we improve the policy by acting greedily with respect to it. For each state $s$, we find the action that maximizes the expected return:</li>
                                <div class="formula" data-katex="\\pi'(s) = \\arg\\max_a \\sum_{s', r} p(s', r | s, a) [r + \\gamma v_\\pi(s')]"></div>
                            </ul>
                            <p class="text-gray-600 mt-4">This process is guaranteed to converge to an optimal policy.</p>
                        </div>
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Value Iteration</h3>
                            <p class="text-gray-600">Value Iteration combines policy evaluation and improvement into a single step. It directly finds the optimal value function by iteratively applying the Bellman Optimality update rule. The process is simpler but each iteration is more complex than in policy evaluation.</p>
                            <div class="formula" data-katex="v_{k+1}(s) = \\max_a \\sum_{s', r} p(s', r | s, a) [r + \\gamma v_k(s')]"></div>
                            <p class="text-gray-600 mt-4">Once the value function converges to $v_*$, the optimal policy can be extracted by choosing the action that maximizes the expected return in each state.</p>
                        </div>
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Generalized Policy Iteration (GPI)</h3>
                            <p class="text-gray-600">GPI is the general idea of letting policy evaluation and policy improvement processes interact. One process learns the value function for the current policy, and the other improves the policy based on the current value function. Most RL algorithms can be framed as GPI, where the two processes may not fully complete before the other begins (e.g., updating values only once before improving the policy).</p>
                            <div class="text-center my-4">
                                <svg width="250" height="200" class="mx-auto">
                                    <defs><marker id="arrow" viewBox="0 0 10 10" refX="5" refY="5" markerWidth="6" markerHeight="6" orient="auto-start-reverse"><path d="M 0 0 L 10 5 L 0 10 z" fill="#4A90E2"/></marker></defs>
                                    <ellipse cx="125" cy="50" rx="60" ry="25" fill="#E9E4DB" stroke="#3D4451"/>
                                    <text x="125" y="55" text-anchor="middle" font-size="16">Policy π</text>
                                    <ellipse cx="125" cy="150" rx="60" ry="25" fill="#E9E4DB" stroke="#3D4451"/>
                                    <text x="125" y="155" text-anchor="middle" font-size="16">Value V</text>
                                    <path d="M 125 75 Q 175 100 125 125" stroke="#4A90E2" stroke-width="2" fill="none" marker-end="url(#arrow)"/>
                                    <text x="165" y="95" fill="#4A90E2" font-size="12">Evaluation</text>
                                    <path d="M 125 125 Q 75 100 125 75" stroke="#D0021B" stroke-width="2" fill="none" marker-end="url(#arrow)"/>
                                    <text x="80" y="95" fill="#D0021B" font-size="12" text-anchor="end">Improvement</text>
                                </svg>
                                <p class="text-sm text-gray-500">The policy and value function interact until they are optimal and consistent.</p>
                            </div>
                        </div>
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Efficiency of Dynamic Programming</h3>
                            <p class="text-gray-600">While DP methods are guaranteed to find optimal policies, their practicality is limited. They require a full model of the environment's dynamics, which is often unavailable. Furthermore, they suffer from the <strong>curse of dimensionality</strong>: the state space (and thus the computation required) grows exponentially with the number of state variables. This makes them infeasible for problems with large or continuous state spaces.</p>
                        </div>
                    </div>
                    <div class="mt-6 card p-6 rounded-lg quiz-container" data-quiz-id="dp-quiz" data-correct-answer="a">
                        <h3 class="text-xl font-semibold text-gray-900 mb-3">Quick Quiz</h3>
                        <p class="font-medium text-gray-700 mb-2">What is a key requirement for using Dynamic Programming to solve an RL problem?</p>
                        <div class="space-y-2">
                            <label class="flex items-center"><input type="radio" name="dp-quiz" value="a" class="mr-2"> A perfect model of the environment (MDP).</label>
                            <label class="flex items-center"><input type="radio" name="dp-quiz" value="b" class="mr-2"> A continuous state space.</label>
                            <label class="flex items-center"><input type="radio" name="dp-quiz" value="c" class="mr-2"> No knowledge of the environment.</label>
                            <label class="flex items-center"><input type="radio" name="dp-quiz" value="d" class="mr-2"> Experience from complete episodes.</label>
                        </div>
                        <button class="quiz-submit-btn mt-4 btn-primary font-semibold py-2 px-4 rounded">Check Answer</button>
                        <div class="quiz-feedback hidden"></div>
                    </div>
                `},
                { id: 'mc', title: 'Monte Carlo Methods', content: `
                    <h2 class="text-3xl font-bold text-gray-900 mb-4 pb-2 border-b border-gray-200">Monte Carlo Methods</h2>
                    <p class="text-lg text-gray-700 mb-6">Monte Carlo (MC) methods are a class of model-free algorithms that learn directly from episodes of experience. Unlike DP, they do not require a model of the environment's dynamics. MC methods wait until the end of an episode to make value-function updates, based on the actual observed return.</p>
                    <div class="space-y-6">
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Monte Carlo Prediction (Policy Evaluation)</h3>
                            <p class="text-gray-600">MC Prediction is used to estimate $v_\\pi(s)$ for a given policy $\\pi$. The value of a state is estimated by averaging the returns that have followed visits to that state. There are two main variants:</p>
                             <ul class="list-disc list-inside text-gray-600 space-y-2 mt-4">
                                <li><strong>First-visit MC:</strong> Averages the returns for the first time a state $s$ is visited in each episode.</li>
                                <li><strong>Every-visit MC:</strong> Averages the returns for every visit to a state $s$ in each episode.</li>
                             </ul>
                             <p class="text-gray-600 mt-2">The update rule is simple: after an episode provides a return $G_t$ following a visit to $S_t$:</p>
                            <div class="formula" data-katex="V(S_t) \\leftarrow V(S_t) + \\alpha (G_t - V(S_t))"></div>
                            <p class="text-gray-600 mt-2">Where $\\alpha$ is a constant step-size parameter.</p>
                        </div>
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Monte Carlo Control</h3>
                            <p class="text-gray-600">MC Control uses the GPI framework to find an optimal policy. It learns action-values $q_\\pi(s,a)$ instead of state-values to make policy improvement possible (since we don't have a model). The loop is similar to Policy Iteration:</p>
                            <ol class="list-decimal list-inside text-gray-600 space-y-1 mt-2">
                                <li>Run an episode following the current policy $\\pi$.</li>
                                <li>For each state-action pair $(s,a)$ in the episode, update its Q-value by averaging returns.</li>
                                <li>Improve the policy by making it $\\epsilon$-greedy with respect to the new Q-values.</li>
                            </ol>
                            <p class="text-gray-600 mt-2">To ensure sufficient exploration, we can use an $\\epsilon$-greedy policy or exploring starts.</p>
                        </div>
                         <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Off-Policy Learning via Importance Sampling</h3>
                            <p class="text-gray-600">Off-policy methods learn about a target policy $\\pi$ while following a different, more exploratory behavior policy $b$. This is crucial for control problems. To correct for the fact that the data comes from $b$, we weight the returns by the <strong>importance sampling ratio</strong>, which is the relative probability of the trajectory under the two policies:</p>
                            <div class="formula" data-katex="\\rho_{t:T-1} = \\prod_{k=t}^{T-1} \\frac{\\pi(A_k|S_k)}{b(A_k|S_k)}"></div>
                            <p class="text-gray-600 mt-2"><strong>Weighted Importance Sampling</strong> is often preferred in practice because it has lower variance. It uses a weighted average for the value function update:</p>
                            <div class="formula" data-katex="V(S_t) \\leftarrow V(S_t) + \\frac{W_t}{C(S_t)}(G_t - V(S_t))"></div>
                            <p class="text-gray-600 mt-2 text-center">where $C(S_t)$ is the sum of weights for state $S_t$.</p>
                        </div>
                    </div>
                    <div class="card p-6 rounded-lg mt-6">
                        <h3 class="text-xl font-semibold text-gray-900 mt-6 mb-4">Interactive MC Example: Blackjack</h3>
                        <p class="text-gray-600 mb-4">Here, an agent learns to play Blackjack using Monte Carlo. It follows a simple policy (stick on 20 or 21, hit otherwise) and learns the value of each state by averaging returns over many games. The state is defined by the player's sum, the dealer's showing card, and whether the player has a usable ace.</p>
                        <div class="flex items-center space-x-4 mb-4">
                            <button id="blackjack-play1-btn" class="btn-primary font-semibold py-2 px-4 rounded">Play 1 Game</button>
                            <button id="blackjack-play100-btn" class="btn-secondary font-semibold py-2 px-4 rounded">Play 100 Games</button>
                            <button id="blackjack-reset-btn" class="btn-danger font-semibold py-2 px-4 rounded">Reset</button>
                            <span id="blackjack-games-played" class="text-gray-600">Games Played: 0</span>
                        </div>
                        <canvas id="blackjack-canvas" width="600" height="350"></canvas>
                    </div>
                    <div class="mt-6 card p-6 rounded-lg quiz-container" data-quiz-id="mc-quiz" data-correct-answer="c">
                        <h3 class="text-xl font-semibold text-gray-900 mb-3">Quick Quiz</h3>
                        <p class="font-medium text-gray-700 mb-2">What is the main advantage of Monte Carlo methods over Dynamic Programming?</p>
                        <div class="space-y-2">
                            <label class="flex items-center"><input type="radio" name="mc-quiz" value="a" class="mr-2"> They are guaranteed to be faster.</label>
                            <label class="flex items-center"><input type="radio" name="mc-quiz" value="b" class="mr-2"> They update values after every step.</label>
                            <label class="flex items-center"><input type="radio" name="mc-quiz" value="c" class="mr-2"> They do not require a model of the environment.</label>
                            <label class="flex items-center"><input type="radio" name="mc-quiz" value="d" class="mr-2"> They have lower variance.</label>
                        </div>
                        <button class="quiz-submit-btn mt-4 btn-primary font-semibold py-2 px-4 rounded">Check Answer</button>
                        <div class="quiz-feedback hidden"></div>
                    </div>
                `},
                { id: 'td', title: 'Temporal Difference Learning', content: `
                    <h2 class="text-3xl font-bold text-gray-900 mb-4 pb-2 border-b border-gray-200">Temporal Difference (TD) Learning</h2>
                    <p class="text-lg text-gray-700 mb-6">TD learning is a central and novel idea in RL. It's a combination of Monte Carlo and Dynamic Programming. Like MC, it learns from raw experience without a model. Like DP, it bootstraps—it updates value estimates based on other learned estimates, without waiting for the final outcome.</p>
                    <div class="space-y-6">
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">TD Prediction</h3>
                            <p class="text-gray-600">TD Prediction estimates $v_\\pi$. The simplest method, TD(0), updates the value of a state $S_t$ towards a <strong>TD Target</strong> ($R_{t+1} + \\gamma V(S_{t+1})$), which is an estimate of the return. The difference between the current value and the target is the <strong>TD Error</strong>.</p>
                            <div class="formula" data-katex="\\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)"></div>
                            <p class="text-gray-600 mt-2">The update rule then becomes:</p>
                             <div class="formula" data-katex="V(S_t) \\leftarrow V(S_t) + \\alpha \\delta_t"></div>
                             <p class="text-gray-600 mt-2">This is powerful because the agent learns after just one step, rather than waiting for the end of an episode.</p>
                        </div>
                        <div class="card p-6 rounded-lg">
                             <h3 class="text-xl font-semibold text-gray-900 mb-2">SARSA: On-Policy TD Control</h3>
                            <p class="text-gray-600">SARSA learns action-values $Q(s,a)$ and uses them to improve the policy. It is <strong>on-policy</strong> because it learns about the policy it is currently following (including its exploratory moves). Its name comes from the quintuple of experience it uses for updates: $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$.</p>
                            <div class="formula" data-katex="Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]"></div>
                             <p class="text-gray-600 mt-2">The target for the update, $Q(S_{t+1}, A_{t+1})$, uses the *actual* action $A_{t+1}$ taken in the next state.</p>
                        </div>
                        <div class="card p-6 rounded-lg">
                             <h3 class="text-xl font-semibold text-gray-900 mb-2">Q-Learning: Off-Policy TD Control</h3>
                            <p class="text-gray-600">Q-learning is a breakthrough off-policy algorithm. It learns the optimal action-value function, $q_*$, regardless of the policy being followed. This allows the agent to explore freely while still learning about the greedy, optimal policy.</p>
                             <div class="formula" data-katex="Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma \\max_{a} Q(S_{t+1}, a) - Q(S_t, A_t)]"></div>
                             <p class="text-gray-600 mt-2">The key difference from SARSA is the $\\max_a$ operator. Q-learning uses the value of the best possible next action to update its current Q-value, rather than the action that was actually taken.</p>
                        </div>
                         <div class="card p-6 rounded-lg">
                             <h3 class="text-xl font-semibold text-gray-900 mb-2">Expected SARSA</h3>
                             <p class="text-gray-600">A more sophisticated version of SARSA that, like Q-learning, is less sensitive to the variance of the next action. Instead of using the Q-value of the specific next action $A_{t+1}$, it uses the *expected* Q-value over all possible next actions, weighted by their probabilities under the current policy $\\pi$.</p>
                            <div class="formula" data-katex="Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[R_{t+1} + \\gamma \\sum_{a} \\pi(a|S_{t+1}) Q(S_{t+1}, a) - Q(S_t, A_t)\\right]"></div>
                        </div>
                    </div>
                     <div class="card p-6 rounded-lg mt-6">
                        <h3 class="text-xl font-semibold text-gray-900 mt-6 mb-4">Interactive TD Gridworld</h3>
                        <p class="text-gray-600 mb-4">Watch an agent learn a path through the grid using TD methods. It has no model; it learns from each step. Notice the difference in the learned Q-values between SARSA (which accounts for exploratory moves) and Q-Learning (which always learns about the greedy path).</p>
                        <div class="flex items-center space-x-4 mb-4">
                            <label class="font-medium">Algorithm:</label>
                            <select id="td-algo-select" class="rounded-md border-gray-300">
                                <option value="sarsa">SARSA</option>
                                <option value="q-learning">Q-Learning</option>
                            </select>
                            <button id="td-step-btn" class="btn-primary font-semibold py-2 px-4 rounded">Take Step</button>
                             <button id="td-play-btn" class="btn-secondary font-semibold py-2 px-4 rounded">Auto-Play</button>
                            <button id="td-reset-btn" class="btn-danger font-semibold py-2 px-4 rounded">Reset</button>
                        </div>
                        <div id="td-gridworld-container" class="relative" style="width: 400px; height: 400px;">
                            <canvas id="td-gridworld-canvas" width="400" height="400"></canvas>
                        </div>
                         <div id="td-info" class="mt-2 text-sm text-gray-600"></div>
                    </div>
                     <div class="mt-6 card p-6 rounded-lg quiz-container" data-quiz-id="td-quiz" data-correct-answer="d">
                        <h3 class="text-xl font-semibold text-gray-900 mb-3">Quick Quiz</h3>
                        <p class="font-medium text-gray-700 mb-2">What is the key difference in the update rule between SARSA and Q-Learning?</p>
                        <div class="space-y-2">
                            <label class="flex items-center"><input type="radio" name="td-quiz" value="a" class="mr-2"> SARSA uses a smaller learning rate.</label>
                            <label class="flex items-center"><input type="radio" name="td-quiz" value="b" class="mr-2"> Q-Learning does not use a discount factor.</label>
                            <label class="flex items-center"><input type="radio" name="td-quiz" value="c" class="mr-2"> SARSA learns state-values while Q-Learning learns action-values.</label>
                            <label class="flex items-center"><input type="radio" name="td-quiz" value="d" class="mr-2"> Q-Learning uses the max Q-value of the next state for its update target.</label>
                        </div>
                        <button class="quiz-submit-btn mt-4 btn-primary font-semibold py-2 px-4 rounded">Check Answer</button>
                        <div class="quiz-feedback hidden"></div>
                    </div>
                `},
                 { id: 'func-approx', title: 'Function Approximation', content: `
                    <h2 class="text-3xl font-bold text-gray-900 mb-4 pb-2 border-b border-gray-200">Function Approximation</h2>
                     <p class="text-lg text-gray-700 mb-6">For problems with large or continuous state spaces, we approximate the value function with a parameterized function $\\hat{v}(s, \\mathbf{w})$. This allows generalization across states.</p>
                     <div class="space-y-6">
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Feature Construction & Tile Coding</h3>
                            <p class="text-gray-600">For linear methods, we must construct a feature vector $\\mathbf{x}(s)$ for each state. <strong>Tile coding</strong> is a popular method for continuous spaces, where the space is overlaid with multiple tilings. Each tile acts as a binary feature, allowing for coarse but effective generalization.</p>
                        </div>
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Semi-Gradient TD Methods</h3>
                            <p class="text-gray-600">We use gradient-descent to update the function's weight vector $\\mathbf{w}$. It's called "semi-gradient" because it ignores the effect of changing $\\mathbf{w}$ on the TD target, but works well in practice.</p>
                            <div class="formula" data-katex="\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha [R + \\gamma \\hat{v}(S', \\mathbf{w}) - \\hat{v}(S, \\mathbf{w})] \\nabla \\hat{v}(S, \\mathbf{w})"></div>
                        </div>
                         <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Off-Policy Function Approximation</h3>
                            <p class="text-gray-600">Combining off-policy methods (like Q-Learning) with function approximation is challenging. The combination of bootstrapping, off-policy training, and function approximation is known as the "deadly triad" and can lead to instability and divergence.</p>
                        </div>
                    </div>
                     <div class="mt-6 card p-6 rounded-lg quiz-container" data-quiz-id="fa-quiz" data-correct-answer="b">
                        <h3 class="text-xl font-semibold text-gray-900 mb-3">Quick Quiz</h3>
                        <p class="font-medium text-gray-700 mb-2">Why is function approximation necessary in some RL problems?</p>
                        <div class="space-y-2">
                            <label class="flex items-center"><input type="radio" name="fa-quiz" value="a" class="mr-2"> To make the rewards higher.</label>
                            <label class="flex items-center"><input type="radio" name="fa-quiz" value="b" class="mr-2"> To handle very large or continuous state spaces.</label>
                            <label class="flex items-center"><input type="radio" name="fa-quiz" value="c" class="mr-2"> To guarantee finding the optimal policy.</label>
                            <label class="flex items-center"><input type="radio" name="fa-quiz" value="d" class="mr-2"> To eliminate the need for a discount factor.</label>
                        </div>
                        <button class="quiz-submit-btn mt-4 btn-primary font-semibold py-2 px-4 rounded">Check Answer</button>
                        <div class="quiz-feedback hidden"></div>
                    </div>
                `},
                 { id: 'deep-rl', title: 'Deep Reinforcement Learning', content: `
                    <h2 class="text-3xl font-bold text-gray-900 mb-4 pb-2 border-b border-gray-200">Deep Reinforcement Learning</h2>
                     <p class="text-lg text-gray-700 mb-6">Deep RL uses deep neural networks as function approximators, allowing agents to learn from high-dimensional sensory inputs like images.</p>
                     <div class="space-y-6">
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Deep Q-Network (DQN)</h3>
                            <p class="text-gray-600">DQN stabilized learning by using two key techniques: Experience Replay and Fixed Target Networks. Experience replay stores transitions and samples them randomly to break correlations. A fixed target network provides stable targets for the Q-value updates.</p>
                        </div>
                         <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Double DQN</h3>
                            <p class="text-gray-600">Standard DQN suffers from overestimation bias because it uses the same network to both select the best action and evaluate its value. Double DQN decouples these steps: the main network selects the best action, and the target network evaluates it, reducing the bias.</p>
                            <div class="formula" data-katex="Y_t^{\\text{DoubleDQN}} = R_{t+1} + \\gamma Q(S_{t+1}, \\arg\\max_{a'} Q(S_{t+1}, a'; \\theta_t); \\theta_t^-)"></div>
                        </div>
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Rainbow</h3>
                            <p class="text-gray-600">Rainbow is not a single new algorithm, but a combination of seven improvements to DQN, including Double DQN, Prioritized Replay, and Dueling Networks. Together, they achieve state-of-the-art performance on Atari benchmarks.</p>
                        </div>
                    </div>
                     <div class="mt-6 card p-6 rounded-lg quiz-container" data-quiz-id="drl-quiz" data-correct-answer="c">
                        <h3 class="text-xl font-semibold text-gray-900 mb-3">Quick Quiz</h3>
                        <p class="font-medium text-gray-700 mb-2">What was a key innovation of the Deep Q-Network (DQN) that stabilized training?</p>
                        <div class="space-y-2">
                            <label class="flex items-center"><input type="radio" name="drl-quiz" value="a" class="mr-2"> Using a linear function approximator.</label>
                            <label class="flex items-center"><input type="radio" name="drl-quiz" value="b" class="mr-2"> Learning the policy directly.</label>
                            <label class="flex items-center"><input type="radio" name="drl-quiz" value="c" class="mr-2"> Using Experience Replay and a target network.</label>
                            <label class="flex items-center"><input type="radio" name="drl-quiz" value="d" class="mr-2"> Removing the discount factor.</label>
                        </div>
                        <button class="quiz-submit-btn mt-4 btn-primary font-semibold py-2 px-4 rounded">Check Answer</button>
                        <div class="quiz-feedback hidden"></div>
                    </div>
                `},
                { id: 'policy-grad', title: 'Policy Gradient Methods', content: `
                    <h2 class="text-3xl font-bold text-gray-900 mb-4 pb-2 border-b border-gray-200">Policy Gradient Methods</h2>
                    <p class="text-lg text-gray-700 mb-6">These methods directly parameterize and optimize the policy $\\pi(a|s, \\mathbf{\\theta})$. They perform gradient ascent on an objective function $J(\\mathbf{\\theta})$ to find the best policy parameters.</p>
                    <div class="space-y-6">
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Policy Gradient Theorem</h3>
                            <p class="text-gray-600">This theorem provides a theoretical foundation by giving an analytic expression for the gradient of the performance objective $J(\\mathbf{\\theta})$, which allows us to perform gradient ascent.</p>
                            <div class="formula" data-katex="\\nabla J(\\mathbf{\\theta}) \\propto \\sum_s d(s) \\sum_a q_\\pi(s,a) \\nabla \\pi(a|s, \\mathbf{\\theta})"></div>
                        </div>
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">REINFORCE with Baseline</h3>
                            <p class="text-gray-600">The basic REINFORCE algorithm has high variance. We can significantly reduce this by subtracting a learned baseline $b(s)$ (often the value function $V(s)$) from the return $G_t$. This doesn't change the expected gradient but lowers its variance.</p>
                            <div class="formula" data-katex="\\mathbf{\\theta} \\leftarrow \\mathbf{\\theta} + \\alpha (G_t - b(S_t)) \\nabla \\ln \\pi(A_t|S_t, \\mathbf{\\theta})"></div>
                        </div>
                         <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Actor-Critic Methods</h3>
                            <p class="text-gray-600">Combine policy gradients with value function learning. The "Actor" (policy) learns what to do, while the "Critic" (value function) evaluates how good those actions are. The critic's feedback (e.g., the TD error) is used to train the actor, reducing variance and improving stability.</p>
                        </div>
                    </div>
                     <div class="mt-6 card p-6 rounded-lg quiz-container" data-quiz-id="pg-quiz" data-correct-answer="b">
                        <h3 class="text-xl font-semibold text-gray-900 mb-3">Quick Quiz</h3>
                        <p class="font-medium text-gray-700 mb-2">What is the primary difference between Policy Gradient methods and Value-based methods?</p>
                        <div class="space-y-2">
                            <label class="flex items-center"><input type="radio" name="pg-quiz" value="a" class="mr-2"> Policy Gradient methods only work in deterministic environments.</label>
                            <label class="flex items-center"><input type="radio" name="pg-quiz" value="b" class="mr-2"> Policy Gradient methods directly optimize a parameterized policy.</label>
                            <label class="flex items-center"><input type="radio" name="pg-quiz" value="c" class="mr-2"> Value-based methods do not use a discount factor.</label>
                            <label class="flex items-center"><input type="radio" name="pg-quiz" value="d" class="mr-2"> Policy Gradient methods do not use neural networks.</label>
                        </div>
                        <button class="quiz-submit-btn mt-4 btn-primary font-semibold py-2 px-4 rounded">Check Answer</button>
                        <div class="quiz-feedback hidden"></div>
                    </div>
                `},
                { id: 'mcts', title: 'MCTS', content: `
                    <h2 class="text-3xl font-bold text-gray-900 mb-4 pb-2 border-b border-gray-200">Monte-Carlo Tree Search (MCTS)</h2>
                    <div class="card p-6 rounded-lg">
                        <p class="text-gray-600">A powerful planning algorithm that combines the precision of tree search with the generality of random sampling. MCTS doesn't require a model of the environment; instead, it builds a search tree step-by-step based on the outcomes of simulated episodes (rollouts). The process iterates through four main steps:</p>
                        <ul class="list-decimal list-inside text-gray-600 space-y-2 mt-4">
                            <li><strong>Selection:</strong> Starting from the root, traverse the tree by selecting the most promising child nodes until a leaf node is reached. A common selection strategy is UCB1 (Upper Confidence Bound 1), which balances exploitation and exploration.</li>
                             <div class="formula" data-katex="\\text{UCB1}(v) = \\underbrace{\\frac{Q(v)}{N(v)}}_{\\text{Exploitation}} + \\underbrace{c \\sqrt{\\frac{\\ln N(p)}{N(v)}}}_{\\text{Exploration}}"></div>
                             <p class="text-sm text-gray-500 mt-2 text-center">Where $Q(v)$ is total reward from node $v$, $N(v)$ is its visit count, $N(p)$ is the parent's visit count, and $c$ is an exploration constant.</p>
                            <li><strong>Expansion:</strong> Add one or more child nodes to the leaf node, representing new actions to explore.</li>
                            <li><strong>Simulation:</strong> From a new node, run a simulated episode (a "rollout") by choosing random actions until a terminal state is reached. The outcome is the return (e.g., +1 for a win, -1 for a loss).</li>
                            <li><strong>Backpropagation:</strong> Update the value estimates ($Q$) and visit counts ($N$) of all nodes traversed during the selection phase with the return from the simulation.</li>
                        </ul>
                    </div>
                    <div class="card p-6 rounded-lg mt-6">
                        <h3 class="text-xl font-semibold text-gray-900 mb-2">Interactive MCTS Animation</h3>
                        <p class="text-gray-600 mb-4">Watch the MCTS algorithm build a search tree step-by-step. Each iteration performs the four key steps to evaluate the best next move from the root node.</p>
                        <canvas id="mcts-canvas" width="600" height="400"></canvas>
                        <div class="mt-4 flex items-center space-x-4">
                            <button id="mcts-step-btn" class="btn-primary font-semibold py-2 px-4 rounded">Step</button>
                            <button id="mcts-play-btn" class="btn-secondary font-semibold py-2 px-4 rounded">Auto-Play</button>
                            <button id="mcts-reset-btn" class="btn-danger font-semibold py-2 px-4 rounded">Reset</button>
                            <div id="mcts-status" class="text-gray-700 font-medium p-2 bg-yellow-100 rounded">Status: Ready</div>
                        </div>
                    </div>
                    <div class="mt-6 card p-6 rounded-lg quiz-container" data-quiz-id="mcts-quiz" data-correct-answer="d">
                        <h3 class="text-xl font-semibold text-gray-900 mb-3">Quick Quiz</h3>
                        <p class="font-medium text-gray-700 mb-2">The UCB1 formula is primarily used in which step of the MCTS algorithm?</p>
                        <div class="space-y-2">
                            <label class="flex items-center"><input type="radio" name="mcts-quiz" value="a" class="mr-2"> Backpropagation</label>
                            <label class="flex items-center"><input type="radio" name="mcts-quiz" value="b" class="mr-2"> Expansion</label>
                            <label class="flex items-center"><input type="radio" name="mcts-quiz" value="c" class="mr-2"> Simulation</label>
                            <label class="flex items-center"><input type="radio" name="mcts-quiz" value="d" class="mr-2"> Selection</label>
                        </div>
                        <button class="quiz-submit-btn mt-4 btn-primary font-semibold py-2 px-4 rounded">Check Answer</button>
                        <div class="quiz-feedback hidden"></div>
                    </div>
                `},
                { id: 'alphago', title: 'AlphaGo & MuZero', content: `
                    <h2 class="text-3xl font-bold text-gray-900 mb-4 pb-2 border-b border-gray-200">AlphaGo, AlphaGo Zero, & MuZero</h2>
                    <div class="card p-6 rounded-lg">
                        <p class="text-gray-600">These landmark systems from DeepMind showcase the power of deep RL and search.
                            <br>• <strong>AlphaGo:</strong> Famously defeated world champion Lee Sedol. It used two neural networks—a "policy network" to select moves and a "value network" to predict the winner—trained on human expert games. These networks guided a powerful MCTS algorithm.
                            <br>• <strong>AlphaGo Zero:</strong> A more powerful version that learned entirely from self-play, without any human data. Starting with random play, it discovered strategies far superior to human play, demonstrating that tabula rasa learning could surpass human knowledge.
                            <br>• <strong>MuZero:</strong> The most general of the three. It masters games like Go, chess, and Atari <span class="italic">without being told the rules</span>. MuZero learns its own model of the environment focused only on the aspects relevant for planning (predicting the next reward, policy, and value), making it a significant step towards more general-purpose algorithms.
                        </p>
                    </div>
                    <div class="mt-6 card p-6 rounded-lg quiz-container" data-quiz-id="alphago-quiz" data-correct-answer="b">
                        <h3 class="text-xl font-semibold text-gray-900 mb-3">Quick Quiz</h3>
                        <p class="font-medium text-gray-700 mb-2">What was a key difference between AlphaGo and AlphaGo Zero?</p>
                        <div class="space-y-2">
                            <label class="flex items-center"><input type="radio" name="alphago-quiz" value="a" class="mr-2"> AlphaGo used MCTS, but AlphaGo Zero did not.</label>
                            <label class="flex items-center"><input type="radio" name="alphago-quiz" value="b" class="mr-2"> AlphaGo Zero learned entirely from self-play without human data.</label>
                            <label class="flex items-center"><input type="radio" name="alphago-quiz" value="c" class="mr-2"> AlphaGo Zero was designed only for the game of Chess.</label>
                            <label class="flex items-center"><input type="radio" name="alphago-quiz" value="d" class="mr-2"> AlphaGo used a single neural network.</label>
                        </div>
                        <button class="quiz-submit-btn mt-4 btn-primary font-semibold py-2 px-4 rounded">Check Answer</button>
                        <div class="quiz-feedback hidden"></div>
                    </div>
                `},
                { id: 'imitation-learning', title: 'Imitation Learning', content: `
                    <h2 class="text-3xl font-bold text-gray-900 mb-4 pb-2 border-b border-gray-200">Imitation Learning</h2>
                    <div class="card p-6 rounded-lg">
                        <p class="text-gray-600">A family of methods where an agent learns by observing an expert, rather than through its own trial-and-error. This is useful when exploration is costly, dangerous, or when rewards are hard to define.
                           <br>• <strong>Behavioral Cloning:</strong> The simplest approach. It treats imitation as a supervised learning problem, training a policy network to map states to the actions an expert took in those states. Its main weakness is the "covariate shift" problem: if the agent enters a state the expert never saw, it may not know how to recover.
                           <br>• <strong>Inverse Reinforcement Learning (IRL):</strong> A more advanced approach. Instead of just copying actions, IRL tries to infer the expert's hidden reward function. The assumption is that the expert is acting optimally according to some reward signal. Once this reward function is learned, it can be used with any RL algorithm to train a robust policy.
                        </p>
                    </div>
                    <div class="mt-6 card p-6 rounded-lg quiz-container" data-quiz-id="imitation-quiz" data-correct-answer="c">
                        <h3 class="text-xl font-semibold text-gray-900 mb-3">Quick Quiz</h3>
                        <p class="font-medium text-gray-700 mb-2">What is the main weakness of the Behavioral Cloning approach to imitation learning?</p>
                        <div class="space-y-2">
                            <label class="flex items-center"><input type="radio" name="imitation-quiz" value="a" class="mr-2"> It is computationally very expensive.</label>
                            <label class="flex items-center"><input type="radio" name="imitation-quiz" value="b" class="mr-2"> It requires a perfect model of the environment.</label>
                            <label class="flex items-center"><input type="radio" name="imitation-quiz" value="c" class="mr-2"> It can fail in unseen states due to covariate shift.</label>
                            <label class="flex items-center"><input type="radio" name="imitation-quiz" value="d" class="mr-2"> It cannot learn a stochastic policy.</label>
                        </div>
                        <button class="quiz-submit-btn mt-4 btn-primary font-semibold py-2 px-4 rounded">Check Answer</button>
                        <div class="quiz-feedback hidden"></div>
                    </div>
                `},
                { id: 'marl', title: 'Multi-Agent RL', content: `
                    <h2 class="text-3xl font-bold text-gray-900 mb-4 pb-2 border-b border-gray-200">Multi-Agent RL (MARL)</h2>
                    <div class="card p-6 rounded-lg">
                        <p class="text-gray-600">MARL extends RL to systems with multiple agents that learn and act in a shared environment. This introduces significant challenges beyond single-agent RL:
                            <br>• <strong>Non-stationarity:</strong> As other agents learn and change their policies, the environment effectively becomes non-stationary from the perspective of any one agent. What was a good action yesterday might be a bad action today.
                            <br>• <strong>Credit Assignment:</strong> In cooperative settings, it can be difficult to determine which agent's actions were responsible for the team's success or failure.
                            <br>• <strong>Coordination/Competition:</strong> Agents must learn to either coordinate with allies or compete effectively against adversaries.
                            <br>A popular paradigm is "Centralized Training with Decentralized Execution" (CTDE), where agents can share information during training but must act based only on their local observations during execution.
                        </p>
                    </div>
                    <div class="mt-6 card p-6 rounded-lg quiz-container" data-quiz-id="marl-quiz" data-correct-answer="c">
                        <h3 class="text-xl font-semibold text-gray-900 mb-3">Quick Quiz</h3>
                        <p class="font-medium text-gray-700 mb-2">The paradigm where agents share information during training but not execution is called:</p>
                        <div class="space-y-2">
                            <label class="flex items-center"><input type="radio" name="marl-quiz" value="a" class="mr-2"> Independent Q-Learning</label>
                            <label class="flex items-center"><input type="radio" name="marl-quiz" value="b" class="mr-2"> Policy Gradients</label>
                            <label class="flex items-center"><input type="radio" name="marl-quiz" value="c" class="mr-2"> Centralized Training with Decentralized Execution (CTDE)</label>
                            <label class="flex items-center"><input type="radio" name="marl-quiz" value="d" class="mr-2"> Markov Games</label>
                        </div>
                        <button class="quiz-submit-btn mt-4 btn-primary font-semibold py-2 px-4 rounded">Check Answer</button>
                        <div class="quiz-feedback hidden"></div>
                    </div>
                `},
                { id: 'hitl', title: 'Human-in-the-Loop RL', content: `
                    <h2 class="text-3xl font-bold text-gray-900 mb-4 pb-2 border-b border-gray-200">Human-in-the-Loop (HITL) Learning</h2>
                    <div class="card p-6 rounded-lg">
                        <p class="text-gray-600">Also known as Interactive RL, this approach integrates human feedback into the agent's learning loop. This is valuable when environmental rewards are sparse or when we want to align the agent's behavior with complex human preferences. The feedback can take many forms:</p>
                         <ul class="list-disc list-inside text-gray-600 space-y-2 mt-4">
                            <li><strong>Reward Shaping:</strong> A human provides additional reward signals to guide the agent.</li>
                            <li><strong>Policy Shaping:</strong> A human can directly influence the agent's actions or policy.</li>
                            <li><strong>Preference-Based RL:</strong> A human provides feedback by comparing two different behaviors from the agent, and the agent learns a reward function that explains these preferences.</li>
                        </ul>
                    </div>
                    <div class="mt-6 card p-6 rounded-lg quiz-container" data-quiz-id="hitl-quiz" data-correct-answer="d">
                        <h3 class="text-xl font-semibold text-gray-900 mb-3">Quick Quiz</h3>
                        <p class="font-medium text-gray-700 mb-2">Which of the following is NOT a common form of human feedback in HITL learning?</p>
                        <div class="space-y-2">
                            <label class="flex items-center"><input type="radio" name="hitl-quiz" value="a" class="mr-2"> Reward Shaping</label>
                            <label class="flex items-center"><input type="radio" name="hitl-quiz" value="b" class="mr-2"> Policy Shaping</label>
                            <label class="flex items-center"><input type="radio" name="hitl-quiz" value="c" class="mr-2"> Preference-Based Feedback</label>
                            <label class="flex items-center"><input type="radio" name="hitl-quiz" value="d" class="mr-2"> State Abstraction</label>
                        </div>
                        <button class="quiz-submit-btn mt-4 btn-primary font-semibold py-2 px-4 rounded">Check Answer</button>
                        <div class="quiz-feedback hidden"></div>
                    </div>
                `},
                { id: 'safety', title: 'Safety in RL', content: `
                    <h2 class="text-3xl font-bold text-gray-900 mb-4 pb-2 border-b border-gray-200">Safety in RL</h2>
                    <div class="card p-6 rounded-lg">
                        <p class="text-gray-600">Ensuring RL agents behave safely and reliably is critical for real-world deployment in areas like robotics, autonomous driving, and healthcare. The goal is to prevent the agent from causing harm or entering catastrophic states, especially during its initial exploration phase. Key approaches include:
                            <br>• <strong>Constrained MDPs:</strong> The problem is formulated to maximize a reward function while satisfying explicit safety constraints (e.g., "keep the robot's temperature below a certain threshold").
                            <br>• <strong>Safe Exploration:</strong> Designing algorithms that explore the environment cautiously, using uncertainty estimates to avoid actions that could lead to dangerous, unknown states.
                            <br>• <strong>Shielding:</strong> Using a predefined set of safety rules or a formal verification module (a "shield") to monitor and override the agent's actions if they are deemed unsafe.
                        </p>
                    </div>
                    <div class="mt-6 card p-6 rounded-lg quiz-container" data-quiz-id="safety-quiz" data-correct-answer="a">
                        <h3 class="text-xl font-semibold text-gray-900 mb-3">Quick Quiz</h3>
                        <p class="font-medium text-gray-700 mb-2">Using a formal module to override an agent's potentially unsafe actions is known as:</p>
                        <div class="space-y-2">
                            <label class="flex items-center"><input type="radio" name="safety-quiz" value="a" class="mr-2"> Shielding</label>
                            <label class="flex items-center"><input type="radio" name="safety-quiz" value="b" class="mr-2"> Reward Shaping</label>
                            <label class="flex items-center"><input type="radio" name="safety-quiz" value="c" class="mr-2"> Constrained MDPs</label>
                            <label class="flex items-center"><input type="radio" name="safety-quiz" value="d" class="mr-2"> Safe Exploration</label>
                        </div>
                        <button class="quiz-submit-btn mt-4 btn-primary font-semibold py-2 px-4 rounded">Check Answer</button>
                        <div class="quiz-feedback hidden"></div>
                    </div>
                `},
                { id: 'sample-questions', title: 'Sample Questions', content: `
                    <h2 class="text-3xl font-bold text-gray-900 mb-4 pb-2 border-b border-gray-200">Sample Exam Questions</h2>
                    <div class="space-y-8">
                        <!-- Question 1 -->
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Question 1: The Discount Factor</h3>
                            <p class="font-medium text-gray-700 mb-4">An agent is in a maze with two paths to a goal. Path A is short (3 steps) but passes through a fire pit that gives a -10 reward. Path B is long (10 steps) with no negative rewards. Each step gives a reward of -1, and the goal gives +100. How does the discount factor, $\\gamma$, influence which path the agent prefers? </p>
                            <div class="bg-gray-50 p-4 rounded-md">
                                <h4 class="font-semibold text-gray-800">Solution:</h4>
                                <p class="text-gray-600 mt-2">The discount factor $\\gamma$ determines how much the agent values future rewards compared to immediate ones. A low $\\gamma$ makes the agent "myopic," while a high $\\gamma$ makes it "farsighted."</p>
                                <ul class="list-disc list-inside text-gray-600 space-y-1 mt-2">
                                    <li><strong>High $\\gamma$ (e.g., 0.99):</strong> The agent is patient. It will heavily weigh the final +100 reward and will likely choose the longer Path B to avoid the large immediate penalty of the fire pit, as the cumulative step penalties will be less significant than the -10 from the fire.</li>
                                    <li><strong>Low $\\gamma$ (e.g., 0.5):</strong> The agent is impatient. The future +100 reward is heavily discounted. The immediate -10 penalty from the fire pit might be preferable to accumulating many -1 step penalties, making Path A seem more attractive. The agent prioritizes avoiding immediate costs.</li>
                                </ul>
                                <p class="text-gray-600 mt-2">Relevant Section: <span class="solution-link" data-target="elements-rl">Elements of RL (Value Function)</span></p>
                            </div>
                        </div>

                        <!-- Question 2 -->
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Question 2: DP vs. MC Methods</h3>
                            <p class="font-medium text-gray-700 mb-4">When would you prefer a Monte Carlo method over a Dynamic Programming method to solve an MDP? Conversely, when is DP a better choice?</p>
                             <div class="bg-gray-50 p-4 rounded-md">
                                <h4 class="font-semibold text-gray-800">Solution:</h4>
                                <p class="text-gray-600 mt-2">The choice depends primarily on whether you have a model of the environment.</p>
                                <ul class="list-disc list-inside text-gray-600 space-y-1 mt-2">
                                    <li><strong>Prefer Monte Carlo when:</strong> The model of the environment (state transition probabilities and rewards) is unknown or too complex to define. MC methods are model-free and learn directly from experienced episodes. This makes them suitable for complex games or real-world interactions where the rules are not fully known.</li>
                                    <li><strong>Prefer Dynamic Programming when:</strong> You have a perfect model of the MDP. If the state and action spaces are manageably small, DP methods like Value Iteration are highly efficient and guaranteed to find the optimal solution. This is ideal for planning problems like inventory management or resource allocation where the system dynamics are well-defined.</li>
                                </ul>
                                <p class="text-gray-600 mt-2">Relevant Sections: <span class="solution-link" data-target="dp">Dynamic Programming</span> and <span class="solution-link" data-target="mc">Monte Carlo Methods</span></p>
                            </div>
                        </div>

                         <!-- Question 3 -->
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Question 3: TD Learning Update</h3>
                            <p class="font-medium text-gray-700 mb-4">An agent is in state A, where $V(A) = 10$. It takes an action and transitions to state B, where $V(B) = 20$, receiving a reward of $R = 5$. Using a learning rate $\\alpha = 0.1$ and discount factor $\\gamma = 0.9$, calculate the new value for $V(A)$ after this one-step transition using TD(0).</p>
                             <div class="bg-gray-50 p-4 rounded-md">
                                <h4 class="font-semibold text-gray-800">Solution:</h4>
                                <p class="text-gray-600 mt-2">We use the TD(0) update rule: $V(S_t) \\leftarrow V(S_t) + \\alpha [R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)]$.</p>
                                <ol class="list-decimal list-inside text-gray-600 space-y-1 mt-2">
                                    <li><strong>Calculate the TD Target:</strong> $R_{t+1} + \\gamma V(S_{t+1}) = 5 + 0.9 \\times 20 = 5 + 18 = 23$.</li>
                                    <li><strong>Calculate the TD Error:</strong> $\\delta_t = \\text{TD Target} - V(S_t) = 23 - 10 = 13$.</li>
                                    <li><strong>Apply the update:</strong> $V(A)_{new} = V(A)_{old} + \\alpha \\times \\delta_t = 10 + 0.1 \\times 13 = 10 + 1.3 = 11.3$.</li>
                                </ol>
                                 <p class="text-gray-600 mt-2">The new value for state A is 11.3.</p>
                                <p class="text-gray-600 mt-2">Relevant Section: <span class="solution-link" data-target="td">Temporal Difference Learning</span></p>
                            </div>
                        </div>
                        
                         <!-- Question 4 -->
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Question 4: Actor-Critic vs. REINFORCE</h3>
                            <p class="font-medium text-gray-700 mb-4">Why does an Actor-Critic method generally have lower variance than the REINFORCE algorithm? Explain the roles of the actor and critic.</p>
                             <div class="bg-gray-50 p-4 rounded-md">
                                <h4 class="font-semibold text-gray-800">Solution:</h4>
                                <p class="text-gray-600 mt-2">Actor-Critic methods reduce variance by using a more stable and less noisy signal to update the policy.</p>
                                 <ul class="list-disc list-inside text-gray-600 space-y-1 mt-2">
                                    <li><strong>REINFORCE:</strong> Updates the policy based on the full Monte Carlo return ($G_t$) of an entire episode. The return is a sum of many random variables (rewards), which makes it a high-variance estimate of an action's goodness. An action that was good by chance in a successful episode will be heavily reinforced, and vice versa.</li>
                                    <li><strong>Actor-Critic:</strong>
                                        <ul>
                                            <li>The <strong>Actor</strong> is the policy, which decides which action to take.</li>
                                            <li>The <strong>Critic</strong> is a value function (e.g., $V(s)$ or $Q(s,a)$) that learns to evaluate the "goodness" of states or state-action pairs.</li>
                                        </ul>
                                       Instead of using the noisy full return, the Actor uses the Critic's feedback, often in the form of the TD error ($\delta_t$). The TD error is a lower-variance estimate of the action's advantage because it's based on the immediate reward and the value of the next state, rather than the entire sequence of future rewards. This bootstrapping leads to more stable and faster learning.
                                    </li>
                                </ul>
                                <p class="text-gray-600 mt-2">Relevant Section: <span class="solution-link" data-target="policy-grad">Policy Gradient Methods</span></p>
                            </div>
                        </div>

                        <!-- Question 5 -->
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Question 5: UCB Calculation</h3>
                            <p class="font-medium text-gray-700 mb-4">An agent is facing a 3-armed bandit problem. After 50 total pulls ($N=50$), the arms have been played as follows: Arm 1 ($N(a_1)=10$, avg reward $Q(a_1)=0.8$), Arm 2 ($N(a_2)=25$, avg reward $Q(a_2)=0.6$), and Arm 3 ($N(a_3)=15$, avg reward $Q(a_3)=0.7$). Using the UCB1 formula with an exploration constant $c = \\sqrt{2}$, which arm should the agent pull next?</p>
                             <div class="bg-gray-50 p-4 rounded-md">
                                <h4 class="font-semibold text-gray-800">Solution:</h4>
                                <p class="text-gray-600 mt-2">The UCB1 formula is: $UCB1(a) = Q(a) + c \\sqrt{\\frac{\\ln N}{N(a)}}$. We calculate this for each arm:</p>
                                <ul class="list-disc list-inside text-gray-600 space-y-2 mt-2">
                                    <li><b>Arm 1:</b> $0.8 + \\sqrt{2} \\sqrt{\\frac{\\ln 50}{10}} \\approx 0.8 + 1.414 \\times \\sqrt{\\frac{3.912}{10}} \\approx 0.8 + 1.414 \\times 0.625 \\approx 0.8 + 0.884 = 1.684$</li>
                                    <li><b>Arm 2:</b> $0.6 + \\sqrt{2} \\sqrt{\\frac{\\ln 50}{25}} \\approx 0.6 + 1.414 \\times \\sqrt{\\frac{3.912}{25}} \\approx 0.6 + 1.414 \\times 0.396 \\approx 0.6 + 0.560 = 1.160$</li>
                                    <li><b>Arm 3:</b> $0.7 + \\sqrt{2} \\sqrt{\\frac{\\ln 50}{15}} \\approx 0.7 + 1.414 \\times \\sqrt{\\frac{3.912}{15}} \\approx 0.7 + 1.414 \\times 0.511 \\approx 0.7 + 0.722 = 1.422$</li>
                                </ul>
                                <p class="text-gray-600 mt-2">The agent should choose <strong>Arm 1</strong> because it has the highest UCB1 score (1.684), indicating the best balance of high past performance and uncertainty.</p>
                                <p class="text-gray-600 mt-2">Relevant Section: <span class="solution-link" data-target="mcts">MCTS (UCB1)</span></p>
                            </div>
                        </div>

                        <!-- Question 6 -->
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Question 6: SARSA vs. Q-Learning Scenario</h3>
                            <p class="font-medium text-gray-700 mb-4">An agent is in a "windy gridworld." From state $S_t$, it takes action $A_t$ (move Right). Due to wind, it lands in $S_{t+1}$. From $S_{t+1}$, its $\\epsilon$-greedy policy selects an exploratory (non-optimal) action $A_{t+1}$ (move Down) into a penalty square. How would the Q-value updates for $Q(S_t, A_t)$ differ for SARSA and Q-Learning in this specific step?</p>
                             <div class="bg-gray-50 p-4 rounded-md">
                                <h4 class="font-semibold text-gray-800">Solution:</h4>
                                <p class="text-gray-600 mt-2">The difference lies in which next-state action value they use for the update target.</p>
                                <ul class="list-disc list-inside text-gray-600 space-y-2 mt-2">
                                    <li><strong>SARSA (On-Policy):</strong> Its update uses the quintuple $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$. It would use the Q-value of the *actual* next action taken, $Q(S_{t+1}, A_{t+1})$ (move Down). Since this action leads to a penalty, the TD target will be low, and $Q(S_t, A_t)$ will be updated downwards. SARSA learns a "safer" policy that accounts for its own exploration.</li>
                                    <li><strong>Q-Learning (Off-Policy):</strong> Its update rule uses $\\max_a Q(S_{t+1}, a)$. It would ignore the exploratory action $A_{t+1}$ and instead use the Q-value of the *best possible action* from $S_{t+1}$. The update for $Q(S_t, A_t)$ is based on the assumption that the agent will act optimally from now on, resulting in a more optimistic update than SARSA's.</li>
                                </ul>
                                <p class="text-gray-600 mt-2">Relevant Section: <span class="solution-link" data-target="td">Temporal Difference Learning</span></p>
                            </div>
                        </div>

                        <!-- Question 7 -->
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Question 7: DQN Components</h3>
                            <p class="font-medium text-gray-700 mb-4">What are the two primary innovations of the Deep Q-Network (DQN), and what problem does each one solve?</p>
                             <div class="bg-gray-50 p-4 rounded-md">
                                <h4 class="font-semibold text-gray-800">Solution:</h4>
                                <p class="text-gray-600 mt-2">The two key innovations that stabilized Q-learning with deep neural networks are:</p>
                                <ol class="list-decimal list-inside text-gray-600 space-y-2 mt-2">
                                    <li><strong>Experience Replay:</strong>
                                        <ul>
                                            <li><strong>What it is:</strong> Storing experiences $(s, a, r, s')$ in a large memory buffer and sampling random mini-batches from it to train the network.</li>
                                            <li><strong>Problem Solved:</strong> It breaks the strong temporal correlations between consecutive samples, making the data more independent and identically distributed (I.I.D.) and stabilizing training. It also increases data efficiency by reusing experiences.</li>
                                        </ul>
                                    </li>
                                    <li><strong>Fixed Target Network:</strong>
                                        <ul>
                                            <li><strong>What it is:</strong> Using a separate, older copy of the Q-network to generate the TD targets. This target network's weights are only updated periodically.</li>
                                            <li><strong>Problem Solved:</strong> It prevents the learning target from constantly changing. If the same network is used to select and evaluate actions, the target value shifts with every update, leading to oscillations and divergence. A fixed target provides a stable learning signal.</li>
                                        </ul>
                                    </li>
                                </ol>
                                <p class="text-gray-600 mt-2">Relevant Section: <span class="solution-link" data-target="deep-rl">Deep Reinforcement Learning</span></p>
                            </div>
                        </div>
                         <!-- Question 8 -->
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Question 8: The Deadly Triad</h3>
                            <p class="font-medium text-gray-700 mb-4">What is the "deadly triad" in reinforcement learning, and why does it pose a challenge?</p>
                             <div class="bg-gray-50 p-4 rounded-md">
                                <h4 class="font-semibold text-gray-800">Solution:</h4>
                                <p class="text-gray-600 mt-2">The "deadly triad" refers to the combination of three elements that can cause learning to become unstable and diverge:</p>
                                <ol class="list-decimal list-inside text-gray-600 space-y-1 mt-2">
                                   <li><strong>Function Approximation:</strong> Using a parameterized function (like a neural network) instead of a table to represent the value function, which is necessary for large state spaces.</li>
                                   <li><strong>Bootstrapping:</strong> Updating value estimates based on other value estimates (e.g., in TD learning and Q-learning).</li>
                                   <li><strong>Off-policy training:</strong> Learning about a target policy while following a different behavior policy (e.g., in Q-learning).</li>
                                </ol>
                                <p class="text-gray-600 mt-2">The combination of these three can lead to a feedback loop where errors in the value function estimate are amplified, causing the network's weights to explode towards infinity. Many modern deep RL algorithms are designed specifically to mitigate the instability caused by the deadly triad.</p>
                                <p class="text-gray-600 mt-2">Relevant Section: <span class="solution-link" data-target="func-approx">Function Approximation</span></p>
                            </div>
                        </div>

                         <!-- Question 9 -->
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Question 9: Imitation Learning</h3>
                            <p class="font-medium text-gray-700 mb-4">You want to teach a robot to perform a complex task like folding laundry. Why might Imitation Learning be a more practical approach here than standard RL with a sparse reward (e.g., +1 only for a perfectly folded shirt)?</p>
                             <div class="bg-gray-50 p-4 rounded-md">
                                <h4 class="font-semibold text-gray-800">Solution:</h4>
                                <p class="text-gray-600 mt-2">Imitation Learning is often more practical for such tasks for two main reasons:</p>
                                <ol class="list-decimal list-inside text-gray-600 space-y-1 mt-2">
                                   <li><strong>Inefficient Exploration:</strong> With a sparse reward, a standard RL agent would have to explore a vast state-action space randomly for a very long time before it stumbles upon the correct sequence of actions to fold a shirt and receive a reward. The probability of discovering this through random exploration is practically zero. Imitation Learning bypasses this by learning from expert demonstrations, which immediately provides a strong learning signal in the relevant part of the state space.</li>
                                   <li><strong>Complex Reward Function:</strong> Defining a dense reward function that correctly guides the agent through all the intermediate steps of folding laundry is extremely difficult. A poorly designed reward function could lead to unexpected and undesirable behaviors (reward hacking). Imitation Learning, particularly Inverse Reinforcement Learning (IRL), can infer the underlying reward function from demonstrations, which is often easier than specifying it manually.</li>
                                </ol>
                                <p class="text-gray-600 mt-2">Relevant Section: <span class="solution-link" data-target="imitation-learning">Imitation Learning</span></p>
                            </div>
                        </div>
                    </div>
                `},
            ];
            
            const mainContent = document.getElementById('main-content');
            const sidebarNav = document.getElementById('sidebar-nav');
            const topicHierarchy = {
                'Introduction': ['intro-rl', 'elements-rl'],
                'Core Problems & Frameworks': ['mab', 'mdp'],
                'Solution Methods': ['dp', 'mc', 'td'],
                'Advanced Topics': ['func-approx', 'deep-rl', 'policy-grad'],
                'Planning & Case Studies': ['mcts', 'alphago', 'imitation-learning'],
                'Frontiers in RL': ['marl', 'hitl', 'safety'],
                'Practice': ['sample-questions']
            };

            // Populate sidebar and content
            Object.entries(topicHierarchy).forEach(([category, topicIds]) => {
                const categoryTitle = document.createElement('h3');
                categoryTitle.className = 'text-xs font-bold text-gray-500 uppercase tracking-wider mt-6 mb-2 px-4';
                categoryTitle.textContent = category;
                sidebarNav.appendChild(categoryTitle);

                topicIds.forEach(topicId => {
                    const topic = topics.find(t => t.id === topicId);
                    if (topic) {
                        const link = document.createElement('a');
                        link.href = '#';
                        link.className = 'block py-2 px-4 rounded-md sidebar-item text-gray-700 text-sm';
                        link.dataset.target = topic.id;
                        link.textContent = topic.title;
                        sidebarNav.appendChild(link);
                        
                        const section = document.createElement('section');
                        section.id = topic.id;
                        section.className = 'content-section';
                        section.innerHTML = topic.content;
                        mainContent.appendChild(section);
                    }
                });
            });

            // Render block-level formulas once after all content is in the DOM.
            // Inline formulas will be handled by the auto-render script in the <head>.
            document.querySelectorAll('.formula').forEach(el => {
                katex.render(el.dataset.katex, el, {
                    throwOnError: false,
                    displayMode: true
                });
            });

            const navLinks = document.querySelectorAll('.sidebar-item');
            const contentSections = document.querySelectorAll('.content-section');

            function switchTab(targetId) {
                if (!document.getElementById(targetId)) return;

                contentSections.forEach(section => {
                    section.classList.toggle('active', section.id === targetId);
                });
                navLinks.forEach(link => {
                    link.classList.toggle('active', link.dataset.target === targetId);
                });
                 // Re-render math in the newly activated tab
                const activeSection = document.getElementById(targetId);
                if (activeSection && window.renderMathInElement) {
                    renderMathInElement(activeSection, {
                        delimiters: [{left: '$', right: '$', display: false}],
                        throwOnError: false
                    });
                }
                
                // Initialize interactive canvas if it's visible
                if (targetId === 'mab') initBanditProblem();
                if (targetId === 'mdp') initGridworld();
                if (targetId === 'mc') initBlackjackMC();
                if (targetId === 'td') initTDGridworld();
                if (targetId === 'mcts') initMCTSAnimation();
            }

            mainContent.addEventListener('click', (e) => {
                 if (e.target.classList.contains('solution-link')) {
                    e.preventDefault();
                    const targetId = e.target.dataset.target;
                    switchTab(targetId);
                    document.getElementById(targetId).scrollIntoView({ behavior: 'smooth' });
                 }
            });


            sidebarNav.addEventListener('click', (e) => {
                if (e.target.closest('.sidebar-item')) {
                    e.preventDefault();
                    const targetId = e.target.closest('.sidebar-item').dataset.target;
                    switchTab(targetId);
                }
            });

            // Quiz Handler using Event Delegation
            mainContent.addEventListener('click', (e) => {
                if (e.target.classList.contains('quiz-submit-btn')) {
                    const quizContainer = e.target.closest('.quiz-container');
                    const quizId = quizContainer.dataset.quizId;
                    const correctAnswer = quizContainer.dataset.correctAnswer;
                    const selectedAnswer = quizContainer.querySelector(`input[name="${quizId}"]:checked`);
                    const feedbackEl = quizContainer.querySelector('.quiz-feedback');
                    
                    if (selectedAnswer) {
                        feedbackEl.classList.remove('hidden', 'correct', 'incorrect');
                        if (selectedAnswer.value === correctAnswer) {
                            feedbackEl.textContent = 'Correct! Well done.';
                            feedbackEl.classList.add('correct');
                        } else {
                            feedbackEl.textContent = 'Not quite. Try again!';
                            feedbackEl.classList.add('incorrect');
                        }
                    } else {
                        feedbackEl.classList.remove('hidden', 'correct');
                        feedbackEl.textContent = 'Please select an answer.';
                        feedbackEl.classList.add('incorrect');
                    }
                }
            });


            // Initial state and initial render
            switchTab('intro-rl');

            // --- Multi-Armed Bandit ---
            function initBanditProblem() {
                const armsContainer = document.getElementById('bandit-arms-container');
                if (!armsContainer || armsContainer.dataset.initialized) return;
                armsContainer.dataset.initialized = 'true';
                let intervalId = null;

                const NUM_ARMS = 5;
                let trueMeans = [];
                let estimatedValues = [];
                let pullCounts = [];
                let totalReward = 0;
                let totalPulls = 0;
                let epsilon = 0.1;
                
                const epsilonSlider = document.getElementById('epsilon-slider');
                const epsilonValueSpan = document.getElementById('epsilon-value');
                const resetButton = document.getElementById('reset-bandit-btn');
                const totalRewardEl = document.getElementById('bandit-total-reward');
                const totalPullsEl = document.getElementById('bandit-total-pulls');

                epsilonSlider.addEventListener('input', (e) => {
                    epsilon = parseFloat(e.target.value);
                    epsilonValueSpan.textContent = epsilon.toFixed(2);
                });

                resetButton.addEventListener('click', setup);

                function setup() {
                    if(intervalId) clearInterval(intervalId);
                    armsContainer.innerHTML = '';
                    trueMeans = Array.from({length: NUM_ARMS}, () => Math.random());
                    estimatedValues = Array(NUM_ARMS).fill(0);
                    pullCounts = Array(NUM_ARMS).fill(0);
                    totalReward = 0;
                    totalPulls = 0;

                    for (let i = 0; i < NUM_ARMS; i++) {
                        const armWrapper = document.createElement('div');
                        armWrapper.className = 'flex flex-col items-center h-full justify-end';
                        armWrapper.innerHTML = `
                            <div class="text-sm text-gray-600 mb-1">Pulls: <span id="pulls-${i}">0</span></div>
                            <div class="text-sm font-mono text-blue-600 mb-1">Q(a): <span id="q-val-${i}">0.00</span></div>
                            <div class="w-16 h-4/5 bg-gray-200 rounded-t-lg cursor-pointer hover:bg-gray-300 relative flex items-end justify-center" id="arm-${i}">
                               <div class="bg-blue-500 w-full rounded-t-lg" style="height: 0%;"></div>
                            </div>
                        `;
                        armsContainer.appendChild(armWrapper);
                        document.getElementById(`arm-${i}`).addEventListener('click', () => chooseAndPull(i));
                    }
                    updateDisplay();
                    autoPlay();
                }

                function chooseAndPull(armIndex = -1) {
                    let choice;
                    if (armIndex !== -1) {
                        choice = armIndex;
                    } else {
                        if (Math.random() < epsilon) {
                            choice = Math.floor(Math.random() * NUM_ARMS);
                        } else {
                            const maxQ = Math.max(...estimatedValues);
                            const bestArms = estimatedValues.map((q, i) => q === maxQ ? i : -1).filter(i => i !== -1);
                            choice = bestArms[Math.floor(Math.random() * bestArms.length)];
                        }
                    }
                    const reward = Math.random() < trueMeans[choice] ? 1 : 0;
                    totalReward += reward;
                    totalPulls++;
                    pullCounts[choice]++;
                    estimatedValues[choice] += (1 / pullCounts[choice]) * (reward - estimatedValues[choice]);
                    updateDisplay();
                }

                function updateDisplay() {
                    for (let i = 0; i < NUM_ARMS; i++) {
                        document.getElementById(`pulls-${i}`).textContent = pullCounts[i];
                        document.getElementById(`q-val-${i}`).textContent = estimatedValues[i].toFixed(2);
                        const armBar = document.querySelector(`#arm-${i} > div`);
                        armBar.style.height = `${estimatedValues[i] * 100}%`;
                    }
                    totalRewardEl.textContent = totalReward;
                    totalPullsEl.textContent = totalPulls;
                }

                function autoPlay() {
                    intervalId = setInterval(() => {
                        if (document.getElementById('bandit-arms-container')) {
                            chooseAndPull();
                        } else {
                            clearInterval(intervalId);
                        }
                    }, 100);
                }
                setup();
            }

            // --- Monte Carlo Blackjack ---
            function initBlackjackMC() {
                const canvas = document.getElementById('blackjack-canvas');
                if (!canvas || canvas.dataset.initialized) return;
                canvas.dataset.initialized = 'true';

                const ctx = canvas.getContext('2d');
                const play1Btn = document.getElementById('blackjack-play1-btn');
                const play100Btn = document.getElementById('blackjack-play100-btn');
                const resetBtn = document.getElementById('blackjack-reset-btn');
                const gamesPlayedEl = document.getElementById('blackjack-games-played');
                
                let V = {}; // State-value function: V[state_string] = { sum: N, count: N }
                let gamesPlayed = 0;

                const reset = () => {
                    V = {};
                    gamesPlayed = 0;
                    gamesPlayedEl.textContent = `Games Played: 0`;
                    drawValueFunction();
                };
                
                const getCard = () => {
                    let card = Math.min(10, Math.floor(Math.random() * 13) + 1);
                    return card === 1 ? 11 : card; // Ace is 11
                };

                const getHandSum = (hand) => {
                    let sum = hand.reduce((a, b) => a + b, 0);
                    let aces = hand.filter(c => c === 11).length;
                    while (sum > 21 && aces > 0) {
                        sum -= 10;
                        aces--;
                    }
                    return sum;
                };

                const runEpisode = () => {
                    let playerHand = [getCard(), getCard()];
                    let dealerHand = [getCard(), getCard()];
                    let episode = []; // Store (state, action, reward) tuples

                    // Player's turn
                    while(true) {
                        let playerSum = getHandSum(playerHand);
                        if(playerSum > 21) break; // Player busts
                        
                        let usableAce = playerHand.includes(11) && getHandSum(playerHand.map(c => c === 11 ? 1 : c)) <= 10;
                        let state = `${playerSum}-${dealerHand[0]}-${usableAce}`;
                        
                        // Policy: stick on 20 or 21, otherwise hit
                        let action = playerSum >= 20 ? 'stick' : 'hit';
                        episode.push({ state });

                        if(action === 'stick') break;
                        playerHand.push(getCard());
                    }
                    
                    // Dealer's turn
                    let playerSum = getHandSum(playerHand);
                    let reward = 0;
                    if(playerSum > 21) {
                        reward = -1; // Player busts
                    } else {
                        while(getHandSum(dealerHand) < 17) {
                            dealerHand.push(getCard());
                        }
                        let dealerSum = getHandSum(dealerHand);
                        if(dealerSum > 21 || playerSum > dealerSum) {
                            reward = 1; // Dealer busts or player wins
                        } else if(playerSum < dealerSum) {
                            reward = -1; // Dealer wins
                        } else {
                            reward = 0; // Draw
                        }
                    }

                    // Update V using first-visit MC
                    const visitedStates = new Set();
                    for(const step of episode) {
                        if(!visitedStates.has(step.state)) {
                            if(!V[step.state]) V[step.state] = { sum: 0, count: 0 };
                            V[step.state].sum += reward;
                            V[step.state].count++;
                            visitedStates.add(step.state);
                        }
                    }
                    gamesPlayed++;
                };

                const runMultipleEpisodes = (count) => {
                    for(let i=0; i<count; i++) runEpisode();
                    gamesPlayedEl.textContent = `Games Played: ${gamesPlayed}`;
                    drawValueFunction();
                };

                const drawValueFunction = () => {
                    ctx.clearRect(0, 0, canvas.width, canvas.height);
                    
                    const drawGrid = (x_offset, y_offset, title) => {
                        ctx.fillStyle = '#3D4451';
                        ctx.font = 'bold 14px Inter';
                        ctx.textAlign = 'center';
                        ctx.fillText(title, x_offset + 125, y_offset - 10);

                        const playerSums = Array.from({length: 10}, (_, i) => 21 - i); // 21 down to 12
                        const dealerCards = Array.from({length: 10}, (_, i) => i + 1); // A, 2..10
                        
                        const colWidth = 25;
                        const rowHeight = 25;

                        for(let r=0; r < playerSums.length; r++) {
                             for(let c=0; c < dealerCards.length; c++) {
                                let stateStr = `${playerSums[r]}-${dealerCards[c] === 1 ? 11 : dealerCards[c]}-${title.includes('Usable Ace')}`;
                                let value = V[stateStr] ? V[stateStr].sum / V[stateStr].count : 0;
                                
                                let color = 'rgba(200, 200, 200, 0.5)';
                                if(value > 0) color = `rgba(74, 222, 128, ${Math.abs(value)})`; // green
                                else if(value < 0) color = `rgba(248, 113, 113, ${Math.abs(value)})`; // red
                                
                                ctx.fillStyle = color;
                                ctx.fillRect(x_offset + c * colWidth, y_offset + r * rowHeight, colWidth, rowHeight);
                                ctx.strokeStyle = '#e5e7eb';
                                ctx.strokeRect(x_offset + c * colWidth, y_offset + r * rowHeight, colWidth, rowHeight);

                                ctx.fillStyle = Math.abs(value) > 0.5 ? 'white' : 'black';
                                ctx.font = '9px Inter';
                                ctx.fillText(value.toFixed(2), x_offset + c * colWidth + colWidth/2, y_offset + r * rowHeight + rowHeight/2);
                             }
                        }
                        // Draw labels
                        ctx.fillStyle = '#3D4451';
                        ctx.font = '11px Inter';
                        for(let r=0; r < playerSums.length; r++) {
                            ctx.fillText(playerSums[r], x_offset - 15, y_offset + r*rowHeight + rowHeight/2);
                        }
                         for(let c=0; c < dealerCards.length; c++) {
                            ctx.fillText(dealerCards[c] === 1 ? 'A' : dealerCards[c], x_offset + c * colWidth + colWidth/2, y_offset + playerSums.length*rowHeight + 10);
                        }
                    };

                    drawGrid(40, 40, "With Usable Ace");
                    drawGrid(330, 40, "No Usable Ace");
                };

                play1Btn.addEventListener('click', () => runMultipleEpisodes(1));
                play100Btn.addEventListener('click', () => runMultipleEpisodes(100));
                resetBtn.addEventListener('click', reset);
                
                reset();
            }

            // --- Gridworld DP ---
            function initGridworld() {
                const canvas = document.getElementById('gridworld-canvas');
                if (!canvas || canvas.dataset.initialized) return;
                canvas.dataset.initialized = 'true';
                let intervalId = null;
                
                const ctx = canvas.getContext('2d');
                const GRID_SIZE = 5;
                const CELL_SIZE = canvas.width / GRID_SIZE;
                
                const grid = [[0, 0, 0, 0, 1], [0, -1, 0, 0, 0], [0, 0, 0, -1, 0], [0, -1, 0, 0, 0], [0, 0, 0, 0, 0]];
                let values = Array(GRID_SIZE).fill(0).map(() => Array(GRID_SIZE).fill(0));
                let iterationCount = 0;
                const GAMMA = 0.9;

                const viButton = document.getElementById('gridworld-vi-btn');
                const resetButton = document.getElementById('gridworld-reset-btn');
                const iterationsEl = document.getElementById('gridworld-iterations');

                function drawGrid() {
                    ctx.clearRect(0, 0, canvas.width, canvas.height);
                    for (let r = 0; r < GRID_SIZE; r++) {
                        for (let c = 0; c < GRID_SIZE; c++) {
                            ctx.strokeStyle = '#e5e7eb';
                            ctx.strokeRect(c * CELL_SIZE, r * CELL_SIZE, CELL_SIZE, CELL_SIZE);
                            if (grid[r][c] === 1) ctx.fillStyle = 'rgba(74, 222, 128, 0.8)';
                            else if (grid[r][c] === -1) ctx.fillStyle = 'rgba(248, 113, 113, 0.8)';
                            else continue;
                            ctx.fillRect(c * CELL_SIZE, r * CELL_SIZE, CELL_SIZE, CELL_SIZE);
                        }
                    }
                     for (let r = 0; r < GRID_SIZE; r++) {
                        for (let c = 0; c < GRID_SIZE; c++) {
                            ctx.fillStyle = '#1f2937';
                            ctx.font = '16px Inter';
                            ctx.textAlign = 'center';
                            ctx.textBaseline = 'middle';
                            ctx.fillText(values[r][c].toFixed(2), c * CELL_SIZE + CELL_SIZE / 2, r * CELL_SIZE + CELL_SIZE / 2);
                        }
                    }
                }
                
                function valueIterationStep() {
                    let newValues = JSON.parse(JSON.stringify(values));
                    for (let r = 0; r < GRID_SIZE; r++) {
                        for (let c = 0; c < GRID_SIZE; c++) {
                             if (grid[r][c] !== 0) continue;
                            let actionValues = [];
                            const actions = [[-1, 0], [1, 0], [0, -1], [0, 1]];
                            for (const [dr, dc] of actions) {
                                const nr = r + dr, nc = c + dc;
                                let reward = -0.1, nextValue = 0;
                                if (nr >= 0 && nr < GRID_SIZE && nc >= 0 && nc < GRID_SIZE) {
                                     if(grid[nr][nc] === 1) reward = 10;
                                     if(grid[nr][nc] === -1) reward = -10;
                                     nextValue = values[nr][nc];
                                } else {
                                    reward = -1;
                                    nextValue = values[r][c];
                                }
                                actionValues.push(reward + GAMMA * nextValue);
                            }
                            newValues[r][c] = Math.max(...actionValues);
                        }
                    }
                    values = newValues;
                    iterationCount++;
                    iterationsEl.textContent = `Iterations: ${iterationCount}`;
                    drawGrid();
                }

                function runValueIteration() {
                     if(intervalId) clearInterval(intervalId);
                     intervalId = setInterval(() => {
                        if (iterationCount < 50 && document.getElementById('gridworld-canvas')) {
                             valueIterationStep();
                        } else {
                            clearInterval(intervalId);
                        }
                    }, 100);
                }

                function reset() {
                    if(intervalId) clearInterval(intervalId);
                    values = Array(GRID_SIZE).fill(0).map(() => Array(GRID_SIZE).fill(0));
                    iterationCount = 0;
                    iterationsEl.textContent = `Iterations: ${iterationCount}`;
                    drawGrid();
                }

                viButton.addEventListener('click', runValueIteration);
                resetButton.addEventListener('click', reset);

                drawGrid();
            }

            // --- Gridworld TD ---
            function initTDGridworld() {
                const canvas = document.getElementById('td-gridworld-canvas');
                if (!canvas || canvas.dataset.initialized) return;
                canvas.dataset.initialized = 'true';

                const container = document.getElementById('td-gridworld-container');
                const ctx = canvas.getContext('2d');
                const algoSelect = document.getElementById('td-algo-select');
                const stepBtn = document.getElementById('td-step-btn');
                const playBtn = document.getElementById('td-play-btn');
                const resetBtn = document.getElementById('td-reset-btn');
                const infoEl = document.getElementById('td-info');

                let playInterval = null;
                const GRID_SIZE = 5;
                const CELL_SIZE = canvas.width / GRID_SIZE;
                const grid = [[0, 0, 0, 0, 1], [0, -1, 0, 0, 0], [0, 0, 0, -1, 0], [0, -1, 0, 0, 0], [0, 0, 0, 0, 0]];
                
                let Q = {}; // Q-table: Q['r,c'] = [q_up, q_down, q_left, q_right]
                let agentPos = { r: 4, c: 0 };
                
                const ALPHA = 0.1;
                const GAMMA = 0.9;
                let EPSILON = 0.1;
                let currentAlgorithm = 'sarsa';

                const actions = [[-1, 0], [1, 0], [0, -1], [0, 1]]; // Up, Down, Left, Right

                const getQ = (r, c) => {
                    const key = `${r},${c}`;
                    if (!Q[key]) Q[key] = [0, 0, 0, 0];
                    return Q[key];
                };

                const chooseAction = (r, c) => {
                    if (Math.random() < EPSILON) {
                        return Math.floor(Math.random() * 4); // Explore
                    }
                    const q_values = getQ(r, c);
                    const maxQ = Math.max(...q_values);
                    const bestActions = q_values.map((q, i) => q === maxQ ? i : -1).filter(i => i !== -1);
                    return bestActions[Math.floor(Math.random() * bestActions.length)]; // Exploit (break ties randomly)
                };

                const reset = () => {
                    if(playInterval) clearInterval(playInterval);
                    playBtn.textContent = 'Auto-Play';
                    Q = {};
                    agentPos = { r: 4, c: 0 };
                    infoEl.textContent = 'Ready to learn!';
                    draw();
                };

                const step = () => {
                    const { r, c } = agentPos;
                    if (grid[r][c] !== 0) { // Terminal state
                        agentPos = { r: 4, c: 0 }; // Reset episode
                        infoEl.textContent = 'Episode finished. Agent reset.';
                        draw();
                        return;
                    }

                    const action_idx = chooseAction(r, c);
                    const [dr, dc] = actions[action_idx];
                    let next_r = r + dr;
                    let next_c = c + dc;
                    let reward = -0.1;

                    if (next_r < 0 || next_r >= GRID_SIZE || next_c < 0 || next_c >= GRID_SIZE) {
                        next_r = r; next_c = c; reward = -1; // Bumped into wall
                    }
                     if (grid[next_r][next_c] === 1) reward = 10;
                     if (grid[next_r][next_c] === -1) reward = -10;

                    const q_current = getQ(r, c)[action_idx];
                    let td_target;
                    
                    if (currentAlgorithm === 'sarsa') {
                        const next_action_idx = chooseAction(next_r, next_c);
                        const q_next = getQ(next_r, next_c)[next_action_idx];
                        td_target = reward + GAMMA * q_next;
                         infoEl.textContent = `SARSA: Took action ${action_idx}, will take ${next_action_idx} next.`;
                    } else { // Q-Learning
                        const q_next_values = getQ(next_r, next_c);
                        const max_q_next = Math.max(...q_next_values);
                        td_target = reward + GAMMA * max_q_next;
                        infoEl.textContent = `Q-Learning: Took action ${action_idx}, but updated with max Q-value.`;
                    }
                    
                    const new_q = q_current + ALPHA * (td_target - q_current);
                    getQ(r, c)[action_idx] = new_q;
                    
                    agentPos = { r: next_r, c: next_c };
                    draw();
                };

                const draw = () => {
                    ctx.clearRect(0, 0, canvas.width, canvas.height);
                    
                    // Draw Q-values first
                    for (let r = 0; r < GRID_SIZE; r++) {
                        for (let c = 0; c < GRID_SIZE; c++) {
                            const q_values = getQ(r, c);
                            const maxQ = Math.max(...q_values);
                            const minQ = Math.min(...q_values);

                            for (let i = 0; i < 4; i++) {
                                const q = q_values[i];
                                let colorVal = 0;
                                if (maxQ > minQ) {
                                    colorVal = (q - minQ) / (maxQ - minQ);
                                }
                                ctx.fillStyle = `rgba(74, 144, 226, ${colorVal * 0.7})`; // Blue intensity
                                
                                const [dr, dc] = actions[i];
                                if(dr === -1) ctx.fillRect(c * CELL_SIZE, r * CELL_SIZE, CELL_SIZE, CELL_SIZE / 3); // Up
                                if(dr === 1) ctx.fillRect(c * CELL_SIZE, (r + 2/3) * CELL_SIZE, CELL_SIZE, CELL_SIZE / 3); // Down
                                if(dc === -1) ctx.fillRect(c * CELL_SIZE, r * CELL_SIZE, CELL_SIZE / 3, CELL_SIZE); // Left
                                if(dc === 1) ctx.fillRect((c + 2/3) * CELL_SIZE, r * CELL_SIZE, CELL_SIZE/3, CELL_SIZE); // Right
                            }
                        }
                    }

                    // Draw grid lines and terminals
                    for (let r = 0; r < GRID_SIZE; r++) {
                        for (let c = 0; c < GRID_SIZE; c++) {
                            ctx.strokeStyle = '#e5e7eb';
                            ctx.strokeRect(c * CELL_SIZE, r * CELL_SIZE, CELL_SIZE, CELL_SIZE);
                             if (grid[r][c] === 1) {
                                ctx.fillStyle = 'rgba(74, 222, 128, 0.8)';
                                ctx.fillRect(c * CELL_SIZE, r * CELL_SIZE, CELL_SIZE, CELL_SIZE);
                             } else if (grid[r][c] === -1) {
                                ctx.fillStyle = 'rgba(248, 113, 113, 0.8)';
                                ctx.fillRect(c * CELL_SIZE, r * CELL_SIZE, CELL_SIZE, CELL_SIZE);
                             }
                        }
                    }

                    // Draw agent
                    ctx.fillStyle = '#D0021B';
                    ctx.beginPath();
                    ctx.arc(agentPos.c * CELL_SIZE + CELL_SIZE/2, agentPos.r * CELL_SIZE + CELL_SIZE/2, CELL_SIZE/3, 0, 2*Math.PI);
                    ctx.fill();
                };

                const togglePlay = () => {
                    if (playInterval) {
                        clearInterval(playInterval);
                        playInterval = null;
                        playBtn.textContent = 'Auto-Play';
                    } else {
                        playInterval = setInterval(step, 100);
                        playBtn.textContent = 'Pause';
                    }
                }

                algoSelect.addEventListener('change', (e) => {
                    currentAlgorithm = e.target.value;
                    reset();
                });
                stepBtn.addEventListener('click', step);
                playBtn.addEventListener('click', togglePlay);
                resetBtn.addEventListener('click', reset);
                
                reset();
            }

            // --- MCTS Animation ---
            function initMCTSAnimation() {
                const canvas = document.getElementById('mcts-canvas');
                if (!canvas || canvas.dataset.initialized) return;
                canvas.dataset.initialized = 'true';

                const ctx = canvas.getContext('2d');
                const statusEl = document.getElementById('mcts-status');
                const stepBtn = document.getElementById('mcts-step-btn');
                const playBtn = document.getElementById('mcts-play-btn');
                const resetBtn = document.getElementById('mcts-reset-btn');

                let tree = null;
                let nodeIdCounter = 0;
                let isPlaying = false;
                let playInterval = null;

                const NODE_RADIUS = 25;
                const EXPLORATION_CONSTANT = 1.41; // sqrt(2)

                function createNode(parent) {
                    return {
                        id: nodeIdCounter++,
                        parent: parent,
                        children: [],
                        visits: 0,
                        value: 0,
                        x: 0, y: 0,
                        numUnexplored: Math.floor(2 + Math.random() * 2) // Each node has 2-3 possible moves
                    };
                }

                function reset() {
                    nodeIdCounter = 0;
                    tree = createNode(null);
                    tree.x = canvas.width / 2;
                    tree.y = 50;
                    statusEl.textContent = "Status: Ready";
                    if(playInterval) clearInterval(playInterval);
                    isPlaying = false;
                    playBtn.textContent = 'Auto-Play';
                    stepBtn.disabled = false;
                    drawTree();
                }

                function drawNode(node, highlight = 'black') {
                    ctx.beginPath();
                    ctx.arc(node.x, node.y, NODE_RADIUS, 0, Math.PI * 2);
                    ctx.fillStyle = 'white';
                    ctx.fill();
                    ctx.strokeStyle = highlight;
                    ctx.lineWidth = highlight === 'black' ? 2 : 4;
                    ctx.stroke();

                    ctx.fillStyle = '#3D4451';
                    ctx.font = '12px Inter';
                    ctx.textAlign = 'center';
                    ctx.textBaseline = 'middle';
                    const valueText = `${node.value.toFixed(1)} / ${node.visits}`;
                    ctx.fillText(valueText, node.x, node.y);
                    ctx.font = '10px Inter';
                    ctx.fillText(`id: ${node.id}`, node.x, node.y + 12);
                }

                function drawEdge(parent, child) {
                    ctx.beginPath();
                    ctx.moveTo(parent.x, parent.y + NODE_RADIUS);
                    ctx.lineTo(child.x, child.y - NODE_RADIUS);
                    ctx.strokeStyle = '#a1a1aa';
                    ctx.lineWidth = 1;
                    ctx.stroke();
                }
                
                function setNodePositions(node, x, y, width) {
                    node.x = x;
                    node.y = y;
                    if (node.children.length === 0) return;

                    const childWidth = width / node.children.length;
                    node.children.forEach((child, i) => {
                        setNodePositions(child, x - width/2 + childWidth/2 + i*childWidth, y + 80, childWidth);
                    });
                }

                function drawTree(highlightPath = []) {
                    ctx.clearRect(0, 0, canvas.width, canvas.height);
                    setNodePositions(tree, canvas.width / 2, 50, canvas.width * 0.8);
                    
                    const nodesToDraw = [tree];
                    while(nodesToDraw.length > 0) {
                        const node = nodesToDraw.pop();
                        if (node.parent) drawEdge(node.parent, node);
                        nodesToDraw.push(...node.children);
                    }

                     const highlightSet = new Set(highlightPath.map(n => n.id));
                     const nodesToDrawAgain = [tree];
                      while(nodesToDrawAgain.length > 0) {
                        const node = nodesToDrawAgain.pop();
                        const highlightColor = highlightSet.has(node.id) ? '#4A90E2' : 'black';
                        drawNode(node, highlightColor);
                        nodesToDrawAgain.push(...node.children);
                    }
                }
                
                function ucb1(node) {
                    if (node.visits === 0) return Infinity;
                    const exploitation = node.value / node.visits;
                    const exploration = EXPLORATION_CONSTANT * Math.sqrt(Math.log(node.parent.visits) / node.visits);
                    return exploitation + exploration;
                }

                async function runIteration() {
                    stepBtn.disabled = true;
                    
                    // 1. Selection
                    statusEl.textContent = '1. Selection: Finding best leaf...';
                    let currentNode = tree;
                    let path = [currentNode];
                    while (currentNode.children.length > 0 && currentNode.numUnexplored === 0) {
                        currentNode = currentNode.children.reduce((best, child) => ucb1(child) > ucb1(best) ? child : best);
                        path.push(currentNode);
                        drawTree(path);
                        await new Promise(r => setTimeout(r, 400));
                    }
                    drawTree(path);

                    // 2. Expansion
                    statusEl.textContent = '2. Expansion: Adding a new node.';
                    await new Promise(r => setTimeout(r, 500));
                    if (currentNode.numUnexplored > 0) {
                        const newNode = createNode(currentNode);
                        currentNode.children.push(newNode);
                        currentNode.numUnexplored--;
                        currentNode = newNode; // The new node becomes current for simulation
                        path.push(currentNode);
                    }
                    drawTree(path);


                    // 3. Simulation
                    statusEl.textContent = '3. Simulation: Running random rollout...';
                    await new Promise(r => setTimeout(r, 500));
                    const result = Math.random() > 0.5 ? 1 : -1; // Win or Loss
                     // Simple visual for simulation
                    ctx.beginPath();
                    ctx.arc(currentNode.x, currentNode.y, NODE_RADIUS + 5, 0, Math.PI * 2);
                    ctx.strokeStyle = result > 0 ? '#10B981' : '#EF4444';
                    ctx.lineWidth = 4;
                    ctx.stroke();
                    ctx.font = 'bold 20px Inter';
                    ctx.fillStyle = result > 0 ? '#10B981' : '#EF4444';
                    ctx.fillText(result > 0 ? '+1' : '-1', currentNode.x + 40, currentNode.y);


                    // 4. Backpropagation
                    statusEl.textContent = '4. Backpropagation: Updating values.';
                    await new Promise(r => setTimeout(r, 800));
                    for (let i = path.length - 1; i >= 0; i--) {
                        const node = path[i];
                        node.visits++;
                        node.value += result;
                        drawTree(path.slice(0, i + 1));
                         await new Promise(r => setTimeout(r, 400));
                    }

                    statusEl.textContent = "Status: Ready";
                    stepBtn.disabled = false;
                }

                stepBtn.addEventListener('click', runIteration);
                resetBtn.addEventListener('click', reset);
                playBtn.addEventListener('click', () => {
                    isPlaying = !isPlaying;
                    if (isPlaying) {
                        playBtn.textContent = 'Pause';
                        stepBtn.disabled = true;
                        runIteration();
                        playInterval = setInterval(runIteration, (path) => (path ? path.length * 800 + 2200 : 5000));
                    } else {
                        playBtn.textContent = 'Auto-Play';
                        stepBtn.disabled = false;
                        clearInterval(playInterval);
                    }
                });

                reset();
            }

        });
    </script>
</body>
</html>


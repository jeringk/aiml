<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Interactive Reinforcement Learning Canvas</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #FAF8F5; /* Warm off-white */
            color: #3D4451; /* Soft dark gray for text */
        }
        aside {
            background-color: #F5F2ED; /* Beige sidebar */
        }
        .sidebar-item {
            border-left: 4px solid transparent;
            transition: all 0.2s ease-in-out;
        }
        .sidebar-item.active {
            background-color: #E9E4DB; /* Darker beige */
            color: #000000;
            border-left-color: #4A90E2; /* Accent blue */
            font-weight: 600;
        }
        .sidebar-item:not(.active):hover {
            background-color: #F0ECE5; /* Lighter beige for hover */
        }
        .content-section {
            display: none;
        }
        .content-section.active {
            display: block;
        }
        .card {
            background-color: #ffffff;
            border: 1px solid #EAEAEA;
        }
        canvas {
            background-color: #fcfcfc;
            border-radius: 0.5rem;
            border: 1px solid #e5e7eb;
            width: 100%;
            height: auto;
        }
        .formula {
            background-color: #F5F2ED;
            padding: 1rem;
            border-radius: 0.5rem;
            margin-top: 0.5rem;
            text-align: center;
            font-size: 1.1rem;
            overflow-x: auto;
        }
        .quiz-feedback {
            padding: 0.5rem 1rem;
            border-radius: 0.375rem;
            margin-top: 1rem;
            font-weight: 500;
        }
        .quiz-feedback.correct {
            background-color: #dcfce7; /* green-100 */
            color: #166534; /* green-800 */
        }
        .quiz-feedback.incorrect {
            background-color: #fee2e2; /* red-100 */
            color: #991b1b; /* red-800 */
        }
        .btn-primary {
             background-color: #4A90E2;
             color: white;
        }
        .btn-primary:hover {
            background-color: #357ABD;
        }
         .btn-secondary {
            background-color: #7DBA31;
             color: white;
        }
        .btn-secondary:hover {
            background-color: #689D2A;
        }
        .btn-danger {
            background-color: #D0021B;
            color: white;
        }
        .btn-danger:hover {
            background-color: #B00216;
        }
        .td-q-value {
            font-size: 10px;
            position: absolute;
            color: #555;
            font-family: monospace;
        }
        .solution-link {
            color: #4A90E2;
            text-decoration: underline;
            cursor: pointer;
        }
        .code-block {
            background-color: #F5F2ED;
            padding: 1rem;
            border-radius: 0.5rem;
            margin-top: 0.5rem;
            font-family: monospace;
            white-space: pre-wrap;
            text-align: left;
        }
    </style>
</head>
<body class="relative min-h-screen md:flex">

    <!-- Sidebar Navigation -->
    <aside id="sidebar" class="w-64 flex-shrink-0 p-6 overflow-y-auto h-full fixed top-0 left-0 z-40 transform -translate-x-full transition-transform duration-300 ease-in-out md:relative md:translate-x-0">
        <h2 class="text-xl font-bold text-[#8A6D53] mb-6 px-4">DRL Guide</h2>
        <nav id="sidebar-nav" class="space-y-1">
            <!-- Navigation items will be injected by JS -->
        </nav>
    </aside>

    <!-- Overlay for mobile -->
    <div id="sidebar-overlay" class="fixed inset-0 bg-black opacity-50 z-30 hidden md:hidden"></div>

    <!-- Main Content -->
    <main class="flex-1 p-4 sm:p-8 lg:p-12 overflow-y-auto">
        <header class="flex justify-between items-center mb-6 pb-4 border-b border-gray-200">
             <button id="hamburger-btn" class="md:hidden p-2">
                <svg class="w-6 h-6" fill="none" stroke="currentColor" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16"></path></svg>
             </button>
            <h2 id="main-title" class="text-xl sm:text-2xl lg:text-3xl font-bold text-gray-900"></h2>
            <a href="https://jeringk.github.io/aiml" class="text-sm font-medium text-[#8A6D53] hover:underline">Home</a>
        </header>
        <div id="content-container">
            <!-- Content sections will be injected here -->
        </div>
    </main>

    <script>
        document.addEventListener('DOMContentLoaded', () => {
            const topics = [
                { id: 'intro-rl', title: 'Introduction to RL', content: `
                    <p class="text-lg text-gray-700 mb-6">Reinforcement Learning is an area of machine learning where an agent learns to make decisions by performing actions in an environment to maximize cumulative reward. Unlike supervised learning, the agent is not told which actions to take, but instead must discover which actions yield the most reward by trying them.</p>
                    <div class="card p-6 rounded-lg">
                        <h3 class="text-xl font-semibold text-gray-900 mb-3">The Agent-Environment Loop</h3>
                        <p class="text-gray-600 mb-4">The core of RL is the interaction loop between the agent and the environment. At each step, the agent observes a state, takes an action, receives a reward, and observes the next state. This process continues, and the agent's goal is to learn a strategy, or policy, that maximizes its total reward over time.</p>
                        <ul class="list-disc list-inside text-gray-600 space-y-2">
                            <li><strong>Agent:</strong> The learner or decision-maker.</li>
                            <li><strong>Environment:</strong> Everything the agent interacts with.</li>
                            <li><strong>Action (A<sub>t</sub>):</strong> A choice made by the agent.</li>
                            <li><strong>State (S<sub>t</sub>):</strong> The agent's current situation.</li>
                            <li><strong>Reward (R<sub>t</sub>):</strong> Immediate feedback from the environment.</li>
                        </ul>
                    </div>
                    <div class="mt-6 card p-6 rounded-lg quiz-container" data-quiz-id="intro-rl-quiz" data-correct-answer="b">
                        <h3 class="text-xl font-semibold text-gray-900 mb-3">Quick Quiz</h3>
                        <p class="font-medium text-gray-700 mb-2">What is the primary goal of a Reinforcement Learning agent?</p>
                        <div class="space-y-2">
                            <label class="flex items-center"><input type="radio" name="intro-rl-quiz" value="a" class="mr-2"> Minimize computation time.</label>
                            <label class="flex items-center"><input type="radio" name="intro-rl-quiz" value="b" class="mr-2"> Maximize cumulative reward.</label>
                            <label class="flex items-center"><input type="radio" name="intro-rl-quiz" value="c" class="mr-2"> Classify data accurately.</label>
                            <label class="flex items-center"><input type="radio" name="intro-rl-quiz" value="d" class="mr-2"> Follow a predefined path.</label>
                        </div>
                        <button class="quiz-submit-btn mt-4 btn-primary font-semibold py-2 px-4 rounded">Check Answer</button>
                        <div class="quiz-feedback hidden"></div>
                    </div>
                `},
                { id: 'elements-rl', title: 'Elements of RL', content: `
                    <div class="space-y-6">
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Policy (π)</h3>
                            <p class="text-gray-600 mb-4">The agent's strategy for selecting actions. It can be deterministic (a specific action for a state) or stochastic (a probability distribution over actions).</p>
                            <div class="formula" data-katex="a = \\pi(s) \\quad \\text{(deterministic)}"></div>
                            <div class="formula mt-2" data-katex="\\pi(a|s) = P[A_t=a | S_t=s] \\quad \\text{(stochastic)}"></div>
                        </div>
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Reward Signal (R)</h3>
                            <p class="text-gray-600">A scalar feedback from the environment indicating the immediate benefit of an action. The agent's sole objective is to maximize the total reward it receives.</p>
                        </div>
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Value Function (V, Q)</h3>
                            <p class="text-gray-600">The expected long-term return from a state or state-action pair, following a particular policy. It quantifies the "goodness" of a state. The return $G_t$ is the sum of discounted future rewards.</p>
                            <div class="formula" data-katex="\\begin{aligned}G_t &= R_{t+1} + \\gamma R_{t+2} + \\dots \\\\ &= \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\\end{aligned}"></div>
                        </div>
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Model of the Environment</h3>
                            <p class="text-gray-600">The agent’s representation of how the environment behaves. It predicts state transitions ($P(s'|s,a)$) and rewards ($R(s,a,s')$). RL methods can be model-based or model-free.</p>
                        </div>
                    </div>
                    <div class="mt-6 card p-6 rounded-lg quiz-container" data-quiz-id="elements-rl-quiz" data-correct-answer="c">
                        <h3 class="text-xl font-semibold text-gray-900 mb-3">Quick Quiz</h3>
                        <p class="font-medium text-gray-700 mb-2">Which element of RL defines the agent's strategy or "brain"?</p>
                        <div class="space-y-2">
                            <label class="flex items-center"><input type="radio" name="elements-rl-quiz" value="a" class="mr-2"> Reward Signal</label>
                            <label class="flex items-center"><input type="radio" name="elements-rl-quiz" value="b" class="mr-2"> Value Function</label>
                            <label class="flex items-center"><input type="radio" name="elements-rl-quiz" value="c" class="mr-2"> Policy</label>
                            <label class="flex items-center"><input type="radio" name="elements-rl-quiz" value="d" class="mr-2"> Model</label>
                        </div>
                        <button class="quiz-submit-btn mt-4 btn-primary font-semibold py-2 px-4 rounded">Check Answer</button>
                        <div class="quiz-feedback hidden"></div>
                    </div>
                `},
                { id: 'mab', title: 'Multi-Armed Bandit', content: `
                    <p class="text-lg text-gray-700 mb-6">The MAB problem exemplifies the exploration-exploitation tradeoff. The agent must choose between multiple options (arms) to maximize total reward, without knowing which option is best. It must 'exploit' the best-known option but also 'explore' others to find potentially better ones.</p>
                    <div class="space-y-6">
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Action-Value Estimation & ε-Greedy</h3>
                            <p class="text-gray-600">We estimate the value of each action $Q(a)$ by averaging rewards. The ε-Greedy policy balances exploring random actions with exploiting the current best action.</p>
                            <div class="formula" data-katex="Q_{n+1} = Q_n + \\frac{1}{n}(R_n - Q_n)"></div>
                        </div>
                        <div class="card p-6 rounded-lg">
                             <h3 class="text-xl font-semibold text-gray-900 mb-2">Bandit Gradient Algorithm</h3>
                             <p class="text-gray-600">Instead of estimating action values, this method learns a preference $H_t(a)$ for each action. Preferences are updated via stochastic gradient ascent. Actions are then selected via a softmax distribution over preferences.</p>
                             <div class="formula" data-katex="H_{t+1}(A_t) = H_t(A_t) + \\alpha(R_t - \\bar{R}_t)(1 - \\pi_t(A_t))"></div>
                        </div>
                         <div class="card p-6 rounded-lg">
                             <h3 class="text-xl font-semibold text-gray-900 mb-2">Associative Search (Contextual Bandits)</h3>
                             <p class="text-gray-600">This is an extension where the agent is given a "context" or state before making a choice. The goal is to learn the best action for each context, effectively learning a policy $\\pi(a|s)$ instead of just finding the single best action overall.</p>
                        </div>
                    </div>
                    <div class="card p-6 rounded-lg mt-6">
                        <h3 class="text-xl font-semibold text-gray-900 mt-6 mb-4">Interactive Bandit Simulation (ε-Greedy)</h3>
                        <div class="flex flex-wrap items-center gap-4 mb-4">
                            <label for="epsilon-slider" class="text-gray-600">Exploration Rate (ε):</label>
                            <input type="range" id="epsilon-slider" min="0" max="1" step="0.01" value="0.1" class="w-48">
                            <span id="epsilon-value" class="font-mono text-blue-600">0.10</span>
                            <button id="reset-bandit-btn" class="btn-danger font-semibold py-2 px-4 rounded">Reset</button>
                        </div>
                        <div id="bandit-arms-container" class="flex justify-around items-end h-64 p-4 bg-gray-50 rounded-lg"></div>
                        <div class="mt-4 grid grid-cols-2 gap-4 text-center">
                            <div><p class="text-gray-600">Total Reward</p><p id="bandit-total-reward" class="text-2xl font-bold text-gray-900">0</p></div>
                            <div><p class="text-gray-600">Total Pulls</p><p id="bandit-total-pulls" class="text-2xl font-bold text-gray-900">0</p></div>
                        </div>
                    </div>
                     <div class="mt-6 card p-6 rounded-lg quiz-container" data-quiz-id="mab-quiz" data-correct-answer="b">
                        <h3 class="text-xl font-semibold text-gray-900 mb-3">Quick Quiz</h3>
                        <p class="font-medium text-gray-700 mb-2">What fundamental tradeoff is the Multi-Armed Bandit problem designed to illustrate?</p>
                        <div class="space-y-2">
                            <label class="flex items-center"><input type="radio" name="mab-quiz" value="a" class="mr-2"> Bias vs. Variance</label>
                            <label class="flex items-center"><input type="radio" name="mab-quiz" value="b" class="mr-2"> Exploration vs. Exploitation</label>
                            <label class="flex items-center"><input type="radio" name="mab-quiz" value="c" class="mr-2"> Speed vs. Accuracy</label>
                            <label class="flex items-center"><input type="radio" name="mab-quiz" value="d" class="mr-2"> Model-based vs. Model-free</label>
                        </div>
                        <button class="quiz-submit-btn mt-4 btn-primary font-semibold py-2 px-4 rounded">Check Answer</button>
                        <div class="quiz-feedback hidden"></div>
                    </div>
                `},
                { id: 'mdp', title: 'Markov Decision Processes', content: `
                    <p class="text-lg text-gray-700 mb-6">An MDP is a formal framework for RL problems. A key assumption is the Markov Property: the future is independent of the past given the present. An MDP is defined by a tuple $(S, A, P, R, \\gamma)$.</p>
                    <div class="space-y-6">
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Value Functions & Bellman Equations</h3>
                            <p class="text-gray-600">The Bellman expectation equation expresses the value of a state in terms of the values of successor states, creating a recursive relationship:</p>
                            <div class="formula" data-katex="\\begin{aligned}v_\\pi(s) = \\sum_{a} \\pi(a|s) \\sum_{s', r} & p(s', r | s, a) \\\\ & \\cdot [r + \\gamma v_\\pi(s')].\\end{aligned}"></div>
                        </div>
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Optimal Value Functions & Policies</h3>
                            <p class="text-gray-600">An optimal policy $\\pi_*$ is one that maximizes the expected return from any state. There is always at least one such policy. Optimal policies share the same optimal value functions, $v_*(s)$ and $q_*(s,a)$. The Bellman optimality equations show how they relate:</p>
                            <div class="formula" data-katex="v_*(s) = \\max_a \\sum_{s', r} p(s', r | s, a) [r + \\gamma v_*(s')]"></div>
                        </div>
                    </div>
                    <div class="card p-6 rounded-lg mt-6">
                        <h3 class="text-xl font-semibold text-gray-900 mt-6 mb-4">Interactive Gridworld</h3>
                        <p class="text-gray-600 mb-4">This Gridworld is an MDP. The agent learns the value of each state by iteratively applying the Bellman equation in an algorithm called Value Iteration.</p>
                        <canvas id="gridworld-canvas"></canvas>
                        <div class="mt-4 flex flex-wrap gap-4">
                            <button id="gridworld-vi-btn" class="btn-primary font-semibold py-2 px-4 rounded">Run Value Iteration</button>
                            <button id="gridworld-reset-btn" class="bg-gray-200 hover:bg-gray-300 text-gray-800 font-semibold py-2 px-4 rounded">Reset</button>
                        </div>
                         <div class="mt-4"><p id="gridworld-iterations" class="text-gray-600">Iterations: 0</p></div>
                    </div>
                     <div class="mt-6 card p-6 rounded-lg quiz-container" data-quiz-id="mdp-quiz" data-correct-answer="c">
                        <h3 class="text-xl font-semibold text-gray-900 mb-3">Quick Quiz</h3>
                        <p class="font-medium text-gray-700 mb-2">The Bellman equation provides a recursive relationship for which of the following?</p>
                        <div class="space-y-2">
                            <label class="flex items-center"><input type="radio" name="mdp-quiz" value="a" class="mr-2"> The policy</label>
                            <label class="flex items-center"><input type="radio" name="mdp-quiz" value="b" class="mr-2"> The reward signal</label>
                            <label class="flex items-center"><input type="radio" name="mdp-quiz" value="c" class="mr-2"> The value function</label>
                            <label class="flex items-center"><input type="radio" name="mdp-quiz" value="d" class="mr-2"> The state transitions</label>
                        </div>
                        <button class="quiz-submit-btn mt-4 btn-primary font-semibold py-2 px-4 rounded">Check Answer</button>
                        <div class="quiz-feedback hidden"></div>
                    </div>
                `},
                { id: 'dp', title: 'Dynamic Programming', content: `
                    <p class="text-lg text-gray-700 mb-6">Dynamic Programming (DP) refers to a collection of algorithms that can compute optimal policies given a perfect model of the environment as a Markov Decision Process (MDP). The key idea is to use value functions to organize and structure the search for good policies. DP methods are powerful but are limited by the need for a full model and their computational expense.</p>
                    <div class="space-y-6">
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Policy Iteration</h3>
                            <p class="text-gray-600">This method finds an optimal policy by alternating between two processes: policy evaluation and policy improvement.</p>
                            <ul class="list-decimal list-inside text-gray-600 space-y-2 mt-4">
                                <li><strong>Policy Evaluation:</strong> For the current policy $\\pi$, we compute the state-value function $v_\\pi$. This is done by iteratively applying the Bellman expectation equation until the values converge. For each state $s$:</li>
                                <div class="formula" data-katex="v_{k+1}(s) = \\sum_{a} \\pi(a|s) \\sum_{s', r} p(s', r | s, a) [r + \\gamma v_k(s')]"></div>
                                <li><strong>Policy Improvement:</strong> Once we have $v_\\pi$, we improve the policy by acting greedily with respect to it. For each state $s$, we find the action that maximizes the expected return:</li>
                                <div class="formula" data-katex="\\pi'(s) = \\arg\\max_a \\sum_{s', r} p(s', r | s, a) [r + \\gamma v_\\pi(s')]"></div>
                            </ul>
                            <p class="text-gray-600 mt-4">This process is guaranteed to converge to an optimal policy.</p>
                        </div>
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Value Iteration</h3>
                            <p class="text-gray-600">Value Iteration combines policy evaluation and improvement into a single step. It directly finds the optimal value function by iteratively applying the Bellman Optimality update rule. The process is simpler but each iteration is more complex than in policy evaluation.</p>
                            <div class="formula" data-katex="v_{k+1}(s) = \\max_a \\sum_{s', r} p(s', r | s, a) [r + \\gamma v_k(s')]"></div>
                            <p class="text-gray-600 mt-4">Once the value function converges to $v_*$, the optimal policy can be extracted by choosing the action that maximizes the expected return in each state.</p>
                        </div>
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Generalized Policy Iteration (GPI)</h3>
                            <p class="text-gray-600">GPI is the general idea of letting policy evaluation and policy improvement processes interact. One process learns the value function for the current policy, and the other improves the policy based on the current value function. Most RL algorithms can be framed as GPI, where the two processes may not fully complete before the other begins (e.g., updating values only once before improving the policy).</p>
                            <div class="text-center my-4">
                                <svg width="250" height="200" class="mx-auto">
                                    <defs><marker id="arrow" viewBox="0 0 10 10" refX="5" refY="5" markerWidth="6" markerHeight="6" orient="auto-start-reverse"><path d="M 0 0 L 10 5 L 0 10 z" fill="#4A90E2"/></marker></defs>
                                    <ellipse cx="125" cy="50" rx="60" ry="25" fill="#E9E4DB" stroke="#3D4451"/>
                                    <text x="125" y="55" text-anchor="middle" font-size="16">Policy π</text>
                                    <ellipse cx="125" cy="150" rx="60" ry="25" fill="#E9E4DB" stroke="#3D4451"/>
                                    <text x="125" y="155" text-anchor="middle" font-size="16">Value V</text>
                                    <path d="M 125 75 Q 175 100 125 125" stroke="#4A90E2" stroke-width="2" fill="none" marker-end="url(#arrow)"/>
                                    <text x="165" y="95" fill="#4A90E2" font-size="12">Evaluation</text>
                                    <path d="M 125 125 Q 75 100 125 75" stroke="#D0021B" stroke-width="2" fill="none" marker-end="url(#arrow)"/>
                                    <text x="80" y="95" fill="#D0021B" font-size="12" text-anchor="end">Improvement</text>
                                </svg>
                                <p class="text-sm text-gray-500">The policy and value function interact until they are optimal and consistent.</p>
                            </div>
                        </div>
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Efficiency of Dynamic Programming</h3>
                            <p class="text-gray-600">While DP methods are guaranteed to find optimal policies, their practicality is limited. They require a full model of the environment's dynamics, which is often unavailable. Furthermore, they suffer from the <strong>curse of dimensionality</strong>: the state space (and thus the computation required) grows exponentially with the number of state variables. This makes them infeasible for problems with large or continuous state spaces.</p>
                        </div>
                    </div>
                    <div class="mt-6 card p-6 rounded-lg quiz-container" data-quiz-id="dp-quiz" data-correct-answer="a">
                        <h3 class="text-xl font-semibold text-gray-900 mb-3">Quick Quiz</h3>
                        <p class="font-medium text-gray-700 mb-2">What is a key requirement for using Dynamic Programming to solve an RL problem?</p>
                        <div class="space-y-2">
                            <label class="flex items-center"><input type="radio" name="dp-quiz" value="a" class="mr-2"> A perfect model of the environment (MDP).</label>
                            <label class="flex items-center"><input type="radio" name="dp-quiz" value="b" class="mr-2"> A continuous state space.</label>
                            <label class="flex items-center"><input type="radio" name="dp-quiz" value="c" class="mr-2"> No knowledge of the environment.</label>
                            <label class="flex items-center"><input type="radio" name="dp-quiz" value="d" class="mr-2"> Experience from complete episodes.</label>
                        </div>
                        <button class="quiz-submit-btn mt-4 btn-primary font-semibold py-2 px-4 rounded">Check Answer</button>
                        <div class="quiz-feedback hidden"></div>
                    </div>
                `},
                { id: 'mc', title: 'Monte Carlo Methods', content: `
                    <p class="text-lg text-gray-700 mb-6">Monte Carlo (MC) methods are a class of model-free algorithms that learn directly from episodes of experience. Unlike DP, they do not require a model of the environment's dynamics. MC methods wait until the end of an episode to make value-function updates, based on the actual observed return.</p>
                    <div class="space-y-6">
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Monte Carlo Prediction (Policy Evaluation)</h3>
                            <p class="text-gray-600">MC Prediction is used to estimate $v_\\pi(s)$ for a given policy $\\pi$. The value of a state is estimated by averaging the returns that have followed visits to that state. There are two main variants:</p>
                             <ul class="list-disc list-inside text-gray-600 space-y-2 mt-4">
                                <li><strong>First-visit MC:</strong> Averages the returns for the first time a state $s$ is visited in each episode.</li>
                                <li><strong>Every-visit MC:</strong> Averages the returns for every visit to a state $s$ in each episode.</li>
                             </ul>
                             <p class="text-gray-600 mt-2">The update rule is simple: after an episode provides a return $G_t$ following a visit to $S_t$:</p>
                            <div class="formula" data-katex="V(S_t) \\leftarrow V(S_t) + \\alpha (G_t - V(S_t))"></div>
                            <p class="text-gray-600 mt-2">Where $\\alpha$ is a constant step-size parameter.</p>
                        </div>
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Monte Carlo Control</h3>
                            <p class="text-gray-600">MC Control uses the GPI framework to find an optimal policy. It learns action-values $q_\\pi(s,a)$ instead of state-values to make policy improvement possible (since we don't have a model). The loop is similar to Policy Iteration:</p>
                            <ol class="list-decimal list-inside text-gray-600 space-y-1 mt-2">
                                <li>Run an episode following the current policy $\\pi$.</li>
                                <li>For each state-action pair $(s,a)$ in the episode, update its Q-value by averaging returns.</li>
                                <li>Improve the policy by making it $\\epsilon$-greedy with respect to the new Q-values.</li>
                            </ol>
                            <p class="text-gray-600 mt-2">To ensure sufficient exploration, we can use an $\\epsilon$-greedy policy or exploring starts.</p>
                        </div>
                         <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Off-Policy Learning via Importance Sampling</h3>
                            <p class="text-gray-600">Off-policy methods learn about a target policy $\\pi$ while following a different, more exploratory behavior policy $b$. This is crucial for control problems. To correct for the fact that the data comes from $b$, we weight the returns by the <strong>importance sampling ratio</strong>, which is the relative probability of the trajectory under the two policies:</p>
                            <div class="formula" data-katex="\\rho_{t:T-1} = \\prod_{k=t}^{T-1} \\frac{\\pi(A_k|S_k)}{b(A_k|S_k)}"></div>
                            <p class="text-gray-600 mt-2"><strong>Weighted Importance Sampling</strong> is often preferred in practice because it has lower variance. It uses a weighted average for the value function update:</p>
                            <div class="formula" data-katex="V(S_t) \\leftarrow V(S_t) + \\frac{W_t}{C(S_t)}(G_t - V(S_t))"></div>
                            <p class="text-gray-600 mt-2 text-center">where $C(S_t)$ is the sum of weights for state $S_t$.</p>
                        </div>
                    </div>
                    <div class="card p-6 rounded-lg mt-6">
                        <h3 class="text-xl font-semibold text-gray-900 mt-6 mb-4">Interactive MC Example: Blackjack</h3>
                        <p class="text-gray-600 mb-4">Here, an agent learns to play Blackjack using Monte Carlo. It follows a simple policy (stick on 20 or 21, hit otherwise) and learns the value of each state by averaging returns over many games. The state is defined by the player's sum, the dealer's showing card, and whether the player has a usable ace.</p>
                        <div class="flex flex-wrap items-center gap-4 mb-4">
                            <button id="blackjack-play1-btn" class="btn-primary font-semibold py-2 px-4 rounded">Play 1 Game</button>
                            <button id="blackjack-play100-btn" class="btn-secondary font-semibold py-2 px-4 rounded">Play 100 Games</button>
                            <button id="blackjack-reset-btn" class="btn-danger font-semibold py-2 px-4 rounded">Reset</button>
                            <span id="blackjack-games-played" class="text-gray-600">Games Played: 0</span>
                        </div>
                        <canvas id="blackjack-canvas"></canvas>
                    </div>
                    <div class="mt-6 card p-6 rounded-lg quiz-container" data-quiz-id="mc-quiz" data-correct-answer="c">
                        <h3 class="text-xl font-semibold text-gray-900 mb-3">Quick Quiz</h3>
                        <p class="font-medium text-gray-700 mb-2">What is the main advantage of Monte Carlo methods over Dynamic Programming?</p>
                        <div class="space-y-2">
                            <label class="flex items-center"><input type="radio" name="mc-quiz" value="a" class="mr-2"> They are guaranteed to be faster.</label>
                            <label class="flex items-center"><input type="radio" name="mc-quiz" value="b" class="mr-2"> They update values after every step.</label>
                            <label class="flex items-center"><input type="radio" name="mc-quiz" value="c" class="mr-2"> They do not require a model of the environment.</label>
                            <label class="flex items-center"><input type="radio" name="mc-quiz" value="d" class="mr-2"> They have lower variance.</label>
                        </div>
                        <button class="quiz-submit-btn mt-4 btn-primary font-semibold py-2 px-4 rounded">Check Answer</button>
                        <div class="quiz-feedback hidden"></div>
                    </div>
                `},
                { id: 'td', title: 'Temporal Difference Learning', content: `
                    <p class="text-lg text-gray-700 mb-6">TD learning is a central and novel idea in RL. It's a combination of Monte Carlo and Dynamic Programming. Like MC, it learns from raw experience without a model. Like DP, it bootstraps—it updates value estimates based on other learned estimates, without waiting for the final outcome.</p>
                    <div class="space-y-6">
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">TD Prediction</h3>
                            <p class="text-gray-600">TD Prediction estimates $v_\\pi$. The simplest method, TD(0), updates the value of a state $S_t$ towards a <strong>TD Target</strong> ($R_{t+1} + \\gamma V(S_{t+1})$), which is an estimate of the return. The difference between the current value and the target is the <strong>TD Error</strong>.</p>
                            <div class="formula" data-katex="\\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)"></div>
                            <p class="text-gray-600 mt-2">The update rule then becomes:</p>
                             <div class="formula" data-katex="V(S_t) \\leftarrow V(S_t) + \\alpha \\delta_t"></div>
                             <p class="text-gray-600 mt-2">This is powerful because the agent learns after just one step, rather than waiting for the end of an episode.</p>
                        </div>
                        <div class="card p-6 rounded-lg">
                             <h3 class="text-xl font-semibold text-gray-900 mb-2">SARSA: On-Policy TD Control</h3>
                            <p class="text-gray-600">SARSA learns action-values $Q(s,a)$ and uses them to improve the policy. It is <strong>on-policy</strong> because it learns about the policy it is currently following (including its exploratory moves). Its name comes from the quintuple of experience it uses for updates: $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$.</p>
                            <div class="formula" data-katex="Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]"></div>
                             <p class="text-gray-600 mt-2">The target for the update, $Q(S_{t+1}, A_{t+1})$, uses the *actual* action $A_{t+1}$ taken in the next state.</p>
                        </div>
                        <div class="card p-6 rounded-lg">
                             <h3 class="text-xl font-semibold text-gray-900 mb-2">Q-Learning: Off-Policy TD Control</h3>
                            <p class="text-gray-600">Q-learning is a breakthrough off-policy algorithm. It learns the optimal action-value function, $q_*$, regardless of the policy being followed. This allows the agent to explore freely while still learning about the greedy, optimal policy.</p>
                             <div class="formula" data-katex="Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma \\max_{a} Q(S_{t+1}, a) - Q(S_t, A_t)]"></div>
                             <p class="text-gray-600 mt-2">The key difference from SARSA is the $\\max_a$ operator. Q-learning uses the value of the best possible next action to update its current Q-value, rather than the action that was actually taken.</p>
                        </div>
                         <div class="card p-6 rounded-lg">
                             <h3 class="text-xl font-semibold text-gray-900 mb-2">Expected SARSA</h3>
                             <p class="text-gray-600">A more sophisticated version of SARSA that, like Q-learning, is less sensitive to the variance of the next action. Instead of using the Q-value of the specific next action $A_{t+1}$, it uses the *expected* Q-value over all possible next actions, weighted by their probabilities under the current policy $\\pi$.</p>
                            <div class="formula" data-katex="Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[R_{t+1} + \\gamma \\sum_{a} \\pi(a|S_{t+1}) Q(S_{t+1}, a) - Q(S_t, A_t)\\right]"></div>
                        </div>
                    </div>
                     <div class="card p-6 rounded-lg mt-6">
                        <h3 class="text-xl font-semibold text-gray-900 mt-6 mb-4">Interactive TD Gridworld</h3>
                        <p class="text-gray-600 mb-4">Watch an agent learn a path through the grid using TD methods. It has no model; it learns from each step. Notice the difference in the learned Q-values between SARSA (which accounts for exploratory moves) and Q-Learning (which always learns about the greedy path).</p>
                        <div class="flex flex-wrap items-center gap-4 mb-4">
                            <label class="font-medium">Algorithm:</label>
                            <select id="td-algo-select" class="rounded-md border-gray-300">
                                <option value="sarsa">SARSA</option>
                                <option value="q-learning">Q-Learning</option>
                            </select>
                            <button id="td-step-btn" class="btn-primary font-semibold py-2 px-4 rounded">Take Step</button>
                             <button id="td-play-btn" class="btn-secondary font-semibold py-2 px-4 rounded">Auto-Play</button>
                            <button id="td-reset-btn" class="btn-danger font-semibold py-2 px-4 rounded">Reset</button>
                        </div>
                        <div id="td-gridworld-container" class="relative mx-auto" style="max-width: 400px;">
                            <canvas id="td-gridworld-canvas"></canvas>
                        </div>
                         <div id="td-info" class="mt-2 text-sm text-gray-600"></div>
                    </div>
                     <div class="mt-6 card p-6 rounded-lg quiz-container" data-quiz-id="td-quiz" data-correct-answer="d">
                        <h3 class="text-xl font-semibold text-gray-900 mb-3">Quick Quiz</h3>
                        <p class="font-medium text-gray-700 mb-2">What is the key difference in the update rule between SARSA and Q-Learning?</p>
                        <div class="space-y-2">
                            <label class="flex items-center"><input type="radio" name="td-quiz" value="a" class="mr-2"> SARSA uses a smaller learning rate.</label>
                            <label class="flex items-center"><input type="radio" name="td-quiz" value="b" class="mr-2"> Q-Learning does not use a discount factor.</label>
                            <label class="flex items-center"><input type="radio" name="td-quiz" value="c" class="mr-2"> SARSA learns state-values while Q-Learning learns action-values.</label>
                            <label class="flex items-center"><input type="radio" name="td-quiz" value="d" class="mr-2"> Q-Learning uses the max Q-value of the next state for its update target.</label>
                        </div>
                        <button class="quiz-submit-btn mt-4 btn-primary font-semibold py-2 px-4 rounded">Check Answer</button>
                        <div class="quiz-feedback hidden"></div>
                    </div>
                `},
                 { id: 'func-approx', title: 'Function Approximation', content: `
                    <p class="text-lg text-gray-700 mb-6">For many real-world problems, the state space is too large to be stored in a table (like a Q-table). Function approximation allows us to generalize from seen states to unseen states by representing the value function with a parameterized function, such as a linear function or a neural network.</p>
                     <div class="space-y-6">
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">The Curse of Dimensionality</h3>
                            <p class="text-gray-600">This is the core problem that function approximation solves. Tabular methods require memory and computation for every single state. As we add more variables to describe a state, the total number of possible states grows exponentially. This is impractical.</p>
                            <ul class="list-disc list-inside text-gray-600 space-y-2 mt-2">
                                <li><strong>Large State Spaces:</strong> Problems like Chess or Go have a finite but astronomically large number of board positions ($>10^{40}$ in Go). A table is impossible.</li>
                                <li><strong>Continuous State Spaces:</strong> In robotics or autonomous driving, a state is defined by real-valued numbers (e.g., position, velocity, angle). Since these values are continuous, there are infinitely many possible states. A table cannot even be defined.</li>
                            </ul>
                        </div>
                         <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Example: A Smart Thermostat</h3>
                            <p class="text-gray-600">Imagine an agent controlling a thermostat. The state is the current temperature (e.g., 21.34°C). This is a continuous state space. Instead of a table, we can approximate the value function with a simple linear function: $ \\hat{v}(temp, \\mathbf{w}) = w_1 \\cdot temp + w_2 $. Now, the agent only needs to learn two parameters ($w_1, w_2$) instead of an infinite number of values. If the agent learns that 22°C is a good temperature, the function will automatically <strong>generalize</strong> and infer that 22.1°C is also likely a good temperature, even if it has never been experienced before.</p>
                        </div>
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Semi-Gradient TD Methods</h3>
                            <p class="text-gray-600">When we use function approximation, we update the function's weight vector $\\mathbf{w}$ using gradient descent to minimize the error. The update rule for the weights becomes:</p>
                            <div class="formula" data-katex="\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha [R + \\gamma \\hat{v}(S', \\mathbf{w}) - \\hat{v}(S, \\mathbf{w})] \\nabla \\hat{v}(S, \\mathbf{w})"></div>
                            <p class="text-gray-600 mt-2">This is called a "semi-gradient" method because the gradient is taken with respect to the current value estimate $\\hat{v}(S, \\mathbf{w})$, but it ignores the fact that the TD target also depends on $\\mathbf{w}$. Despite this simplification, it works well in practice.</p>
                        </div>
                         <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">The Deadly Triad</h3>
                            <p class="text-gray-600">Combining off-policy methods (like Q-Learning) with function approximation and bootstrapping is notoriously unstable and can cause the learning process to diverge. This combination is known as the "deadly triad," and many advanced Deep RL algorithms are designed specifically to mitigate this instability.</p>
                        </div>
                    </div>
                     <div class="mt-6 card p-6 rounded-lg quiz-container" data-quiz-id="fa-quiz" data-correct-answer="b">
                        <h3 class="text-xl font-semibold text-gray-900 mb-3">Quick Quiz</h3>
                        <p class="font-medium text-gray-700 mb-2">Why is function approximation necessary in some RL problems?</p>
                        <div class="space-y-2">
                            <label class="flex items-center"><input type="radio" name="fa-quiz" value="a" class="mr-2"> To make the rewards higher.</label>
                            <label class="flex items-center"><input type="radio" name="fa-quiz" value="b" class="mr-2"> To handle very large or continuous state spaces.</label>
                            <label class="flex items-center"><input type="radio" name="fa-quiz" value="c" class="mr-2"> To guarantee finding the optimal policy.</label>
                            <label class="flex items-center"><input type="radio" name="fa-quiz" value="d" class="mr-2"> To eliminate the need for a discount factor.</label>
                        </div>
                        <button class="quiz-submit-btn mt-4 btn-primary font-semibold py-2 px-4 rounded">Check Answer</button>
                        <div class="quiz-feedback hidden"></div>
                    </div>
                `},
                 { id: 'deep-rl', title: 'Deep Reinforcement Learning', content: `
                     <p class="text-lg text-gray-700 mb-6">Deep RL uses deep neural networks as function approximators, allowing agents to learn from high-dimensional sensory inputs like images.</p>
                     <div class="space-y-6">
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Deep Q-Network (DQN)</h3>
                            <p class="text-gray-600">DQN stabilized learning by using two key techniques: Experience Replay and Fixed Target Networks. Experience replay stores transitions and samples them randomly to break correlations. A fixed target network provides stable targets for the Q-value updates.</p>
                        </div>
                         <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Double DQN</h3>
                            <p class="text-gray-600">Standard DQN suffers from overestimation bias because it uses the same network to both select the best action and evaluate its value. Double DQN decouples these steps: the main network selects the best action, and the target network evaluates it, reducing the bias.</p>
                            <div class="formula" data-katex="Y_t^{\\text{DoubleDQN}} = R_{t+1} + \\gamma Q(S_{t+1}, \\arg\\max_{a'} Q(S_{t+1}, a'; \\theta_t); \\theta_t^-)"></div>
                        </div>
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Rainbow</h3>
                            <p class="text-gray-600">Rainbow is not a single new algorithm, but a combination of seven improvements to DQN, including Double DQN, Prioritized Replay, and Dueling Networks. Together, they achieve state-of-the-art performance on Atari benchmarks.</p>
                        </div>
                    </div>
                     <div class="mt-6 card p-6 rounded-lg quiz-container" data-quiz-id="drl-quiz" data-correct-answer="c">
                        <h3 class="text-xl font-semibold text-gray-900 mb-3">Quick Quiz</h3>
                        <p class="font-medium text-gray-700 mb-2">What was a key innovation of the Deep Q-Network (DQN) that stabilized training?</p>
                        <div class="space-y-2">
                            <label class="flex items-center"><input type="radio" name="drl-quiz" value="a" class="mr-2"> Using a linear function approximator.</label>
                            <label class="flex items-center"><input type="radio" name="drl-quiz" value="b" class="mr-2"> Learning the policy directly.</label>
                            <label class="flex items-center"><input type="radio" name="drl-quiz" value="c" class="mr-2"> Using Experience Replay and a target network.</label>
                            <label class="flex items-center"><input type="radio" name="drl-quiz" value="d" class="mr-2"> Removing the discount factor.</label>
                        </div>
                        <button class="quiz-submit-btn mt-4 btn-primary font-semibold py-2 px-4 rounded">Check Answer</button>
                        <div class="quiz-feedback hidden"></div>
                    </div>
                `},
                { id: 'policy-grad', title: 'Policy Gradient Methods', content: `
                    <p class="text-lg text-gray-700 mb-6">These methods directly parameterize and optimize the policy $\\pi(a|s, \\mathbf{\\theta})$. They perform gradient ascent on an objective function $J(\\mathbf{\\theta})$ to find the best policy parameters.</p>
                    <div class="space-y-6">
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Policy Gradient Theorem</h3>
                            <p class="text-gray-600">This theorem provides a theoretical foundation by giving an analytic expression for the gradient of the performance objective $J(\\mathbf{\\theta})$, which allows us to perform gradient ascent.</p>
                            <div class="formula" data-katex="\\nabla J(\\mathbf{\\theta}) \\propto \\sum_s d(s) \\sum_a q_\\pi(s,a) \\nabla \\pi(a|s, \\mathbf{\\theta})"></div>
                        </div>
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">REINFORCE with Baseline</h3>
                            <p class="text-gray-600">The basic REINFORCE algorithm has high variance. We can significantly reduce this by subtracting a learned baseline $b(s)$ (often the value function $V(s)$) from the return $G_t$. This doesn't change the expected gradient but lowers its variance.</p>
                            <div class="formula" data-katex="\\mathbf{\\theta} \\leftarrow \\mathbf{\\theta} + \\alpha (G_t - b(S_t)) \\nabla \\ln \\pi(A_t|S_t, \\mathbf{\\theta})"></div>
                        </div>
                         <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Actor-Critic Methods</h3>
                            <p class="text-gray-600">Combine policy gradients with value function learning. The "Actor" (policy) learns what to do, while the "Critic" (value function) evaluates how good those actions are. The critic's feedback (e.g., the TD error) is used to train the actor, reducing variance and improving stability.</p>
                        </div>
                    </div>
                     <div class="mt-6 card p-6 rounded-lg quiz-container" data-quiz-id="pg-quiz" data-correct-answer="b">
                        <h3 class="text-xl font-semibold text-gray-900 mb-3">Quick Quiz</h3>
                        <p class="font-medium text-gray-700 mb-2">What is the primary difference between Policy Gradient methods and Value-based methods?</p>
                        <div class="space-y-2">
                            <label class="flex items-center"><input type="radio" name="pg-quiz" value="a" class="mr-2"> Policy Gradient methods only work in deterministic environments.</label>
                            <label class="flex items-center"><input type="radio" name="pg-quiz" value="b" class="mr-2"> Policy Gradient methods directly optimize a parameterized policy.</label>
                            <label class="flex items-center"><input type="radio" name="pg-quiz" value="c" class="mr-2"> Value-based methods do not use a discount factor.</label>
                            <label class="flex items-center"><input type="radio" name="pg-quiz" value="d" class="mr-2"> Policy Gradient methods do not use neural networks.</label>
                        </div>
                        <button class="quiz-submit-btn mt-4 btn-primary font-semibold py-2 px-4 rounded">Check Answer</button>
                        <div class="quiz-feedback hidden"></div>
                    </div>
                `},
                { id: 'mcts', title: 'MCTS', content: `
                    <div class="card p-6 rounded-lg">
                        <p class="text-gray-600">A powerful planning algorithm that combines the precision of tree search with the generality of random sampling. MCTS doesn't require a model of the environment; instead, it builds a search tree step-by-step based on the outcomes of simulated episodes (rollouts). The process iterates through four main steps:</p>
                        <ul class="list-decimal list-inside text-gray-600 space-y-2 mt-4">
                            <li><strong>Selection:</strong> Starting from the root, traverse the tree by selecting the most promising child nodes until a leaf node is reached. A common selection strategy is UCB1 (Upper Confidence Bound 1), which balances exploitation and exploration.</li>
                             <div class="formula" data-katex="\\text{UCB1}(v) = \\underbrace{\\frac{Q(v)}{N(v)}}_{\\text{Exploitation}} + \\underbrace{c \\sqrt{\\frac{\\ln N(p)}{N(v)}}}_{\\text{Exploration}}"></div>
                             <p class="text-sm text-gray-500 mt-2 text-center">Where $Q(v)$ is total reward from node $v$, $N(v)$ is its visit count, $N(p)$ is the parent's visit count, and $c$ is an exploration constant.</p>
                            <li><strong>Expansion:</strong> Add one or more child nodes to the leaf node, representing new actions to explore.</li>
                            <li><strong>Simulation:</strong> From a new node, run a simulated episode (a "rollout") by choosing random actions until a terminal state is reached. The outcome is the return (e.g., +1 for a win, -1 for a loss).</li>
                            <li><strong>Backpropagation:</strong> Update the value estimates ($Q$) and visit counts ($N$) of all nodes traversed during the selection phase with the return from the simulation.</li>
                        </ul>
                    </div>
                    <div class="card p-6 rounded-lg mt-6">
                        <h3 class="text-xl font-semibold text-gray-900 mb-2">Interactive MCTS Animation</h3>
                        <p class="text-gray-600 mb-4">Watch the MCTS algorithm build a search tree step-by-step. Each iteration performs the four key steps to evaluate the best next move from the root node.</p>
                        <canvas id="mcts-canvas"></canvas>
                        <div class="mt-4 flex flex-wrap items-center gap-4">
                            <button id="mcts-step-btn" class="btn-primary font-semibold py-2 px-4 rounded">Step</button>
                            <button id="mcts-play-btn" class="btn-secondary font-semibold py-2 px-4 rounded">Auto-Play</button>
                            <button id="mcts-reset-btn" class="btn-danger font-semibold py-2 px-4 rounded">Reset</button>
                            <div id="mcts-status" class="text-gray-700 font-medium p-2 bg-yellow-100 rounded">Status: Ready</div>
                        </div>
                    </div>
                    <div class="mt-6 card p-6 rounded-lg quiz-container" data-quiz-id="mcts-quiz" data-correct-answer="d">
                        <h3 class="text-xl font-semibold text-gray-900 mb-3">Quick Quiz</h3>
                        <p class="font-medium text-gray-700 mb-2">The UCB1 formula is primarily used in which step of the MCTS algorithm?</p>
                        <div class="space-y-2">
                            <label class="flex items-center"><input type="radio" name="mcts-quiz" value="a" class="mr-2"> Backpropagation</label>
                            <label class="flex items-center"><input type="radio" name="mcts-quiz" value="b" class="mr-2"> Expansion</label>
                            <label class="flex items-center"><input type="radio" name="mcts-quiz" value="c" class="mr-2"> Simulation</label>
                            <label class="flex items-center"><input type="radio" name="mcts-quiz" value="d" class="mr-2"> Selection</label>
                        </div>
                        <button class="quiz-submit-btn mt-4 btn-primary font-semibold py-2 px-4 rounded">Check Answer</button>
                        <div class="quiz-feedback hidden"></div>
                    </div>
                `},
                { id: 'alphago', title: 'AlphaGo & MuZero', content: `
                     <p class="text-lg text-gray-700 mb-6">These landmark systems from DeepMind showcase the rapid evolution of deep reinforcement learning, combining powerful search algorithms with deep neural networks to achieve superhuman performance in complex strategy games.</p>
                     <div class="space-y-6">
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">AlphaGo: The Hybrid Approach</h3>
                            <p class="text-gray-600">The first program to defeat a professional human Go player, AlphaGo used a combination of supervised learning from human expert games and reinforcement learning from self-play. Its architecture consisted of two main neural networks:</p>
                             <ul class="list-disc list-inside text-gray-600 space-y-2 mt-4">
                                <li><strong>Policy Network:</strong> Trained to predict the next move a human expert would make. This network was first trained on a large dataset of human games (Supervised Learning) and then refined by playing against older versions of itself (Reinforcement Learning).</li>
                                <li><strong>Value Network:</strong> Trained to predict the outcome (win/loss) from any given board position. This helps the MCTS algorithm evaluate positions more efficiently than traditional rollouts.</li>
                             </ul>
                             <p class="text-gray-600 mt-2">During play, MCTS used the policy network to guide its search towards promising moves and the value network to evaluate board positions, making the search far more intelligent and efficient.</p>
                             <div class="code-block">
    1. Supervised Learning (SL) Policy Network (p_sigma):
       - Train on a large dataset of human expert games.
       - Goal: Predict the expert's move given a board state.

    2. Reinforcement Learning (RL) Policy Network (p_rho):
       - Initialize with the weights of the SL policy network.
       - Play games between the current RL network and older versions.
       - Update weights using policy gradients (REINFORCE) to maximize wins.

    3. Value Network (v_theta):
       - Generate a new dataset of self-play games using the RL policy network.
       - Train the value network to predict the winner from game positions.
       - Goal: Minimize error between predicted value and actual game outcome.

    4. Monte Carlo Tree Search (MCTS) for Game Play:
       - Use the RL policy network to select promising moves (Expansion).
       - Use the Value network to evaluate leaf nodes, reducing search depth.
       - Select the final move based on the most visited path in the tree.
                            </div>
                        </div>
                        <div class="card p-6 rounded-lg">
                             <h3 class="text-xl font-semibold text-gray-900 mb-2">AlphaGo Zero: Learning from Scratch</h3>
                             <p class="text-gray-600">AlphaGo Zero surpassed its predecessor by learning entirely through self-play, without any human data. It started with a randomly initialized network and learned the game from first principles. Its key innovation was a single, powerful neural network that combined the policy and value functions. This network takes the board state as input and outputs:</p>
                             <ul class="list-disc list-inside text-gray-600 space-y-2 mt-4">
                                <li>A <strong>policy vector</strong> $\\mathbf{p}$, representing the probability of playing each possible move.</li>
                                <li>A scalar <strong>value</strong> $v$, estimating the probability of the current player winning.</li>
                             </ul>
                             <p class="text-gray-600 mt-2">This single network guided a more efficient MCTS. The policy output was used to direct the search, and the value output was used to evaluate leaf nodes, completely replacing the costly Monte Carlo rollouts used in the original AlphaGo.</p>
                            <div class="code-block">
    1. Initialize network weights θ randomly.
    2. Loop forever:
    3.   Play a game vs. itself using MCTS guided by the network(θ).
    4.   Store game data (state, MCTS_policy, winner).
    5.   Sample a minibatch of data from recent games.
    6.   Train network(θ) to match the MCTS policy and game winner.
                            </div>
                        </div>
                         <div class="card p-6 rounded-lg">
                             <h3 class="text-xl font-semibold text-gray-900 mb-2">MuZero: Learning the Rules</h3>
                             <p class="text-gray-600">MuZero is the most general of the three. It achieves superhuman performance in Go, chess, shogi, and Atari games without being told the rules. Its breakthrough was to learn its own model of the environment dynamics, but only the aspects relevant for planning.</p>
                             <p class="text-gray-600 mt-2">It learns three functions:</p>
                             <ul class="list-disc list-inside text-gray-600 space-y-2 mt-4">
                                <li><strong>Representation ($h$):</strong> Maps a history of observations to a hidden state.</li>
                                <li><strong>Dynamics ($g$):</strong> Predicts the next reward and hidden state given a state and action. $r_k, s_k = g(s_{k-1}, a_k)$.</li>
                                <li><strong>Prediction ($f$):</strong> Predicts the policy and value from a hidden state. $\\mathbf{p}_k, v_k = f(s_k)$.</li>
                             </ul>
                            <p class="text-gray-600 mt-2">The MCTS search in MuZero operates entirely within this learned, latent space. It can "imagine" future sequences of moves using its dynamics and prediction functions to find the best path forward, making it a powerful model-based RL algorithm that learns its own model.</p>
                             <div class="code-block">
    1. Initialize network weights θ (for h, g, f) randomly.
    2. Loop forever:
    3.   Play a game, at each step t:
    4.     Run MCTS using the learned model (h, g, f) to select action.
    5.   Store the full game trajectory.
    6.   Sample a position from the game.
    7.   Train the network(θ) to predict the observed policy, value, and rewards.
                             </div>
                        </div>
                    </div>
                    <div class="mt-6 card p-6 rounded-lg quiz-container" data-quiz-id="alphago-quiz" data-correct-answer="b">
                        <h3 class="text-xl font-semibold text-gray-900 mb-3">Quick Quiz</h3>
                        <p class="font-medium text-gray-700 mb-2">What was a key difference between AlphaGo and AlphaGo Zero?</p>
                        <div class="space-y-2">
                            <label class="flex items-center"><input type="radio" name="alphago-quiz" value="a" class="mr-2"> AlphaGo used MCTS, but AlphaGo Zero did not.</label>
                            <label class="flex items-center"><input type="radio" name="alphago-quiz" value="b" class="mr-2"> AlphaGo Zero learned entirely from self-play without human data.</label>
                            <label class="flex items-center"><input type="radio" name="alphago-quiz" value="c" class="mr-2"> AlphaGo Zero was designed only for the game of Chess.</label>
                            <label class="flex items-center"><input type="radio" name="alphago-quiz" value="d" class="mr-2"> AlphaGo used a single neural network.</label>
                        </div>
                        <button class="quiz-submit-btn mt-4 btn-primary font-semibold py-2 px-4 rounded">Check Answer</button>
                        <div class="quiz-feedback hidden"></div>
                    </div>
                `},
                { id: 'imitation-learning', title: 'Imitation Learning', content: `
                    <div class="card p-6 rounded-lg">
                        <p class="text-gray-600">A family of methods where an agent learns by observing an expert, rather than through its own trial-and-error. This is useful when exploration is costly, dangerous, or when rewards are hard to define.
                           <br>• <strong>Behavioral Cloning:</strong> The simplest approach. It treats imitation as a supervised learning problem, training a policy network to map states to the actions an expert took in those states. Its main weakness is the "covariate shift" problem: if the agent enters a state the expert never saw, it may not know how to recover.
                           <br>• <strong>Inverse Reinforcement Learning (IRL):</strong> A more advanced approach. Instead of just copying actions, IRL tries to infer the expert's hidden reward function. The assumption is that the expert is acting optimally according to some reward signal. Once this reward function is learned, it can be used with any RL algorithm to train a robust policy.
                        </p>
                    </div>
                    <div class="mt-6 card p-6 rounded-lg quiz-container" data-quiz-id="imitation-quiz" data-correct-answer="c">
                        <h3 class="text-xl font-semibold text-gray-900 mb-3">Quick Quiz</h3>
                        <p class="font-medium text-gray-700 mb-2">What is the main weakness of the Behavioral Cloning approach to imitation learning?</p>
                        <div class="space-y-2">
                            <label class="flex items-center"><input type="radio" name="imitation-quiz" value="a" class="mr-2"> It is computationally very expensive.</label>
                            <label class="flex items-center"><input type="radio" name="imitation-quiz" value="b" class="mr-2"> It requires a perfect model of the environment.</label>
                            <label class="flex items-center"><input type="radio" name="imitation-quiz" value="c" class="mr-2"> It can fail in unseen states due to covariate shift.</label>
                            <label class="flex items-center"><input type="radio" name="imitation-quiz" value="d" class="mr-2"> It cannot learn a stochastic policy.</label>
                        </div>
                        <button class="quiz-submit-btn mt-4 btn-primary font-semibold py-2 px-4 rounded">Check Answer</button>
                        <div class="quiz-feedback hidden"></div>
                    </div>
                `},
                { id: 'marl', title: 'Multi-Agent RL', content: `
                    <div class="card p-6 rounded-lg">
                        <p class="text-gray-600">MARL extends RL to systems with multiple agents that learn and act in a shared environment. This introduces significant challenges beyond single-agent RL:
                            <br>• <strong>Non-stationarity:</strong> As other agents learn and change their policies, the environment effectively becomes non-stationary from the perspective of any one agent. What was a good action yesterday might be a bad action today.
                            <br>• <strong>Credit Assignment:</strong> In cooperative settings, it can be difficult to determine which agent's actions were responsible for the team's success or failure.
                            <br>• <strong>Coordination/Competition:</strong> Agents must learn to either coordinate with allies or compete effectively against adversaries.
                            <br>A popular paradigm is "Centralized Training with Decentralized Execution" (CTDE), where agents can share information during training but must act based only on their local observations during execution.
                        </p>
                    </div>
                    <div class="mt-6 card p-6 rounded-lg quiz-container" data-quiz-id="marl-quiz" data-correct-answer="c">
                        <h3 class="text-xl font-semibold text-gray-900 mb-3">Quick Quiz</h3>
                        <p class="font-medium text-gray-700 mb-2">The paradigm where agents share information during training but not execution is called:</p>
                        <div class="space-y-2">
                            <label class="flex items-center"><input type="radio" name="marl-quiz" value="a" class="mr-2"> Independent Q-Learning</label>
                            <label class="flex items-center"><input type="radio" name="marl-quiz" value="b" class="mr-2"> Policy Gradients</label>
                            <label class="flex items-center"><input type="radio" name="marl-quiz" value="c" class="mr-2"> Centralized Training with Decentralized Execution (CTDE)</label>
                            <label class="flex items-center"><input type="radio" name="marl-quiz" value="d" class="mr-2"> Markov Games</label>
                        </div>
                        <button class="quiz-submit-btn mt-4 btn-primary font-semibold py-2 px-4 rounded">Check Answer</button>
                        <div class="quiz-feedback hidden"></div>
                    </div>
                `},
                { id: 'hitl', title: 'Human-in-the-Loop RL', content: `
                    <div class="card p-6 rounded-lg">
                        <p class="text-gray-600">Also known as Interactive RL, this approach integrates human feedback into the agent's learning loop. This is valuable when environmental rewards are sparse or when we want to align the agent's behavior with complex human preferences. The feedback can take many forms:</p>
                         <ul class="list-disc list-inside text-gray-600 space-y-2 mt-4">
                            <li><strong>Reward Shaping:</strong> A human provides additional reward signals to guide the agent.</li>
                            <li><strong>Policy Shaping:</strong> A human can directly influence the agent's actions or policy.</li>
                            <li><strong>Preference-Based RL:</strong> A human provides feedback by comparing two different behaviors from the agent, and the agent learns a reward function that explains these preferences.</li>
                        </ul>
                    </div>
                    <div class="mt-6 card p-6 rounded-lg quiz-container" data-quiz-id="hitl-quiz" data-correct-answer="d">
                        <h3 class="text-xl font-semibold text-gray-900 mb-3">Quick Quiz</h3>
                        <p class="font-medium text-gray-700 mb-2">Which of the following is NOT a common form of human feedback in HITL learning?</p>
                        <div class="space-y-2">
                            <label class="flex items-center"><input type="radio" name="hitl-quiz" value="a" class="mr-2"> Reward Shaping</label>
                            <label class="flex items-center"><input type="radio" name="hitl-quiz" value="b" class="mr-2"> Policy Shaping</label>
                            <label class="flex items-center"><input type="radio" name="hitl-quiz" value="c" class="mr-2"> Preference-Based Feedback</label>
                            <label class="flex items-center"><input type="radio" name="hitl-quiz" value="d" class="mr-2"> State Abstraction</label>
                        </div>
                        <button class="quiz-submit-btn mt-4 btn-primary font-semibold py-2 px-4 rounded">Check Answer</button>
                        <div class="quiz-feedback hidden"></div>
                    </div>
                `},
                { id: 'safety', title: 'Safety in RL', content: `
                    <div class="card p-6 rounded-lg">
                        <p class="text-gray-600">Ensuring RL agents behave safely and reliably is critical for real-world deployment in areas like robotics, autonomous driving, and healthcare. The goal is to prevent the agent from causing harm or entering catastrophic states, especially during its initial exploration phase. Key approaches include:
                            <br>• <strong>Constrained MDPs:</strong> The problem is formulated to maximize a reward function while satisfying explicit safety constraints (e.g., "keep the robot's temperature below a certain threshold").
                            <br>• <strong>Safe Exploration:</strong> Designing algorithms that explore the environment cautiously, using uncertainty estimates to avoid actions that could lead to dangerous, unknown states.
                            <br>• <strong>Shielding:</strong> Using a predefined set of safety rules or a formal verification module (a "shield") to monitor and override the agent's actions if they are deemed unsafe.
                        </p>
                    </div>
                    <div class="mt-6 card p-6 rounded-lg quiz-container" data-quiz-id="safety-quiz" data-correct-answer="a">
                        <h3 class="text-xl font-semibold text-gray-900 mb-3">Quick Quiz</h3>
                        <p class="font-medium text-gray-700 mb-2">Using a formal module to override an agent's potentially unsafe actions is known as:</p>
                        <div class="space-y-2">
                            <label class="flex items-center"><input type="radio" name="safety-quiz" value="a" class="mr-2"> Shielding</label>
                            <label class="flex items-center"><input type="radio" name="safety-quiz" value="b" class="mr-2"> Reward Shaping</label>
                            <label class="flex items-center"><input type="radio" name="safety-quiz" value="c" class="mr-2"> Constrained MDPs</label>
                            <label class="flex items-center"><input type="radio" name="safety-quiz" value="d" class="mr-2"> Safe Exploration</label>
                        </div>
                        <button class="quiz-submit-btn mt-4 btn-primary font-semibold py-2 px-4 rounded">Check Answer</button>
                        <div class="quiz-feedback hidden"></div>
                    </div>
                `},
                { id: 'sample-questions', title: 'Sample Questions', content: `
                    <div class="space-y-8">
                        <!-- Question 1 -->
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Question 1: The Discount Factor</h3>
                            <p class="font-medium text-gray-700 mb-4">An agent is in a maze with two paths to a goal. Path A is short (3 steps) but passes through a fire pit that gives a -10 reward. Path B is long (10 steps) with no negative rewards. Each step gives a reward of -1, and the goal gives +100. How does the discount factor, $\\gamma$, influence which path the agent prefers? </p>
                            <div class="bg-gray-50 p-4 rounded-md">
                                <h4 class="font-semibold text-gray-800">Solution:</h4>
                                <p class="text-gray-600 mt-2">The discount factor $\\gamma$ determines how much the agent values future rewards compared to immediate ones. A low $\\gamma$ makes the agent "myopic," while a high $\\gamma$ makes it "farsighted."</p>
                                <ul class="list-disc list-inside text-gray-600 space-y-1 mt-2">
                                    <li><strong>High $\\gamma$ (e.g., 0.99):</strong> The agent is patient. It will heavily weigh the final +100 reward and will likely choose the longer Path B to avoid the large immediate penalty of the fire pit, as the cumulative step penalties will be less significant than the -10 from the fire.</li>
                                    <li><strong>Low $\\gamma$ (e.g., 0.5):</strong> The agent is impatient. The future +100 reward is heavily discounted. The immediate -10 penalty from the fire pit might be preferable to accumulating many -1 step penalties, making Path A seem more attractive. The agent prioritizes avoiding immediate costs.</li>
                                </ul>
                                <p class="text-gray-600 mt-2">Relevant Section: <span class="solution-link" data-target="elements-rl">Elements of RL (Value Function)</span></p>
                            </div>
                        </div>

                        <!-- Question 2 -->
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Question 2: DP vs. MC Methods</h3>
                            <p class="font-medium text-gray-700 mb-4">When would you prefer a Monte Carlo method over a Dynamic Programming method to solve an MDP? Conversely, when is DP a better choice?</p>
                             <div class="bg-gray-50 p-4 rounded-md">
                                <h4 class="font-semibold text-gray-800">Solution:</h4>
                                <p class="text-gray-600 mt-2">The choice depends primarily on whether you have a model of the environment.</p>
                                <ul class="list-disc list-inside text-gray-600 space-y-1 mt-2">
                                    <li><strong>Prefer Monte Carlo when:</strong> The model of the environment (state transition probabilities and rewards) is unknown or too complex to define. MC methods are model-free and learn directly from experienced episodes. This makes them suitable for complex games or real-world interactions where the rules are not fully known.</li>
                                    <li><strong>Prefer Dynamic Programming when:</strong> You have a perfect model of the MDP. If the state and action spaces are manageably small, DP methods like Value Iteration are highly efficient and guaranteed to find the optimal solution. This is ideal for planning problems like inventory management or resource allocation where the system dynamics are well-defined.</li>
                                </ul>
                                <p class="text-gray-600 mt-2">Relevant Sections: <span class="solution-link" data-target="dp">Dynamic Programming</span> and <span class="solution-link" data-target="mc">Monte Carlo Methods</span></p>
                            </div>
                        </div>

                         <!-- Question 3 -->
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Question 3: TD Learning Update</h3>
                            <p class="font-medium text-gray-700 mb-4">An agent is in state A, where $V(A) = 10$. It takes an action and transitions to state B, where $V(B) = 20$, receiving a reward of $R = 5$. Using a learning rate $\\alpha = 0.1$ and discount factor $\\gamma = 0.9$, calculate the new value for $V(A)$ after this one-step transition using TD(0).</p>
                             <div class="bg-gray-50 p-4 rounded-md">
                                <h4 class="font-semibold text-gray-800">Solution:</h4>
                                <p class="text-gray-600 mt-2">We use the TD(0) update rule: $V(S_t) \\leftarrow V(S_t) + \\alpha [R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)]$.</p>
                                <ol class="list-decimal list-inside text-gray-600 space-y-1 mt-2">
                                    <li><strong>Calculate the TD Target:</strong> $R_{t+1} + \\gamma V(S_{t+1}) = 5 + 0.9 \\times 20 = 5 + 18 = 23$.</li>
                                    <li><strong>Calculate the TD Error:</strong> $\\delta_t = \\text{TD Target} - V(S_t) = 23 - 10 = 13$.</li>
                                    <li><strong>Apply the update:</strong> $V(A)_{new} = V(A)_{old} + \\alpha \\times \\delta_t = 10 + 0.1 \\times 13 = 10 + 1.3 = 11.3$.</li>
                                </ol>
                                 <p class="text-gray-600 mt-2">The new value for state A is 11.3.</p>
                                <p class="text-gray-600 mt-2">Relevant Section: <span class="solution-link" data-target="td">Temporal Difference Learning</span></p>
                            </div>
                        </div>
                        
                         <!-- Question 4 -->
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Question 4: Actor-Critic vs. REINFORCE</h3>
                            <p class="font-medium text-gray-700 mb-4">Why does an Actor-Critic method generally have lower variance than the REINFORCE algorithm? Explain the roles of the actor and critic.</p>
                             <div class="bg-gray-50 p-4 rounded-md">
                                <h4 class="font-semibold text-gray-800">Solution:</h4>
                                <p class="text-gray-600 mt-2">Actor-Critic methods reduce variance by using a more stable and less noisy signal to update the policy.</p>
                                 <ul class="list-disc list-inside text-gray-600 space-y-1 mt-2">
                                    <li><strong>REINFORCE:</strong> Updates the policy based on the full Monte Carlo return ($G_t$) of an entire episode. The return is a sum of many random variables (rewards), which makes it a high-variance estimate of an action's goodness. An action that was good by chance in a successful episode will be heavily reinforced, and vice versa.</li>
                                    <li><strong>Actor-Critic:</strong>
                                        <ul>
                                            <li>The <strong>Actor</strong> is the policy, which decides which action to take.</li>
                                            <li>The <strong>Critic</strong> is a value function (e.g., $V(s)$ or $Q(s,a)$) that learns to evaluate the "goodness" of states or state-action pairs.</li>
                                        </ul>
                                       Instead of using the noisy full return, the Actor uses the Critic's feedback, often in the form of the TD error ($\delta_t$). The TD error is a lower-variance estimate of the action's advantage because it's based on the immediate reward and the value of the next state, rather than the entire sequence of future rewards. This bootstrapping leads to more stable and faster learning.
                                    </li>
                                </ul>
                                <p class="text-gray-600 mt-2">Relevant Section: <span class="solution-link" data-target="policy-grad">Policy Gradient Methods</span></p>
                            </div>
                        </div>

                        <!-- Question 5 -->
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Question 5: UCB Calculation</h3>
                            <p class="font-medium text-gray-700 mb-4">An agent is facing a 3-armed bandit problem. After 50 total pulls ($N=50$), the arms have been played as follows: Arm 1 ($N(a_1)=10$, avg reward $Q(a_1)=0.8$), Arm 2 ($N(a_2)=25$, avg reward $Q(a_2)=0.6$), and Arm 3 ($N(a_3)=15$, avg reward $Q(a_3)=0.7$). Using the UCB1 formula with an exploration constant $c = \\sqrt{2}$, which arm should the agent pull next?</p>
                             <div class="bg-gray-50 p-4 rounded-md">
                                <h4 class="font-semibold text-gray-800">Solution:</h4>
                                <p class="text-gray-600 mt-2">The UCB1 formula is: $UCB1(a) = Q(a) + c \\sqrt{\\frac{\\ln N}{N(a)}}$. We calculate this for each arm:</p>
                                <ul class="list-disc list-inside text-gray-600 space-y-2 mt-2">
                                    <li><b>Arm 1:</b> $0.8 + \\sqrt{2} \\sqrt{\\frac{\\ln 50}{10}} \\approx 0.8 + 1.414 \\times \\sqrt{\\frac{3.912}{10}} \\approx 0.8 + 1.414 \\times 0.625 \\approx 0.8 + 0.884 = 1.684$</li>
                                    <li><b>Arm 2:</b> $0.6 + \\sqrt{2} \\sqrt{\\frac{\\ln 50}{25}} \\approx 0.6 + 1.414 \\times \\sqrt{\\frac{3.912}{25}} \\approx 0.6 + 1.414 \\times 0.396 \\approx 0.6 + 0.560 = 1.160$</li>
                                    <li><b>Arm 3:</b> $0.7 + \\sqrt{2} \\sqrt{\\frac{\\ln 50}{15}} \\approx 0.7 + 1.414 \\times \\sqrt{\\frac{3.912}{15}} \\approx 0.7 + 1.414 \\times 0.511 \\approx 0.7 + 0.722 = 1.422$</li>
                                </ul>
                                <p class="text-gray-600 mt-2">The agent should choose <strong>Arm 1</strong> because it has the highest UCB1 score (1.684), indicating the best balance of high past performance and uncertainty.</p>
                                <p class="text-gray-600 mt-2">Relevant Section: <span class="solution-link" data-target="mcts">MCTS (UCB1)</span></p>
                            </div>
                        </div>

                        <!-- Question 6 -->
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Question 6: SARSA vs. Q-Learning Scenario</h3>
                            <p class="font-medium text-gray-700 mb-4">An agent is in a "windy gridworld." From state $S_t$, it takes action $A_t$ (move Right). Due to wind, it lands in $S_{t+1}$. From $S_{t+1}$, its $\\epsilon$-greedy policy selects an exploratory (non-optimal) action $A_{t+1}$ (move Down) into a penalty square. How would the Q-value updates for $Q(S_t, A_t)$ differ for SARSA and Q-Learning in this specific step?</p>
                             <div class="bg-gray-50 p-4 rounded-md">
                                <h4 class="font-semibold text-gray-800">Solution:</h4>
                                <p class="text-gray-600 mt-2">The difference lies in which next-state action value they use for the update target.</p>
                                <ul class="list-disc list-inside text-gray-600 space-y-2 mt-2">
                                    <li><strong>SARSA (On-Policy):</strong> Its update uses the quintuple $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$. It would use the Q-value of the *actual* next action taken, $Q(S_{t+1}, A_{t+1})$ (move Down). Since this action leads to a penalty, the TD target will be low, and $Q(S_t, A_t)$ will be updated downwards. SARSA learns a "safer" policy that accounts for its own exploration.</li>
                                    <li><strong>Q-Learning (Off-Policy):</strong> Its update rule uses $\\max_a Q(S_{t+1}, a)$. It would ignore the exploratory action $A_{t+1}$ and instead use the Q-value of the *best possible action* from $S_{t+1}$. The update for $Q(S_t, A_t)$ is based on the assumption that the agent will act optimally from now on, resulting in a more optimistic update than SARSA's.</li>
                                </ul>
                                <p class="text-gray-600 mt-2">Relevant Section: <span class="solution-link" data-target="td">Temporal Difference Learning</span></p>
                            </div>
                        </div>

                        <!-- Question 7 -->
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Question 7: DQN Components</h3>
                            <p class="font-medium text-gray-700 mb-4">What are the two primary innovations of the Deep Q-Network (DQN), and what problem does each one solve?</p>
                             <div class="bg-gray-50 p-4 rounded-md">
                                <h4 class="font-semibold text-gray-800">Solution:</h4>
                                <p class="text-gray-600 mt-2">The two key innovations that stabilized Q-learning with deep neural networks are:</p>
                                <ol class="list-decimal list-inside text-gray-600 space-y-2 mt-2">
                                    <li><strong>Experience Replay:</strong>
                                        <ul>
                                            <li><strong>What it is:</strong> Storing experiences $(s, a, r, s')$ in a large memory buffer and sampling random mini-batches from it to train the network.</li>
                                            <li><strong>Problem Solved:</strong> It breaks the strong temporal correlations between consecutive samples, making the data more independent and identically distributed (I.I.D.) and stabilizing training. It also increases data efficiency by reusing experiences.</li>
                                        </ul>
                                    </li>
                                    <li><strong>Fixed Target Network:</strong>
                                        <ul>
                                            <li><strong>What it is:</strong> Using a separate, older copy of the Q-network to generate the TD targets. This target network's weights are only updated periodically.</li>
                                            <li><strong>Problem Solved:</strong> It prevents the learning target from constantly changing. If the same network is used to select and evaluate actions, the target value shifts with every update, leading to oscillations and divergence. A fixed target provides a stable learning signal.</li>
                                        </ul>
                                    </li>
                                </ol>
                                <p class="text-gray-600 mt-2">Relevant Section: <span class="solution-link" data-target="deep-rl">Deep Reinforcement Learning</span></p>
                            </div>
                        </div>
                         <!-- Question 8 -->
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Question 8: The Deadly Triad</h3>
                            <p class="font-medium text-gray-700 mb-4">What is the "deadly triad" in reinforcement learning, and why does it pose a challenge?</p>
                             <div class="bg-gray-50 p-4 rounded-md">
                                <h4 class="font-semibold text-gray-800">Solution:</h4>
                                <p class="text-gray-600 mt-2">The "deadly triad" refers to the combination of three elements that can cause learning to become unstable and diverge:</p>
                                <ol class="list-decimal list-inside text-gray-600 space-y-1 mt-2">
                                   <li><strong>Function Approximation:</strong> Using a parameterized function (like a neural network) instead of a table to represent the value function, which is necessary for large state spaces.</li>
                                   <li><strong>Bootstrapping:</strong> Updating value estimates based on other value estimates (e.g., in TD learning and Q-learning).</li>
                                   <li><strong>Off-policy training:</strong> Learning about a target policy while following a different behavior policy (e.g., in Q-learning).</li>
                                </ol>
                                <p class="text-gray-600 mt-2">The combination of these three can lead to a feedback loop where errors in the value function estimate are amplified, causing the network's weights to explode towards infinity. Many modern deep RL algorithms are designed specifically to mitigate the instability caused by the deadly triad.</p>
                                <p class="text-gray-600 mt-2">Relevant Section: <span class="solution-link" data-target="func-approx">Function Approximation</span></p>
                            </div>
                        </div>

                         <!-- Question 9 -->
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Question 9: Imitation Learning</h3>
                            <p class="font-medium text-gray-700 mb-4">You want to teach a robot to perform a complex task like folding laundry. Why might Imitation Learning be a more practical approach here than standard RL with a sparse reward (e.g., +1 only for a perfectly folded shirt)?</p>
                             <div class="bg-gray-50 p-4 rounded-md">
                                <h4 class="font-semibold text-gray-800">Solution:</h4>
                                <p class="text-gray-600 mt-2">Imitation Learning is often more practical for such tasks for two main reasons:</p>
                                <ol class="list-decimal list-inside text-gray-600 space-y-1 mt-2">
                                   <li><strong>Inefficient Exploration:</strong> With a sparse reward, a standard RL agent would have to explore a vast state-action space randomly for a very long time before it stumbles upon the correct sequence of actions to fold a shirt and receive a reward. The probability of discovering this through random exploration is practically zero. Imitation Learning bypasses this by learning from expert demonstrations, which immediately provides a strong learning signal in the relevant part of the state space.</li>
                                   <li><strong>Complex Reward Function:</strong> Defining a dense reward function that correctly guides the agent through all the intermediate steps of folding laundry is extremely difficult. A poorly designed reward function could lead to unexpected and undesirable behaviors (reward hacking). Imitation Learning, particularly Inverse Reinforcement Learning (IRL), can infer the underlying reward function from demonstrations, which is often easier than specifying it manually.</li>
                                </ol>
                                <p class="text-gray-600 mt-2">Relevant Section: <span class="solution-link" data-target="imitation-learning">Imitation Learning</span></p>
                            </div>
                        </div>
                    </div>
                `},
            ];
            
            const mainContentContainer = document.getElementById('content-container');
            const mainTitle = document.getElementById('main-title');
            const sidebarNav = document.getElementById('sidebar-nav');
            const topicHierarchy = {
                'Introduction': ['intro-rl', 'elements-rl'],
                'Core Problems & Frameworks': ['mab', 'mdp'],
                'Solution Methods': ['dp', 'mc', 'td'],
                'Advanced Topics': ['func-approx', 'deep-rl', 'policy-grad'],
                'Planning & Case Studies': ['mcts', 'alphago', 'imitation-learning'],
                'Frontiers in RL': ['marl', 'hitl', 'safety'],
                'Practice': ['sample-questions']
            };

            // Populate sidebar and content
            Object.entries(topicHierarchy).forEach(([category, topicIds]) => {
                const categoryTitle = document.createElement('h3');
                categoryTitle.className = 'text-xs font-bold text-gray-500 uppercase tracking-wider mt-6 mb-2 px-4';
                categoryTitle.textContent = category;
                sidebarNav.appendChild(categoryTitle);

                topicIds.forEach(topicId => {
                    const topic = topics.find(t => t.id === topicId);
                    if (topic) {
                        const link = document.createElement('a');
                        link.href = '#';
                        link.className = 'block py-2 px-4 rounded-md sidebar-item text-gray-700 text-sm';
                        link.dataset.target = topic.id;
                        link.textContent = topic.title;
                        sidebarNav.appendChild(link);
                        
                        const section = document.createElement('section');
                        section.id = topic.id;
                        section.className = 'content-section';
                        section.innerHTML = topic.content;
                        mainContentContainer.appendChild(section);
                    }
                });
            });

            // Render block-level formulas once after all content is in the DOM.
            // Inline formulas will be handled by the auto-render script in the <head>.
            document.querySelectorAll('.formula').forEach(el => {
                katex.render(el.dataset.katex, el, {
                    throwOnError: false,
                    displayMode: true
                });
            });
            
            // --- Mobile Sidebar Logic ---
            const sidebar = document.getElementById('sidebar');
            const overlay = document.getElementById('sidebar-overlay');
            const hamburgerBtn = document.getElementById('hamburger-btn');

            const toggleSidebar = () => {
                sidebar.classList.toggle('-translate-x-full');
                overlay.classList.toggle('hidden');
            };

            hamburgerBtn.addEventListener('click', toggleSidebar);
            overlay.addEventListener('click', toggleSidebar);


            const navLinks = document.querySelectorAll('.sidebar-item');
            const contentSections = document.querySelectorAll('.content-section');

            function switchTab(targetId) {
                if (!document.getElementById(targetId)) return;

                const currentTopic = topics.find(t => t.id === targetId);
                if (currentTopic) {
                    mainTitle.textContent = currentTopic.title;
                }

                contentSections.forEach(section => {
                    section.classList.toggle('active', section.id === targetId);
                });
                navLinks.forEach(link => {
                    link.classList.toggle('active', link.dataset.target === targetId);
                });
                 // Re-render math in the newly activated tab
                const activeSection = document.getElementById(targetId);
                if (activeSection && window.renderMathInElement) {
                    renderMathInElement(activeSection, {
                        delimiters: [{left: '$', right: '$', display: false}],
                        throwOnError: false
                    });
                }
                
                // Initialize or resize interactive canvas if it's visible
                 const activeCanvasId = `${targetId}-canvas`;
                 const activeCanvas = document.getElementById(activeCanvasId);
                 if(activeCanvas) {
                    if (targetId === 'mab' && !activeCanvas.dataset.initialized) initBanditProblem();
                    if (targetId === 'mdp' && !activeCanvas.dataset.initialized) initGridworld();
                    if (targetId === 'mc' && !activeCanvas.dataset.initialized) initBlackjackMC();
                    if (targetId === 'td' && !activeCanvas.dataset.initialized) initTDGridworld();
                    if (targetId === 'mcts' && !activeCanvas.dataset.initialized) initMCTSAnimation();
                    
                    // Trigger resize for the active canvas
                    window.dispatchEvent(new Event('resize'));
                 }


                // Close sidebar on mobile after navigation
                if (window.innerWidth < 768 && !sidebar.classList.contains('-translate-x-full')) {
                    toggleSidebar();
                }
            }

            mainContentContainer.addEventListener('click', (e) => {
                 if (e.target.classList.contains('solution-link')) {
                    e.preventDefault();
                    const targetId = e.target.dataset.target;
                    switchTab(targetId);
                    document.getElementById(targetId).scrollIntoView({ behavior: 'smooth' });
                 }
            });


            sidebarNav.addEventListener('click', (e) => {
                if (e.target.closest('.sidebar-item')) {
                    e.preventDefault();
                    const targetId = e.target.closest('.sidebar-item').dataset.target;
                    switchTab(targetId);
                }
            });

            // Quiz Handler using Event Delegation
            mainContentContainer.addEventListener('click', (e) => {
                if (e.target.classList.contains('quiz-submit-btn')) {
                    const quizContainer = e.target.closest('.quiz-container');
                    const quizId = quizContainer.dataset.quizId;
                    const correctAnswer = quizContainer.dataset.correctAnswer;
                    const selectedAnswer = quizContainer.querySelector(`input[name="${quizId}"]:checked`);
                    const feedbackEl = quizContainer.querySelector('.quiz-feedback');
                    
                    if (selectedAnswer) {
                        feedbackEl.classList.remove('hidden', 'correct', 'incorrect');
                        if (selectedAnswer.value === correctAnswer) {
                            feedbackEl.textContent = 'Correct! Well done.';
                            feedbackEl.classList.add('correct');
                        } else {
                            feedbackEl.textContent = 'Not quite. Try again!';
                            feedbackEl.classList.add('incorrect');
                        }
                    } else {
                        feedbackEl.classList.remove('hidden', 'correct');
                        feedbackEl.textContent = 'Please select an answer.';
                        feedbackEl.classList.add('incorrect');
                    }
                }
            });


            // Initial state and initial render
            switchTab('intro-rl');

            // --- DEBOUNCE FOR RESIZE ---
            function debounce(func, wait, immediate) {
                var timeout;
                return function() {
                    var context = this, args = arguments;
                    var later = function() {
                        timeout = null;
                        if (!immediate) func.apply(context, args);
                    };
                    var callNow = immediate && !timeout;
                    clearTimeout(timeout);
                    timeout = setTimeout(later, wait);
                    if (callNow) func.apply(context, args);
                };
            };
            
            const onResize = debounce(() => {
                const activeCanvas = document.querySelector('.content-section.active canvas');
                if(!activeCanvas) return;

                const id = activeCanvas.id.split('-')[0];
                if(id === 'gridworld') initGridworld(true);
                if(id === 'blackjack') initBlackjackMC(true);
                if(id === 'td') initTDGridworld(true);
                if(id === 'mcts') initMCTSAnimation(true);

            }, 250);

            window.addEventListener('resize', onResize);


            // --- Multi-Armed Bandit ---
            function initBanditProblem(isResize = false) {
                const armsContainer = document.getElementById('bandit-arms-container');
                if (!armsContainer || (armsContainer.dataset.initialized && !isResize)) return;
                armsContainer.dataset.initialized = 'true';
                // This one is flexbox-based, no canvas, so no resize logic needed.
                if(isResize) return; 
                
                let intervalId = null;

                const NUM_ARMS = 5;
                let trueMeans = [];
                let estimatedValues = [];
                let pullCounts = [];
                let totalReward = 0;
                let totalPulls = 0;
                let epsilon = 0.1;
                
                const epsilonSlider = document.getElementById('epsilon-slider');
                const epsilonValueSpan = document.getElementById('epsilon-value');
                const resetButton = document.getElementById('reset-bandit-btn');
                const totalRewardEl = document.getElementById('bandit-total-reward');
                const totalPullsEl = document.getElementById('bandit-total-pulls');

                epsilonSlider.addEventListener('input', (e) => {
                    epsilon = parseFloat(e.target.value);
                    epsilonValueSpan.textContent = epsilon.toFixed(2);
                });

                resetButton.addEventListener('click', setup);

                function setup() {
                    if(intervalId) clearInterval(intervalId);
                    armsContainer.innerHTML = '';
                    trueMeans = Array.from({length: NUM_ARMS}, () => Math.random());
                    estimatedValues = Array(NUM_ARMS).fill(0);
                    pullCounts = Array(NUM_ARMS).fill(0);
                    totalReward = 0;
                    totalPulls = 0;

                    for (let i = 0; i < NUM_ARMS; i++) {
                        const armWrapper = document.createElement('div');
                        armWrapper.className = 'flex flex-col items-center h-full justify-end';
                        armWrapper.innerHTML = `
                            <div class="text-sm text-gray-600 mb-1">Pulls: <span id="pulls-${i}">0</span></div>
                            <div class="text-sm font-mono text-blue-600 mb-1">Q(a): <span id="q-val-${i}">0.00</span></div>
                            <div class="w-12 sm:w-16 h-4/5 bg-gray-200 rounded-t-lg cursor-pointer hover:bg-gray-300 relative flex items-end justify-center" id="arm-${i}">
                               <div class="bg-blue-500 w-full rounded-t-lg" style="height: 0%;"></div>
                            </div>
                        `;
                        armsContainer.appendChild(armWrapper);
                        document.getElementById(`arm-${i}`).addEventListener('click', () => chooseAndPull(i));
                    }
                    updateDisplay();
                    if(window.banditInterval) clearInterval(window.banditInterval);
                    window.banditInterval = setInterval(() => {
                        if (document.getElementById('bandit-arms-container')) {
                            chooseAndPull();
                        } else {
                            clearInterval(window.banditInterval);
                        }
                    }, 100);
                }

                function chooseAndPull(armIndex = -1) {
                    let choice;
                    if (armIndex !== -1) {
                        choice = armIndex;
                    } else {
                        if (Math.random() < epsilon) {
                            choice = Math.floor(Math.random() * NUM_ARMS);
                        } else {
                            const maxQ = Math.max(...estimatedValues);
                            const bestArms = estimatedValues.map((q, i) => q === maxQ ? i : -1).filter(i => i !== -1);
                            choice = bestArms[Math.floor(Math.random() * bestArms.length)];
                        }
                    }
                    const reward = Math.random() < trueMeans[choice] ? 1 : 0;
                    totalReward += reward;
                    totalPulls++;
                    pullCounts[choice]++;
                    estimatedValues[choice] += (1 / pullCounts[choice]) * (reward - estimatedValues[choice]);
                    updateDisplay();
                }

                function updateDisplay() {
                    for (let i = 0; i < NUM_ARMS; i++) {
                        document.getElementById(`pulls-${i}`).textContent = pullCounts[i];
                        document.getElementById(`q-val-${i}`).textContent = estimatedValues[i].toFixed(2);
                        const armBar = document.querySelector(`#arm-${i} > div`);
                        armBar.style.height = `${estimatedValues[i] * 100}%`;
                    }
                    totalRewardEl.textContent = totalReward;
                    totalPullsEl.textContent = totalPulls;
                }
                
                setup();
            }

            // --- Monte Carlo Blackjack ---
            let blackjackState = {};
            function initBlackjackMC(isResize = false) {
                const canvas = document.getElementById('blackjack-canvas');
                if (!canvas || (canvas.dataset.initialized && !isResize)) return;
                
                if(!isResize) {
                    canvas.dataset.initialized = 'true';
                    blackjackState = {
                        V: {},
                        gamesPlayed: 0
                    };
                    const play1Btn = document.getElementById('blackjack-play1-btn');
                    const play100Btn = document.getElementById('blackjack-play100-btn');
                    const resetBtn = document.getElementById('blackjack-reset-btn');
                    
                    play1Btn.onclick = () => runMultipleEpisodes(1);
                    play100Btn.onclick = () => runMultipleEpisodes(100);
                    resetBtn.onclick = () => {
                         blackjackState.V = {};
                         blackjackState.gamesPlayed = 0;
                         drawValueFunction();
                    };
                }
                
                const getCard = () => {
                    let card = Math.min(10, Math.floor(Math.random() * 13) + 1);
                    return card === 1 ? 11 : card;
                };

                const getHandSum = (hand) => {
                    let sum = hand.reduce((a, b) => a + b, 0);
                    let aces = hand.filter(c => c === 11).length;
                    while (sum > 21 && aces > 0) {
                        sum -= 10;
                        aces--;
                    }
                    return sum;
                };

                const runEpisode = () => {
                    let playerHand = [getCard(), getCard()];
                    let dealerHand = [getCard(), getCard()];
                    let episode = []; 
                    while(true) {
                        let playerSum = getHandSum(playerHand);
                        if(playerSum > 21) break; 
                        let usableAce = playerHand.includes(11) && getHandSum(playerHand.map(c => c === 11 ? 1 : c)) <= 10;
                        let state = `${playerSum}-${dealerHand[0]}-${usableAce}`;
                        let action = playerSum >= 20 ? 'stick' : 'hit';
                        episode.push({ state });
                        if(action === 'stick') break;
                        playerHand.push(getCard());
                    }
                    
                    let playerSum = getHandSum(playerHand);
                    let reward = 0;
                    if(playerSum > 21) {
                        reward = -1;
                    } else {
                        while(getHandSum(dealerHand) < 17) dealerHand.push(getCard());
                        let dealerSum = getHandSum(dealerHand);
                        if(dealerSum > 21 || playerSum > dealerSum) reward = 1;
                        else if(playerSum < dealerSum) reward = -1;
                        else reward = 0;
                    }

                    const visitedStates = new Set();
                    for(const step of episode) {
                        if(!visitedStates.has(step.state)) {
                            if(!blackjackState.V[step.state]) blackjackState.V[step.state] = { sum: 0, count: 0 };
                            blackjackState.V[step.state].sum += reward;
                            blackjackState.V[step.state].count++;
                            visitedStates.add(step.state);
                        }
                    }
                    blackjackState.gamesPlayed++;
                };

                const runMultipleEpisodes = (count) => {
                    for(let i=0; i<count; i++) runEpisode();
                    drawValueFunction();
                };

                const drawValueFunction = () => {
                    const gamesPlayedEl = document.getElementById('blackjack-games-played');
                    gamesPlayedEl.textContent = `Games Played: ${blackjackState.gamesPlayed}`;
                    const ctx = canvas.getContext('2d');
                    
                    const containerWidth = canvas.parentElement.clientWidth;
                    const isMobile = containerWidth < 600;

                    canvas.width = containerWidth;
                    canvas.height = isMobile ? containerWidth * 1.2 : containerWidth / 2;

                    ctx.clearRect(0, 0, canvas.width, canvas.height);
                    
                    const drawGrid = (x_offset, y_offset, grid_width, title) => {
                        ctx.fillStyle = '#3D4451';
                        ctx.font = 'bold 14px Inter';
                        ctx.textAlign = 'center';
                        ctx.fillText(title, x_offset + grid_width / 2, y_offset - 15);

                        const playerSums = Array.from({length: 10}, (_, i) => 21 - i); 
                        const dealerCards = Array.from({length: 10}, (_, i) => i + 1);
                        
                        const colWidth = grid_width / 10;
                        const rowHeight = colWidth;

                        for(let r=0; r < playerSums.length; r++) {
                             for(let c=0; c < dealerCards.length; c++) {
                                let stateStr = `${playerSums[r]}-${dealerCards[c] === 1 ? 11 : dealerCards[c]}-${title.includes('Usable Ace')}`;
                                let value = blackjackState.V[stateStr] ? blackjackState.V[stateStr].sum / blackjackState.V[stateStr].count : 0;
                                let color = 'rgba(200, 200, 200, 0.5)';
                                if(value > 0) color = `rgba(74, 222, 128, ${Math.abs(value)})`;
                                else if(value < 0) color = `rgba(248, 113, 113, ${Math.abs(value)})`;
                                
                                ctx.fillStyle = color;
                                ctx.fillRect(x_offset + c * colWidth, y_offset + r * rowHeight, colWidth, rowHeight);
                                ctx.strokeStyle = '#e5e7eb';
                                ctx.strokeRect(x_offset + c * colWidth, y_offset + r * rowHeight, colWidth, rowHeight);
                             }
                        }
                    };
                    if(isMobile) {
                        const gridWidth = containerWidth * 0.9;
                        const xOffset = (containerWidth - gridWidth) / 2;
                        drawGrid(xOffset, 40, gridWidth, "With Usable Ace");
                        drawGrid(xOffset, 40 + gridWidth * 1.1, gridWidth, "No Usable Ace");
                    } else {
                        const gridWidth = (containerWidth / 2) * 0.8;
                        drawGrid(containerWidth * 0.05, 40, gridWidth, "With Usable Ace");
                        drawGrid(containerWidth * 0.55, 40, gridWidth, "No Usable Ace");
                    }
                };
                
                drawValueFunction();
            }

            // --- Gridworld DP ---
            let dpState = {};
            function initGridworld(isResize = false) {
                const canvas = document.getElementById('gridworld-canvas');
                if (!canvas || (canvas.dataset.initialized && !isResize)) return;
                
                const GRID_SIZE = 5;
                const grid = [[0, 0, 0, 0, 1], [0, -1, 0, 0, 0], [0, 0, 0, -1, 0], [0, -1, 0, 0, 0], [0, 0, 0, 0, 0]];
                const GAMMA = 0.9;

                if (!isResize) {
                    canvas.dataset.initialized = 'true';
                    dpState = {
                        values: Array(GRID_SIZE).fill(0).map(() => Array(GRID_SIZE).fill(0)),
                        iterationCount: 0,
                        intervalId: null
                    };

                    const viButton = document.getElementById('gridworld-vi-btn');
                    const resetButton = document.getElementById('gridworld-reset-btn');
                    viButton.onclick = runValueIteration;
                    resetButton.onclick = reset;
                }
                
                const ctx = canvas.getContext('2d');
                canvas.width = canvas.parentElement.clientWidth;
                canvas.height = canvas.width;
                const CELL_SIZE = canvas.width / GRID_SIZE;
                
                const draw = () => {
                    const iterationsEl = document.getElementById('gridworld-iterations');
                    iterationsEl.textContent = `Iterations: ${dpState.iterationCount}`;
                    ctx.clearRect(0, 0, canvas.width, canvas.height);
                    for (let r = 0; r < GRID_SIZE; r++) {
                        for (let c = 0; c < GRID_SIZE; c++) {
                            ctx.strokeStyle = '#e5e7eb';
                            ctx.strokeRect(c * CELL_SIZE, r * CELL_SIZE, CELL_SIZE, CELL_SIZE);
                            if (grid[r][c] === 1) ctx.fillStyle = 'rgba(74, 222, 128, 0.8)';
                            else if (grid[r][c] === -1) ctx.fillStyle = 'rgba(248, 113, 113, 0.8)';
                            else continue;
                            ctx.fillRect(c * CELL_SIZE, r * CELL_SIZE, CELL_SIZE, CELL_SIZE);
                        }
                    }
                     for (let r = 0; r < GRID_SIZE; r++) {
                        for (let c = 0; c < GRID_SIZE; c++) {
                            ctx.fillStyle = '#1f2937';
                            ctx.font = `${Math.max(10, CELL_SIZE/5)}px Inter`;
                            ctx.textAlign = 'center';
                            ctx.textBaseline = 'middle';
                            ctx.fillText(dpState.values[r][c].toFixed(2), c * CELL_SIZE + CELL_SIZE / 2, r * CELL_SIZE + CELL_SIZE / 2);
                        }
                    }
                }
                
                function valueIterationStep() {
                    let newValues = JSON.parse(JSON.stringify(dpState.values));
                    for (let r = 0; r < GRID_SIZE; r++) {
                        for (let c = 0; c < GRID_SIZE; c++) {
                             if (grid[r][c] !== 0) continue;
                            let actionValues = [];
                            const actions = [[-1, 0], [1, 0], [0, -1], [0, 1]];
                            for (const [dr, dc] of actions) {
                                const nr = r + dr, nc = c + dc;
                                let reward = -0.1, nextValue = 0;
                                if (nr >= 0 && nr < GRID_SIZE && nc >= 0 && nc < GRID_SIZE) {
                                     if(grid[nr][nc] === 1) reward = 10;
                                     if(grid[nr][nc] === -1) reward = -10;
                                     nextValue = dpState.values[nr][nc];
                                } else {
                                    reward = -1;
                                    nextValue = dpState.values[r][c];
                                }
                                actionValues.push(reward + GAMMA * nextValue);
                            }
                            newValues[r][c] = Math.max(...actionValues);
                        }
                    }
                    dpState.values = newValues;
                    dpState.iterationCount++;
                    draw();
                }

                function runValueIteration() {
                     if(dpState.intervalId) clearInterval(dpState.intervalId);
                     dpState.intervalId = setInterval(() => {
                        if (dpState.iterationCount < 50 && document.getElementById('gridworld-canvas')) {
                             valueIterationStep();
                        } else {
                            clearInterval(dpState.intervalId);
                        }
                    }, 100);
                }

                function reset() {
                    if(dpState.intervalId) clearInterval(dpState.intervalId);
                    dpState.values = Array(GRID_SIZE).fill(0).map(() => Array(GRID_SIZE).fill(0));
                    dpState.iterationCount = 0;
                    draw();
                }

                draw();
            }

            // --- Gridworld TD ---
            let tdState = {};
            function initTDGridworld(isResize = false) {
                const canvas = document.getElementById('td-gridworld-canvas');
                if (!canvas || (canvas.dataset.initialized && !isResize)) return;

                const GRID_SIZE = 5;
                const grid = [[0, 0, 0, 0, 1], [0, -1, 0, 0, 0], [0, 0, 0, -1, 0], [0, -1, 0, 0, 0], [0, 0, 0, 0, 0]];
                const actions = [[-1, 0], [1, 0], [0, -1], [0, 1]]; 
                const ALPHA = 0.1;
                const GAMMA = 0.9;
                let EPSILON = 0.1;

                if(!isResize) {
                    canvas.dataset.initialized = 'true';
                    tdState = {
                        Q: {},
                        agentPos: { r: 4, c: 0 },
                        currentAlgorithm: 'sarsa',
                        playInterval: null
                    };

                    const algoSelect = document.getElementById('td-algo-select');
                    const stepBtn = document.getElementById('td-step-btn');
                    const playBtn = document.getElementById('td-play-btn');
                    const resetBtn = document.getElementById('td-reset-btn');

                    algoSelect.onchange = (e) => { tdState.currentAlgorithm = e.target.value; reset(); };
                    stepBtn.onclick = step;
                    playBtn.onclick = togglePlay;
                    resetBtn.onclick = reset;
                }
                
                const ctx = canvas.getContext('2d');
                canvas.width = canvas.parentElement.clientWidth;
                canvas.height = canvas.width;
                const CELL_SIZE = canvas.width / GRID_SIZE;

                const getQ = (r, c) => {
                    const key = `${r},${c}`;
                    if (!tdState.Q[key]) tdState.Q[key] = [0, 0, 0, 0];
                    return tdState.Q[key];
                };

                const chooseAction = (r, c) => {
                    if (Math.random() < EPSILON) return Math.floor(Math.random() * 4);
                    const q_values = getQ(r, c);
                    const maxQ = Math.max(...q_values);
                    const bestActions = q_values.map((q, i) => q === maxQ ? i : -1).filter(i => i !== -1);
                    return bestActions[Math.floor(Math.random() * bestActions.length)];
                };

                function reset() {
                    if(tdState.playInterval) clearInterval(tdState.playInterval);
                    tdState.playInterval = null;
                    document.getElementById('td-play-btn').textContent = 'Auto-Play';
                    tdState.Q = {};
                    tdState.agentPos = { r: 4, c: 0 };
                    draw();
                };

                function step() {
                    const { r, c } = tdState.agentPos;
                    if (grid[r][c] !== 0) { 
                        tdState.agentPos = { r: 4, c: 0 }; 
                        draw();
                        return;
                    }

                    const action_idx = chooseAction(r, c);
                    const [dr, dc] = actions[action_idx];
                    let next_r = r + dr, next_c = c + dc;
                    let reward = -0.1;

                    if (next_r < 0 || next_r >= GRID_SIZE || next_c < 0 || next_c >= GRID_SIZE) {
                        next_r = r; next_c = c; reward = -1;
                    }
                     if (grid[next_r][next_c] === 1) reward = 10;
                     if (grid[next_r][next_c] === -1) reward = -10;

                    const q_current = getQ(r, c)[action_idx];
                    let td_target;
                    
                    if (tdState.currentAlgorithm === 'sarsa') {
                        const next_action_idx = chooseAction(next_r, next_c);
                        const q_next = getQ(next_r, next_c)[next_action_idx];
                        td_target = reward + GAMMA * q_next;
                    } else { // Q-Learning
                        const q_next_values = getQ(next_r, next_c);
                        td_target = reward + GAMMA * Math.max(...q_next_values);
                    }
                    getQ(r, c)[action_idx] += ALPHA * (td_target - q_current);
                    tdState.agentPos = { r: next_r, c: next_c };
                    draw();
                };

                const draw = () => {
                    ctx.clearRect(0, 0, canvas.width, canvas.height);
                    for (let r = 0; r < GRID_SIZE; r++) {
                        for (let c = 0; c < GRID_SIZE; c++) {
                             const q_values = getQ(r,c);
                             const maxQ = Math.max(...q_values);
                             const minQ = Math.min(...q_values);
                             for(let i=0; i<4; i++){
                                let colorVal = 0;
                                if(maxQ > minQ) colorVal = (q_values[i] - minQ) / (maxQ - minQ);
                                ctx.fillStyle = `rgba(74, 144, 226, ${colorVal * 0.7})`;
                                const [dr, dc] = actions[i];
                                if(dr === -1) ctx.fillRect(c*CELL_SIZE, r*CELL_SIZE, CELL_SIZE, CELL_SIZE/3);
                                if(dr === 1) ctx.fillRect(c*CELL_SIZE, (r+2/3)*CELL_SIZE, CELL_SIZE, CELL_SIZE/3);
                                if(dc === -1) ctx.fillRect(c*CELL_SIZE, r*CELL_SIZE, CELL_SIZE/3, CELL_SIZE);
                                if(dc === 1) ctx.fillRect((c+2/3)*CELL_SIZE, r*CELL_SIZE, CELL_SIZE/3, CELL_SIZE);
                             }
                        }
                    }
                    for (let r = 0; r < GRID_SIZE; r++) {
                        for (let c = 0; c < GRID_SIZE; c++) {
                            ctx.strokeStyle = '#e5e7eb';
                            ctx.strokeRect(c * CELL_SIZE, r * CELL_SIZE, CELL_SIZE, CELL_SIZE);
                             if (grid[r][c] === 1) {
                                ctx.fillStyle = 'rgba(74, 222, 128, 0.8)';
                                ctx.fillRect(c * CELL_SIZE, r * CELL_SIZE, CELL_SIZE, CELL_SIZE);
                             } else if (grid[r][c] === -1) {
                                ctx.fillStyle = 'rgba(248, 113, 113, 0.8)';
                                ctx.fillRect(c * CELL_SIZE, r * CELL_SIZE, CELL_SIZE, CELL_SIZE);
                             }
                        }
                    }
                    ctx.fillStyle = '#D0021B';
                    ctx.beginPath();
                    ctx.arc(tdState.agentPos.c * CELL_SIZE + CELL_SIZE/2, tdState.agentPos.r * CELL_SIZE + CELL_SIZE/2, CELL_SIZE/4, 0, 2*Math.PI);
                    ctx.fill();
                };

                const togglePlay = () => {
                    if (tdState.playInterval) {
                        clearInterval(tdState.playInterval);
                        tdState.playInterval = null;
                        document.getElementById('td-play-btn').textContent = 'Auto-Play';
                    } else {
                        tdState.playInterval = setInterval(step, 100);
                        document.getElementById('td-play-btn').textContent = 'Pause';
                    }
                }
                draw();
            }

            // --- MCTS Animation ---
             let mctsState = {};
            function initMCTSAnimation(isResize = false) {
                const canvas = document.getElementById('mcts-canvas');
                if (!canvas || (canvas.dataset.initialized && !isResize)) return;

                const NODE_RADIUS = 25;
                const EXPLORATION_CONSTANT = 1.41;

                if (!isResize) {
                     canvas.dataset.initialized = 'true';
                     mctsState = {
                        tree: null,
                        nodeIdCounter: 0,
                        isPlaying: false,
                        playInterval: null
                     };
                    const stepBtn = document.getElementById('mcts-step-btn');
                    const playBtn = document.getElementById('mcts-play-btn');
                    const resetBtn = document.getElementById('mcts-reset-btn');
                    stepBtn.onclick = runIteration;
                    playBtn.onclick = togglePlay;
                    resetBtn.onclick = reset;
                }
                const ctx = canvas.getContext('2d');
                canvas.width = canvas.parentElement.clientWidth;
                canvas.height = canvas.width * 0.66;


                function createNode(parent) {
                    return {
                        id: mctsState.nodeIdCounter++, parent: parent, children: [], visits: 0, value: 0, x: 0, y: 0,
                        numUnexplored: Math.floor(2 + Math.random() * 2) 
                    };
                }

                function reset() {
                    mctsState.nodeIdCounter = 0;
                    mctsState.tree = createNode(null);
                    if(mctsState.playInterval) clearInterval(mctsState.playInterval);
                    mctsState.isPlaying = false;
                    document.getElementById('mcts-play-btn').textContent = 'Auto-Play';
                    document.getElementById('mcts-step-btn').disabled = false;
                    drawTree();
                }

                function drawNode(node, highlight = 'black') {
                    ctx.beginPath();
                    ctx.arc(node.x, node.y, NODE_RADIUS, 0, Math.PI * 2);
                    ctx.fillStyle = 'white';
                    ctx.fill();
                    ctx.strokeStyle = highlight;
                    ctx.lineWidth = highlight === 'black' ? 2 : 4;
                    ctx.stroke();

                    ctx.fillStyle = '#3D4451';
                    ctx.font = '12px Inter';
                    ctx.textAlign = 'center';
                    ctx.textBaseline = 'middle';
                    const valueText = `${node.value.toFixed(1)} / ${node.visits}`;
                    ctx.fillText(valueText, node.x, node.y);
                }

                function drawEdge(parent, child) {
                    ctx.beginPath();
                    ctx.moveTo(parent.x, parent.y);
                    ctx.lineTo(child.x, child.y);
                    ctx.strokeStyle = '#a1a1aa';
                    ctx.lineWidth = 1;
                    ctx.stroke();
                }
                
                function setNodePositions(node, x, y, width, level) {
                    node.x = x;
                    node.y = y + level * 80;
                    if (node.children.length === 0) return;
                    const childWidth = width / node.children.length;
                    node.children.forEach((child, i) => {
                        setNodePositions(child, x - width/2 + childWidth/2 + i*childWidth, y, level + 1);
                    });
                }

                function drawTree(highlightPath = []) {
                    const statusEl = document.getElementById('mcts-status');
                    statusEl.textContent = "Status: Ready";
                    ctx.clearRect(0, 0, canvas.width, canvas.height);
                    setNodePositions(mctsState.tree, canvas.width / 2, 50, canvas.width * 0.9, 0);
                    
                    const nodesToDraw = [mctsState.tree];
                    while(nodesToDraw.length > 0) {
                        const node = nodesToDraw.pop();
                        node.children.forEach(child => drawEdge(node, child));
                        nodesToDraw.push(...node.children);
                    }
                     const highlightSet = new Set(highlightPath.map(n => n.id));
                     const nodesToDrawAgain = [mctsState.tree];
                      while(nodesToDrawAgain.length > 0) {
                        const node = nodesToDrawAgain.pop();
                        const highlightColor = highlightSet.has(node.id) ? '#4A90E2' : 'black';
                        drawNode(node, highlightColor);
                        nodesToDrawAgain.push(...node.children);
                    }
                }
                
                function ucb1(node) {
                    if (node.visits === 0) return Infinity;
                    const exploitation = node.value / node.visits;
                    const exploration = EXPLORATION_CONSTANT * Math.sqrt(Math.log(node.parent.visits) / node.visits);
                    return exploitation + exploration;
                }

                async function runIteration() {
                    document.getElementById('mcts-step-btn').disabled = true;
                    let currentNode = mctsState.tree;
                    let path = [currentNode];
                    while (currentNode.children.length > 0 && currentNode.numUnexplored === 0) {
                        currentNode = currentNode.children.reduce((best, child) => ucb1(child) > ucb1(best) ? child : best);
                        path.push(currentNode);
                    }
                    if (currentNode.numUnexplored > 0) {
                        const newNode = createNode(currentNode);
                        currentNode.children.push(newNode);
                        currentNode.numUnexplored--;
                        currentNode = newNode;
                        path.push(currentNode);
                    }
                    const result = Math.random() > 0.5 ? 1 : -1;
                    for (let i = path.length - 1; i >= 0; i--) {
                        const node = path[i];
                        node.visits++;
                        node.value += result;
                    }
                    drawTree();
                    document.getElementById('mcts-step-btn').disabled = false;
                }

                 const togglePlay = () => {
                    mctsState.isPlaying = !mctsState.isPlaying;
                    const playBtn = document.getElementById('mcts-play-btn');
                    if (mctsState.isPlaying) {
                        playBtn.textContent = 'Pause';
                        document.getElementById('mcts-step-btn').disabled = true;
                        runIteration();
                        mctsState.playInterval = setInterval(runIteration, 1000);
                    } else {
                        playBtn.textContent = 'Auto-Play';
                        document.getElementById('mcts-step-btn').disabled = false;
                        clearInterval(mctsState.playInterval);
                    }
                }

                if(!mctsState.tree) reset();
                else drawTree();
            }

        });
    </script>
</body>
</html>

